<h3 id="migrating-an-rds-instance">Migrating an RDS instance</h3><p>This guide covers one way of migrating an RDS instance from one AWS account to another.
This is a standard approach using<code>pg_dump</code> and <code>psql</code>.</p>
<p>This guide assumes the migration comply with the following :</p>

<ul>
<li>The migration happens from a <em>source</em> postresql RDS instance to a <em>target</em> postresql RDS instance</li>
<li>Elevated &amp; short-lived sets of postgres credentials are available for both <em>source</em> and <em>target</em></li>
</ul>
<h4 id="overview">Overview</h4><h5 id="postgres-utilities">Postgres utilities</h5><p>It is possible to do a full database migration using only official CLI tools, provided by Postgres. 
Using <code>pg_dump</code> and <code>psql</code>, this documents describes the migration process.</p>
<p><a href="https://www.postgresql.org/docs/9.4/app-pgdump.html">pg_dump</a></p>
<p><strong>Using this tooling implies having a <em>source</em> database downtime</strong>. (As you don&rsquo;t want data being written to it while migrating it.)</p>
<p>The steps including those tools will always be the same; on one side we export from source, on the target side we restore.</p>
<h5 id="pre-data-data-post-data">Pre-Data, Data, Post-Data</h5><p><code>pg_dump</code> can be used to export one big archive that can then be restored with <code>pg_restore</code>.
One issue with that approach is the difficulty of troubleshooting migration issues.</p>
<p>Since we are trying to make this process as clear as possible, the following guide is decomposing <code>pg_dump</code> into its three components :</p>

<ul>
<li>Pre-data : The table structures, functions.</li>
<li>Post-data: indexes, triggers, rules, and constraints</li>
<li>Data : data</li>
</ul>
<p>With that deconstructed process, it is easier to debug issues (and get help from the Cloud Platform team) and most importantly your team can perform validation/testing incrementally.</p>
<h4 id="step-0-pod">Step 0 - Pod</h4><p>In order to run postgresql commands against both of those endpoints, there needs to be a place that has access to both.</p>
<p>This is solved by running a pod into the kubernetes cluster, on live-1, into the team&rsquo;s namespace. 
The migration steps outlined below have been tested from a pod running a <code>bitnami/postgresql</code> Docker image.</p>
<p>Regarding the network access:</p>

<ul>
<li>The <em>source</em> RDS needs to have its <code>public accessibility</code> config turned on.</li>
<li>The RDS security group needs to be opened to the Cluster. For that, add inbound rules from the NAT gateways&rsquo; IP address on the 5432 port.<br></li>
<li>The RDS instance needs to support SSL connections</li>
</ul>
<h4 id="step-1-pre-data">Step 1 - Pre-Data</h4><p>First, to export,  we run :</p>
<div class="highlight"><pre class="highlight plaintext"><code>pg_dump -U source_username \
     -h source_endpoint \
     -d source_database \
     -O \
     --section=pre-data &gt; pre-data.sql
</code></pre></div><p>Here, <code>-O</code> tells RDS to export the table structure without owners.
The command above stores the data in a local file.</p>
<p>Then to restore this into the <em>target</em>, we use <code>psql</code>:</p>
<div class="highlight"><pre class="highlight plaintext"><code>psql -U target_username \
     -h target_endpoint \
     -d target_database \
     -f pre-data.sql

</code></pre></div><p>If using a local file is problematic, those two commands can be piped together (<code>|</code>)</p>
<h4 id="step-2-sequences">Step 2 - Sequences</h4><p>Sequences are essential for your database to know what the latest increment of the primary keys is. Sequences are held in special tables that will not be migrated from step 1.</p>
<p>First, to export,  we run :</p>
<div class="highlight"><pre class="highlight plaintext"><code>pg_dump -U source_username \
     -h source_endpoint \
     -d source_database \
     -t '*_seq' &gt; sequences.sql
</code></pre></div><p>Here, <code>-t &#39;*_seq&#39;</code> indicates to <code>pg_dump</code> that we only want to export the table ending in <code>_seq</code>, which are the sequences.</p>
<p>Then to restore this into the <em>target</em>, we use <code>psql</code>:</p>
<div class="highlight"><pre class="highlight plaintext"><code>psql -U target_username \
     -h target_endpoint \
     -d target_database \
     -f sequences.sql

</code></pre></div><p>If using a local file is problematic, those two commands can be piped together (<code>|</code>)</p>
<h4 id="step-3">Step 3</h4><p>Sequences are essential for your database to know what the latest increment of the primary keys is. Sequences are held in special tables that will not be migrated from step 1.</p>
<p>First, to export,  we run :</p>
<div class="highlight"><pre class="highlight plaintext"><code>pg_dump -U source_username \
     -h source_endpoint \
     -d source_database \
     -O \
     --section=data &gt; data.sql
</code></pre></div><p>Here, <code>-O</code> tells RDS to export the table structure without owners.
The command above stores the data in a local file.</p>
<p>Then to restore this into the <em>target</em>, we use <code>psql</code>:</p>
<div class="highlight"><pre class="highlight plaintext"><code>psql -U target_username \
     -h target_endpoint \
     -d target_database \
     -f data.sql

</code></pre></div><p>If using a local file is problematic, those two commands can be piped together (<code>|</code>)</p>
<h4 id="step-4-post-data">Step 4 - Post-Data</h4><p>Any constraints, indexes and foreign keys are also a special kind of metadata that would not be migrated during any of the steps above. 
All of data is contained within the <code>post-data</code> section.</p>
<p>The process is almost identical as Step 1 :</p>
<p>First, to export,  we run :</p>
<div class="highlight"><pre class="highlight plaintext"><code>pg_dump -U source_username \
     -h source_endpoint \
     -d source_database \
     -O \
     --section=post-data &gt; post-data.sql
</code></pre></div><p>Then to restore this into the <em>target</em>, we use <code>psql</code>:</p>
<div class="highlight"><pre class="highlight plaintext"><code>psql -U target_username \
     -h target_endpoint \
     -d target_database \
     -f post-data.sql
</code></pre></div><p>If using a local file is problematic, those two commands can be piped together (<code>|</code>)</p>
<h4 id="step-5-data-validation-very-important">Step 5 - Data Validation (Very Important)</h4><p>After a migration, <strong>it is your team&rsquo;s responsibility</strong> to ensure the data, its integrity and anything required by your application to operate properly have been preserved.</p>
<p>Even though the process above is handling the data and the meta-data migration, it is essential for you to have a <em>data validation strategy</em> to confirm everything is in order.</p>
<p>The Cloud Platform team can&rsquo;t provide a how-to guide on data validation, as each database migrations are wildly different.</p>
<h4 id="step-6-clean-up">Step 6 - Clean up</h4><p>After a successful migration, we can clean up by :</p>

<ul>
<li>Deleting the pod from STEP 1 </li>
<li>Disabling the network access from the live-1 cluster to the <em>source</em> RDS</li>
<li>Revoke the temporary credentials created for the migration</li>
</ul>
