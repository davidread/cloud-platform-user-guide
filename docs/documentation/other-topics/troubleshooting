<h3 id="troubleshooting-guide">Troubleshooting guide</h3><h4 id="overview">Overview</h4><p>This document intends to give you an idea of potential troubleshooting tips and techniques to investigate and resolve application issues on the Cloud Platform. The Kubernetes project also offers a resource on <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/">troubleshooting</a> so we will attempt to avoid overlap.</p>
<p>Throughout this document we will refer to terms such as <code>&lt;pod_name&gt;</code> and <code>&lt;namespace&gt;</code>, it is assumed you&rsquo;ll know how to gather this information using the <code>kubectl get</code> commands.</p>
<h4 id="contents">Contents</h4>
<ul>
<li><a href="#my-pod-shows-createcontainerconfigerror-after-deployment">My pod shows &lsquo;CreateContainerConfigError&rsquo; after deployment</a></li>
<li><a href="#my-pod-shows-crashloopbackoff-after-deployment">My pod shows &#39;CrashLoopBackOff&rsquo; after deployment</a></li>
<li><a href="#my-pod-shows-imagepullbackoff-after-deployment">My pod shows &#39;ImagePullBackOff&rsquo; after deployment</a></li>
<li><a href="#i-get-the-error-39-container-is-unhealthy-it-will-be-killed-and-re-created-39">I get the error &#39;container is unhealthy, it will be killed and re-created’</a></li>
<li><a href="#i-get-the-error-39-error-from-server-forbidden-39-when-trying-to-run-kubectl-commands">I get the error &#39;Error from server (Forbidden)&rsquo; when trying to run kubectl commands</a></li>
</ul>
<h4 id="my-pod-shows-39-createcontainerconfigerror-39-after-deployment">My pod shows &#39;CreateContainerConfigError&rsquo; after deployment</h4><h5 id="scenario">Scenario</h5><p>You have deployed an application to the Cloud Platform and its status shows <code>CreateContainerConfigError</code> (you can get the status by running <code>kubect get pods -n &lt;namespace&gt;</code>).</p>
<h5 id="cause">Cause</h5><p>There are a number of reasons why this error will appear but the most probable cause is a missing secret or environment variable.</p>
<h5 id="troubleshooting">Troubleshooting</h5><p>To investigate this issue you&rsquo;ll want to look at the events of your pod. Run <code>kubectl -n &lt;namespace&gt; describe &lt;pod_name&gt;</code>, in the section titled <code>Events</code> you&rsquo;ll have something similar to:</p>
<div class="highlight"><pre class="highlight plaintext"><code>Events:
  Type     Reason     Age                    From                                                   Message
  ----     ------     ----                   ----                                                   -------
  Normal   Scheduled  54m                    default-scheduler                                      Successfully assigned myapplication-namespace/myapplication to worker-node
  Warning  Failed     49m (x8 over 53m)      kubelet, worker-node  Error: Couldn't find key postgresUser in Secret myapplication-dev/myapplication
</code></pre></div>
<blockquote>
<p>Note: you can also find this out using <code>kubectl -n &lt;namespace&gt; get events</code></p>
</blockquote>
<h5 id="solution">Solution</h5><p>Ensure all secrets and environment variables have been defined.</p>
<h4 id="my-pod-shows-crashloopbackoff-after-deployment">My pod shows <code>CrashLoopBackOff</code> after deployment</h4><h5 id="my-pod-shows-crashloopbackoff-after-deployment-scenario">Scenario</h5><p>Following the deployment of your application, you receive a status of <code>CrashLoopBackOff</code> when querying the pods in your namespace with <code>kubectl get pods -n &lt;namespace&gt;</code>.</p>
<h5 id="my-pod-shows-crashloopbackoff-after-deployment-cause">Cause</h5><p>A <code>CrashloopBackOff</code> means that you have a pod starting, crashing, starting again, and then crashing again.</p>
<p>A PodSpec has a restartPolicy field with possible values Always, OnFailure, and Never which applies to all containers in a pod. The default value is Always and the restartPolicy only refers to restarts of the containers by the kubelet on the same node (so the restart count will reset if the pod is rescheduled in a different node). Failed containers that are restarted by the kubelet are restarted with an exponential back-off delay (10s, 20s, 40s …) capped at five minutes, and is reset after ten minutes of successful execution. This is an example of a PodSpec with the restartPolicy field:</p>
<div class="highlight"><pre class="highlight plaintext"><code>apiVersion: v1
kind: Pod
metadata:
  name: dummy-pod
spec:
  containers:
    - name: dummy-pod
      image: ubuntu
  restartPolicy: Always
</code></pre></div><p>The main causes of <code>CrashloopBackOff</code> are:</p>

<ul>
<li>The application inside the container keeps crashing.</li>
<li>Some parameters of the pod or container have been configured incorrectly.</li>
<li>An error has been made when deploying Kubernetes.</li>
</ul>
<h5 id="my-pod-shows-crashloopbackoff-after-deployment-troubleshooting">Troubleshooting</h5><p>As this issue is quite vague, you&rsquo;ll want to start by checking what your pod is printing to STDOUT using the command <code>kubectl -n &lt;namespace&gt; logs &lt;pod_name&gt;</code>. This should give you a clear understanding of whether the application has crashed. If your application is crashing, you need to identify the reason and try to fix it. Some possibilities are missing environment variables, insufficient resources, missing files or directories, or a lack of required permissions.</p>
<h5 id="my-pod-shows-crashloopbackoff-after-deployment-solution">Solution</h5><p>Fix application or misconfiguration errors.</p>
<h4 id="my-pod-shows-imagepullbackoff-after-deployment">My pod shows <code>ImagePullBackOff</code> after deployment</h4><h5 id="my-pod-shows-imagepullbackoff-after-deployment-scenario">Scenario</h5><p>You have deployed an application to the Cloud Platform and its status shows <code>ImagePullBackOff</code> (you can get the status by running <code>kubect get pods -n &lt;namespace&gt;</code>).</p>
<h5 id="my-pod-shows-imagepullbackoff-after-deployment-cause">Cause</h5><p>This error is caused by a misconfiguration in your deployment, there are three primary culprits besides network connectivity issues:</p>

<ul>
<li>The image tag is incorrect</li>
<li>The image doesn&rsquo;t exist (or is in a different registry)</li>
<li>Kubernetes doesn&rsquo;t have permissions to pull that image</li>
</ul>
<h5 id="my-pod-shows-imagepullbackoff-after-deployment-troubleshooting">Troubleshooting</h5><p>Again, utilising the <code>kubectl -n &lt;namespace&gt; describe &lt;pod_name&gt;</code> command you can see that the pod has failed to pull down the correct image:
<code>
  Normal   Pulling    10m (x4 over 12m)    kubelet, worker-node  pulling image &quot;redis:foobar&quot;
  Warning  Failed     10m (x4 over 12m)    kubelet, worker-node  Error: ErrImagePull
</code></p>
<h5 id="my-pod-shows-imagepullbackoff-after-deployment-solution">Solution</h5><p>This can be corrected by amending the <code>image</code> in your deployment.</p>
<h4 id="i-get-the-error-39-container-is-unhealthy-it-will-be-killed-and-re-created-39">I get the error &#39;container is unhealthy, it will be killed and re-created&rsquo;</h4><h5 id="situation">Situation</h5><p>Your pod appears to be jumping between an <code>error</code> and <code>ready</code> state. You describe the pod and get an error <code>container is unhealthy, it will be killed and re-created</code></p>
<h5 id="i-get-the-error-39-container-is-unhealthy-it-will-be-killed-and-re-created-39-cause">Cause</h5><p>Kubernetes provides two essential features called <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">Liveness Probes and Readiness Probes</a>. Essentially, Liveness/Readiness Probes will periodically perform an action (e.g. make an HTTP request, open a tcp connection, or run a command in your container) to confirm that your application is working as intended.</p>
<p>If the Liveness Probe fails, Kubernetes will kill your container and create a new one. If the Readiness Probe fails, that Pod will not be available as a Service endpoint, meaning no traffic will be sent to that Pod until it becomes Ready.</p>
<h5 id="i-get-the-error-39-container-is-unhealthy-it-will-be-killed-and-re-created-39-troubleshooting">Troubleshooting</h5><p>Using the log output (<code>kubectl -n &lt;namespace&gt; &lt;pod_name&gt;</code>) you should see the cause of your Probe failure.</p>
<p>There are likely three possibilities:</p>

<ul>
<li> Your Probes are now incorrect - Did the health URL change?</li>
<li> Your Probes are too sensitive - Does your application take a while to start or respond?</li>
<li> Your application is no longer responding correctly to the Probe - Is your database misconfigured?</li>
</ul>
<h5 id="i-get-the-error-39-container-is-unhealthy-it-will-be-killed-and-re-created-39-solution">Solution</h5><p>Once a change has been made to either your application or Probe, a fresh deployment should succeed.</p>
<h4 id="i-get-the-error-39-error-from-server-forbidden-39-when-trying-to-run-kubectl-commands">I get the error &#39;Error from server (Forbidden)&rsquo; when trying to run kubectl commands</h4><h5 id="i-get-the-error-39-error-from-server-forbidden-39-when-trying-to-run-kubectl-commands-situation">Situation</h5><p>When attempting to use a command such as <code>kubectl get pods -n &lt;namespace&gt;</code> the following error occurs:</p>
<p><code>Error from server (Forbidden): pods is forbidden: User &quot;https://justice-cloud-platform.eu.auth0.com/#&lt;username&gt;&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;&lt;namespace&gt;&quot;</code></p>
<h5 id="i-get-the-error-39-error-from-server-forbidden-39-when-trying-to-run-kubectl-commands-cause">Cause</h5><p>This is usually one of two things:</p>

<ul>
<li>Your token has expired and you need to re-authenticate.</li>
<li>You don&rsquo;t belong to the GitHub team assigned to the namespace you&rsquo;re attempting to communicate with.</li>
</ul>
<h5 id="i-get-the-error-39-error-from-server-forbidden-39-when-trying-to-run-kubectl-commands-troubleshooting">Troubleshooting</h5><p>First check if you&rsquo;re a member of the GitHub team assigned in your <a href="https://github.com/ministryofjustice/cloud-platform-environments/blob/master/namespaces/live-1.cloud-platform.service.justice.gov.uk/hmpps-book-secure-move-api-production/01-rbac.yaml#L8">rbac.yaml</a> file, which is located in the <a href="https://github.com/ministryofjustice/cloud-platform-environments/tree/master/namespaces">cloud-platform-environments</a> repository.</p>
<h5 id="i-get-the-error-39-error-from-server-forbidden-39-when-trying-to-run-kubectl-commands-solution">Solution</h5><p>Depending on the cause of your issue you&rsquo;ll need to do one of the following:</p>

<ul>
<li>Re-authenticate to the cluster using the <a href="https://user-guide.cloud-platform.service.justice.gov.uk/tasks.html#authentication">cloud-platform user-guide</a>.</li>
<li>Ask a team member to <a href="https://help.github.com/en/github/setting-up-and-managing-organizations-and-teams/adding-organization-members-to-a-team">add your GitHub user account</a> to the correct team (please note, the Cloud Platform team cannot do this for you).</li>
</ul>
