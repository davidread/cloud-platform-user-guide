<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta content="IE=edge" http-equiv="X-UA-Compatible">
    <meta charset="utf-8">
    <meta content="width=device-width,initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0,user-scalable=no" name="viewport">
      <meta name="robots" content="noindex">

    <title>Tasks | Cloud Platform User Guide</title>

    <!--[if gt IE 8]><!--><link href="/stylesheets/screen.css" rel="stylesheet" media="screen" /><!--<![endif]-->
    <!--[if lte IE 8]><link href="/stylesheets/screen-old-ie.css" rel="stylesheet" media="screen" /><![endif]-->

    <link rel="canonical" href="https://user-guide.cloud-platform.service.justice.gov.uk/tasks.html">


    <link href="/stylesheets/print.css" rel="stylesheet" media="print" />
    <script src="/javascripts/application.js"></script>

      <meta property="og:image" content="https://user-guide.cloud-platform.service.justice.gov.uk/images/govuk-large.png" />
      <meta property="og:site_name" content="Cloud Platform User Guide" />
      <meta property="og:title" content="Tasks" />
      <meta property="og:type" content="object" />
      <meta property="og:url" content="https://user-guide.cloud-platform.service.justice.gov.uk/tasks.html" />
      <meta property="twitter:card" content="summary" />
      <meta property="twitter:domain" content="user-guide.cloud-platform.service.justice.gov.uk" />
      <meta property="twitter:image" content="https://user-guide.cloud-platform.service.justice.gov.uk/images/govuk-large.png" />
      <meta property="twitter:title" content="Tasks | Cloud Platform User Guide" />
      <meta property="twitter:url" content="https://user-guide.cloud-platform.service.justice.gov.uk/tasks.html" />

    
  </head>

  <body>
    <div class="app-pane">
      <div class="app-pane__header toc-open-disabled">
        <a href="#content" class="skip-link">Skip to main content</a>

        <header class="header header--full-width">
  <div class="header__container">
    <div class="header__brand">
        <a href="/">
        <span class="header__title">
          Cloud Platform User Guide
            <span class="phase-banner">INTERNAL</span>
        </span>
        </a>
    </div>

      <div data-module="navigation">
        <button type="button" class="header__navigation-toggle js-nav-toggle" aria-controls="navigation" aria-label="Show or hide top level navigation">Menu</button>

        <nav id="navigation" class="header__navigation js-nav" aria-label="Top Level Navigation" aria-hidden="true">
          <ul>
              <li>
                <a href="mailto:platforms+user-guide@digital.justice.gov.uk?subject=User+guide+feedback">Feedback / Report a problem</a>
              </li>
              <li>
                <a href="/">Documentation</a>
              </li>
              <li>
                <a href="https://github.com/ministryofjustice/cloud-platform-user-guide">GitHub</a>
              </li>
          </ul>
        </nav>
      </div>
  </div>
</header>

      </div>

        <div id="toc-heading" class="toc-show fixedsticky">
          <a href="#toc" class="toc-show__label js-toc-show" aria-controls="toc">
            Table of contents <span class="toc-show__icon"></span>
          </a>
        </div>

      <div class="app-pane__body" data-module="in-page-navigation">
          <div class="app-pane__toc">
            <div class="toc" data-module="table-of-contents">
              <div class="search" data-module="search">
  <form action="https://www.google.co.uk/search" method="get" role="search">
    <input type="hidden" name="as_sitesearch" value="https://user-guide.cloud-platform.service.justice.gov.uk"/>
    <label for="search"  class="search__label">Search (via Google)</label>
    <input type="text" id="search" name="q" placeholder="Search" aria-controls="search-results" class="form-control" />
  </form>
  <div id="search-results" class="search-results" aria-hidden="true" role="dialog" aria-labelledby="search-results-title">
    <div class="search-results__inner">
      <button class="search-results__close">Close<span class="search-results__close-label"> search results</span></button>
      <h2 id="search-results-title" class="search-results__title" aria-live="polite">Results</h2>
      <div class="search-results__content"></div>
    </div>
  </div>
</div>

              <a href="#" class="toc__close js-toc-close" aria-controls="toc" aria-label="Hide table of contents"></a>
              <nav id="toc" class="js-toc-list toc__list" aria-labelledby="toc-heading" data-module="collapsible-navigation">
                      <ul>
  <li>
    <a href="/#cloud-platform-user-guide">Cloud platform user guide</a>
    <ul>
      <li>
        <ul>
          <li>
            <a href="/#this-guide">This guide</a>
          </li>
          <li>
            <a href="/#who-is-the-platform-for">Who is the platform for</a>
          </li>
          <li>
            <a href="/#what-do-we-currently-support">What do we currently support</a>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<ul>
  <li>
    <a href="/concepts.html#concepts">Concepts</a>
    <ul>
      <li>
        <a href="/concepts.html#kubernetes">Kubernetes</a>
        <ul>
          <li>
            <a href="/concepts.html#resources">Resources</a>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<ul>
  <li>
    <a href="/tasks.html#tasks">Tasks</a>
    <ul>
      <li>
        <a href="/tasks.html#how-to-use-kubectl-to-connect-to-the-cluster">How to use kubectl to connect to the cluster</a>
        <ul>
          <li>
            <a href="/tasks.html#installation">Installation</a>
          </li>
          <li>
            <a href="/tasks.html#authentication">Authentication</a>
          </li>
          <li>
            <a href="/tasks.html#where-to-go-from-here">Where to go from here?</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#creating-a-cloud-platform-environment">Creating a Cloud Platform Environment</a>
        <ul>
          <li>
            <a href="/tasks.html#introduction">Introduction</a>
          </li>
          <li>
            <a href="/tasks.html#objective">Objective</a>
          </li>
          <li>
            <a href="/tasks.html#create-an-environment">Create an environment</a>
          </li>
          <li>
            <a href="/tasks.html#accessing-your-environments">Accessing your environments</a>
          </li>
          <li>
            <a href="/tasks.html#next-steps">Next steps</a>
          </li>
          <li>
            <a href="/tasks.html#more-information-on-environment-definition">More information on environment definition</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#creating-an-ecr-repository">Creating an ECR repository</a>
        <ul>
          <li>
            <a href="/tasks.html#creating-an-ecr-repository-introduction">Introduction</a>
          </li>
          <li>
            <a href="/tasks.html#accessing-the-credentials">Accessing the credentials</a>
          </li>
          <li>
            <a href="/tasks.html#creating-an-ecr-repository-next-steps">Next steps</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#adding-aws-resources-to-your-environment">Adding AWS resources to your environment</a>
        <ul>
          <li>
            <a href="/tasks.html#available-modules">Available modules</a>
          </li>
          <li>
            <a href="/tasks.html#usage">Usage</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#deploying-applications">Deploying Applications</a>
        <ul>
          <li>
            <a href="/tasks.html#deploying-a-39-hello-world-39-application-to-the-cloud-platform">Deploying a ‘Hello World’ application to the Cloud Platform</a>
          </li>
          <li>
            <a href="/tasks.html#deploying-a-multi-container-application-to-the-cloud-platform">Deploying a multi-container application to the Cloud Platform</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#how-do-i-run-rails-database-migrations">How do I run Rails database migrations?</a>
        <ul>
          <li>
            <a href="/tasks.html#do-not-run-migrations-on-container-startup">Do not run migrations on container startup</a>
          </li>
          <li>
            <a href="/tasks.html#further-reading">Further reading</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#adding-a-secret-to-an-application">Adding a secret to an application</a>
        <ul>
          <li>
            <a href="/tasks.html#adding-a-secret-to-an-application-overview">Overview</a>
          </li>
          <li>
            <a href="/tasks.html#adding-a-secret-to-an-application-prerequisites">Prerequisites</a>
          </li>
          <li>
            <a href="/tasks.html#configuring-secrets">Configuring secrets</a>
          </li>
          <li>
            <a href="/tasks.html#creating-the-secret">Creating the secret</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#creating-pingdom-checks">Creating Pingdom checks</a>
        <ul>
          <li>
            <a href="/tasks.html#creating-pingdom-checks-overview">Overview</a>
          </li>
          <li>
            <a href="/tasks.html#creating-pingdom-checks-prerequisites">Prerequisites</a>
          </li>
          <li>
            <a href="/tasks.html#create-a-pingdom-check">Create a Pingdom check</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#cleaning-up">Cleaning up</a>
        <ul>
          <li>
            <a href="/tasks.html#2-tell-terraform-to-delete-your-aws-resources">2. Tell terraform to delete your AWS resources</a>
          </li>
          <li>
            <a href="/tasks.html#3-remove-your-namespace-code-from-the-cloud-platform-environments-repository">3. Remove your namespace code from the cloud-platform-environments repository</a>
          </li>
          <li>
            <a href="/tasks.html#4-delete-all-of-the-kubernetes-resources-inside-your-namespace">4. Delete all of the kubernetes resources inside your namespace.</a>
          </li>
          <li>
            <a href="/tasks.html#5-delete-your-namespace-from-the-cluster">5. Delete your namespace from the cluster.</a>
          </li>
          <li>
            <a href="/tasks.html#summary">Summary</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#migrating-from-live-0-to-live-1">Migrating from Live-0 to Live-1</a>
        <ul>
          <li>
            <a href="/tasks.html#migrating-from-live-0-to-live-1-overview">Overview</a>
          </li>
          <li>
            <a href="/tasks.html#accessing-the-live-1-cluster">Accessing the Live-1 cluster</a>
          </li>
          <li>
            <a href="/tasks.html#generating-a-new-environment">Generating a new environment</a>
          </li>
          <li>
            <a href="/tasks.html#generating-a-new-ecr-repository">Generating a new ECR repository</a>
          </li>
          <li>
            <a href="/tasks.html#changing-the-circleci-environment-variables">Changing the CircleCI environment variables</a>
          </li>
          <li>
            <a href="/tasks.html#deleting-your-live-0-deployment">Deleting your Live-0 deployment</a>
          </li>
          <li>
            <a href="/tasks.html#other-considerations">Other considerations</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#other-topics">Other Topics</a>
        <ul>
          <li>
            <a href="/tasks.html#git-crypt">Git-Crypt</a>
          </li>
          <li>
            <a href="/tasks.html#secrets-overview">Secrets overview</a>
          </li>
          <li>
            <a href="/tasks.html#kubectl-quick-reference">Kubectl quick reference</a>
          </li>
          <li>
            <a href="/tasks.html#cloud-platform-support">Cloud Platform Support</a>
          </li>
          <li>
            <a href="/tasks.html#zero-downtime-deployments">Zero Downtime Deployments</a>
          </li>
          <li>
            <a href="/tasks.html#using-an-externally-managed-hostname">Using an externally-managed hostname</a>
          </li>
          <li>
            <a href="/tasks.html#ip-whitelisting">IP Whitelisting</a>
          </li>
          <li>
            <a href="/tasks.html#ecr-lifecycle-policy">ECR Lifecycle Policy</a>
          </li>
          <li>
            <a href="/tasks.html#applying-a-maintenance-page">Applying a Maintenance Page</a>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<ul>
  <li>
    <a href="/reference.html#reference">Reference</a>
    <ul>
      <li>
        <a href="/reference.html#kubernetes-resources">Kubernetes resources</a>
        <ul>
          <li>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<ul>
  <li>
    <a href="/getting-help.html#getting-help">Getting Help</a>
    <ul>
      <li>
        <a href="/getting-help.html#slack-channel">Slack Channel</a>
      </li>
      <li>
        <a href="/getting-help.html#raise-a-support-ticket">Raise a support ticket</a>
      </li>
    </ul>
  </li>
</ul>
<ul>
  <li>
    <a href="/archive.html#archive">Archive</a>
    <ul>
      <li>
        <a href="/archive.html#getting-help-deploying-applications">Deploying Applications</a>
        <ul>
          <li>
            <a href="/archive.html#prerequisite-for-live-1-deployment">Prerequisite for Live-1 deployment</a>
          </li>
          <li>
            <a href="/archive.html#deploying-an-application-to-the-cloud-platform-with-helm">Deploying an application to the Cloud Platform with Helm</a>
          </li>
          <li>
            <a href="/archive.html#continuous-deployment-of-an-application-using-circleci-and-helm">Continuous Deployment of an application using CircleCI and Helm</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/archive.html#monitoring-applications">Monitoring Applications</a>
        <ul>
          <li>
            <a href="/archive.html#using-the-cloud-platform-prometheus-alertmanager-and-grafana">Using the Cloud Platform Prometheus, AlertManager and Grafana</a>
          </li>
          <li>
            <a href="/archive.html#creating-your-own-custom-alerts">Creating your own custom alerts</a>
          </li>
          <li>
            <a href="/archive.html#getting-application-metrics-into-prometheus">Getting Application Metrics into Prometheus</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/archive.html#application-logging">Application Logging</a>
        <ul>
          <li>
            <a href="/archive.html#application-log-collection-and-storage">Application Log Collection and Storage</a>
          </li>
          <li>
            <a href="/archive.html#accessing-application-log-data">Accessing Application Log Data</a>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>


              </nav>
            </div>
          </div>

        <div class="app-pane__content toc-open-disabled">
          <main id="content" class="technical-documentation" data-module="anchored-headings">
              <h1 id="tasks">Tasks</h1><p>This is a placeholder page, into which content will be migrated.</p>
<p>Please look in the <a href="/archive.html">archive section</a> to find existing content
which is yet to be moved.</p>

<h2 id="how-to-use-kubectl-to-connect-to-the-cluster">How to use <code>kubectl</code> to connect to the cluster</h2><p>The aim of this guide is to provide a walkthrough of the installation of the <code>kubectl</code> command-line tool, the official command-line tool for Kubernetes, as well as setting it up by authenticating with the Cloud Platform cluster.</p>
<p><code>kubectl</code> is used to deploy and manage applications on Kubernetes and is fundamental for anyone who wants to interact with the cluster.</p>
<p>When complete you should have access to perform API calls via a tool called <code>kubectl</code>.</p>
<h3 id="installation">Installation</h3><p>Please read the <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl">official documentation</a> on how to install <code>kubectl</code>.</p>
<h3 id="authentication">Authentication</h3><p>You must have a GitHub account and be a member of the Ministry of Justice Organisation.</p>
<p>For our use case, we want authentication and identity to be handled by Github, and to derive all cluster access control from Github teams - projects will be deployed into namespaces (e.g. <code>pvb-production</code>, <code>cla-staging</code>), and access to resources in those namespaces should be available to the appropriate teams only (e.g. <code>PVB</code> and <code>CLA</code> teams).</p>
<p>Kubernetes supports authentication from external identity providers, including group definition, via <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens">OIDC</a>. Github however only support OAuth2 as an authentication method, so an identity broker is required to translate from OAuth2 to OIDC.</p>
<p>As work on MOJ&rsquo;s identity service is ongoing, a development <a href="https://www.auth0.com">Auth0</a> account has been created to act as a standin in the meantime.</p>
<h4 id="live-clusters">Live clusters</h4><p>Live clusters are those available to users:</p>

<div style="height:1px;font-size:1px;">&nbsp;</div>
<div class="table-container">
        <table>
          <tr>
<th>Cluster Name</th>
<th>Login page</th>
</tr>
<tr>
<td><code>cloud-platform-live-0</code></td>
<td><a href="https://login.apps.cloud-platform-live-0.k8s.integration.dsd.io/">https://login.apps.cloud-platform-live-0.k8s.integration.dsd.io/</a></td>
</tr>
<tr>
<td><code>live-1.cloud-platform</code></td>
<td><a href="https://login.cloud-platform.service.justice.gov.uk/">https://login.cloud-platform.service.justice.gov.uk/</a></td>
</tr>

        </table>
      </div>
<div style="height:1px;font-size:1px;">&nbsp;</div>
<h4 id="how-do-i-connect-to-a-cluster">How do I connect to a cluster?</h4><p>We employ <a href="https://github.com/negz/kuberos">Kuberos</a> to help with the setup, a service that can generate the client configuration for users.</p>
<p>To authenticate with a cluster, please follow the steps below;</p>

<ul>
<li>Navigate to a login page from the table above</li>
<li>Click the login with GitHub option and authorise kuberos</li>
<li>Follow the instructions on the page presented, once finished you should have a <a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/"><code>kubeconfig</code></a> file in <code>~/.kube/config</code>.</li>
<li>You should now be able to run <code>kubectl</code> commands; try running such <code>kubectl get namespaces</code></li>
</ul>
<h5 id="troubleshooting-quot-current-quot-context">Troubleshooting: &ldquo;current&rdquo; context</h5><p>If you receive <code>The connection to the server localhost:8080 was refused</code> errors while executing <code>kubectl</code> commands,
check that your &ldquo;current&rdquo; context is set.</p>
<p>Run <code>kubectl config get-contexts</code>:</p>
<div class="highlight"><pre class="highlight plaintext"><code>CURRENT   NAME                                           CLUSTER                                        AUTHINFO                 NAMESPACE
          live-1.cloud-platform.service.justice.gov.uk   live-1.cloud-platform.service.justice.gov.uk   &lt;your github e-mail&gt;
</code></pre></div><p>Set the context you want to use as &ldquo;current&rdquo; with: <code>kubectl config use-context &lt;NAME&gt;</code>.</p>
<p>E.g. <code>kubectl config use-context live-1.cloud-platform.service.justice.gov.uk</code></p>
<h4 id="multiple-clusters">Multiple clusters</h4><p>To setup additional clusters, follow the process above and save the generated <code>kubeconfig</code> with a different filename (eg.: <code>~/.kube/config_live1</code>).</p>
<p>You can then use the <code>KUBECONFIG</code> environment variable to have <code>kubectl</code> parse the additional configuration files, eg.: <code>KUBECONFIG=~/.kube/config:~/.kube/config_live1</code></p>
<p>For more information please read the <a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">official documentation</a></p>
<h5 id="usernames">Usernames</h5><p>By default, Kuberos will use your email address as the username in the generated <code>kubeconfig</code>.</p>
<p>When setting up multiple clusters, this will generate conflicts so you should rename the users by editing the files (and update the contexts appropriately).</p>
<h3 id="where-to-go-from-here">Where to go from here?</h3><p>Now that you&rsquo;ve setup <code>kubectl</code>, you might want to look at this handy <a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/">cheatsheet</a>.</p>
<p>Once you are ready to deploy applications you will need to create an environment first.</p>

<h2 id="creating-a-cloud-platform-environment">Creating a Cloud Platform Environment</h2><h3 id="introduction">Introduction</h3><p>This is a guide to creating a environment in one of our Kubernetes clusters.</p>
<p>We define an environment as a Kubernetes
<a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/">namespace</a>
with some key resources deployed in it. Each Kubernetes namespace creates a
logical separation within our cluster that provides isolation from any other
namespace.</p>
<p>Once you have created an environment you will be able to perform actions using
the <code>kubectl</code> tool in the namespace you have created.</p>
<h3 id="objective">Objective</h3><p>By the end of this guide you&rsquo;ll have created a Kubernetes namespace ready for
you to <a href="/tasks.html#deploying-a-39-hello-world-39-application-to-the-cloud-platform">deploy an application</a> into.</p>
<h3 id="create-an-environment">Create an environment</h3><p>You create an environment by adding the definition of the environment in YAML
to the following repository, hosted on GitHub:</p>
<p><a href="https://github.com/ministryofjustice/cloud-platform-environments">cloud-platform-environments</a></p>
<p>Adding your environment definition kicks off a pipeline which builds your
environment on the appropriate cluster.</p>
<h4 id="set-up">Set up</h4><p>First we need to clone the repository, change directory and create a new branch:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ git clone git@github.com:ministryofjustice/cloud-platform-environments.git
$ cd cloud-platform-environments
$ git checkout -b &lt;yourBranch&gt;
</code></pre></div><h4 id="the-directory-structure">The directory structure</h4><p>We build new environments by creating a new directory for our environment and
putting the YAML files that define the environment into that directory. To
understand where to create the directory it is useful to understand the overall
structure of the repo:</p>
<div class="highlight"><pre class="highlight plaintext"><code>cloud-platform-environments
├── namespace-resources
└── namespaces
    └── live-1.cloud-platform.service.justice.gov.uk
        ├── kube-system

        ...

        └── user-roles.yaml
</code></pre></div><p><strong>cloud-platform-environments</strong></p>
<p>This is the root of the repo, containing <code>namespaces</code> directory</p>
<p><strong>/namespaces</strong></p>
<p>The namespaces directory contains a directory for each of the clusters that you
can build environments on. Create your environment in the
<code>live-1.cloud-platform.service.justice.gov.uk</code> directory.</p>
<p><strong>/namespaces/live-1.cloud-platform.service.justice.gov.uk/</strong></p>
<p>Within the cluster directory you will generate a directory for your environment
in the format <code>&lt;servicename-env&gt;</code>, for example <code>myapp-dev</code>.</p>
<p><strong>/namespaces/live-1.cloud-platform.service.justice.gov.uk/servicename-env/</strong></p>
<p>The <code>&lt;servicename-env&gt;</code> directory for your environment defines the specific
resources we will create in your namespace. We describe these resources in more
detail in <a href="#how-we-set-up-an-environment">how we set up an environment</a>.</p>
<h4 id="how-we-set-up-an-environment">How we set up an environment</h4><p>To set up an environment we create 5 files in the directory for your namespace:</p>

<ul>
<li><a href="#00-namespace-yaml"><code>00-namespace.yaml</code></a></li>
<li><a href="#01-rbac-yaml"><code>01-rbac.yaml</code></a></li>
<li><a href="#02-limitrange-yaml"><code>02-limitrange.yaml</code></a></li>
<li><a href="#03-resourcequota-yaml"><code>03-resourcequota.yaml</code></a></li>
<li><a href="#04-networkpolicy-yaml"><code>04-networkpolicy.yaml</code></a></li>
</ul>
<p>These files define key elements of the namespace and restrictions we want to
place on it so that we have security and resource allocation properties. We
will use terraform to create these files from templates. We also describe each
of these files <a href="#00-namespace-yaml">in more detail below</a> in case you want to
make future changes.</p>
<h4 id="create-your-namespace-and-namespace-resources">Create your namespace and namespace resources</h4><p>We automate the creation of the namespace resource files using terraform. You will need to install terraform locally:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ brew install terraform
</code></pre></div><p>In each of the template files you need to replace some example values with
information about your namespace, team or app. We do this by running terraform
commands and filling in the values.</p>
<p>These are the inputs for the terraform module, that you will need to fill:</p>

<div style="height:1px;font-size:1px;">&nbsp;</div>
<div class="table-container">
        <table>
          <tr>
<th>Name</th>
<th>Description</th>
<th style="text-align: center">Type</th>
<th style="text-align: center">Default</th>
<th style="text-align: center">Required</th>
</tr>
<tr>
<td>application</td>
<td>The name of your application</td>
<td style="text-align: center">string</td>
<td style="text-align: center">-</td>
<td style="text-align: center">yes</td>
</tr>
<tr>
<td>business-unit</td>
<td>Area of the MOJ responsible for the service</td>
<td style="text-align: center">string</td>
<td style="text-align: center">-</td>
<td style="text-align: center">yes</td>
</tr>
<tr>
<td>cluster</td>
<td>What cluster are you deploying your namespace. i.e cloud-platform-test-1</td>
<td style="text-align: center">string</td>
<td style="text-align: center"><code>cloud-platform-live-1</code></td>
<td style="text-align: center">no</td>
</tr>
<tr>
<td>contact_email</td>
<td>Contact email address for owner of the application</td>
<td style="text-align: center">string</td>
<td style="text-align: center">-</td>
<td style="text-align: center">yes</td>
</tr>
<tr>
<td>environment</td>
<td>A label for your environment (e.g. dev/staging/&hellip;)</td>
<td style="text-align: center">string</td>
<td style="text-align: center">-</td>
<td style="text-align: center">yes</td>
</tr>
<tr>
<td>github_team</td>
<td>This is your team name as defined by the GITHUB api. This has to match the team name on the Github API</td>
<td style="text-align: center">string</td>
<td style="text-align: center">-</td>
<td style="text-align: center">yes</td>
</tr>
<tr>
<td>is-production</td>
<td></td>
<td style="text-align: center">string</td>
<td style="text-align: center"><code>false</code></td>
<td style="text-align: center">no</td>
</tr>
<tr>
<td>namespace</td>
<td>Namespace you would like to create on cluster <application>-<environment>. i.e myapp-dev</td>
<td style="text-align: center">string</td>
<td style="text-align: center">-</td>
<td style="text-align: center">yes</td>
</tr>
<tr>
<td>owner</td>
<td>Who is the owner/Who is responsible for this application</td>
<td style="text-align: center">string</td>
<td style="text-align: center">-</td>
<td style="text-align: center">yes</td>
</tr>
<tr>
<td>source_code_url</td>
<td>Url of the source code for your application</td>
<td style="text-align: center">string</td>
<td style="text-align: center">-</td>
<td style="text-align: center">yes</td>
</tr>

        </table>
      </div>
<div style="height:1px;font-size:1px;">&nbsp;</div>
<p>Run the following commands to create your namespace and these resources files:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ cd cloud-platform-environments/namespace-resources/
$ terraform init
$ terraform apply
</code></pre></div><p>Our terraform module creates the files for a new namespace on the live-1
cluster by default but if you would like to deploy to another cluster you can
use:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ terraform apply -var "cluster=&lt;cluster-name&gt;"
</code></pre></div><p>Fill in your values in response to the prompts.</p>
<p>For <code>var.namespace</code>, this is the name of your (team&rsquo;s) &lsquo;private&rsquo; part of the
cluster. The name of your namespace must be unique across the whole of the
cluster. If you try to create a new namespace using the name of one which
already exists, you will get an error when you try to apply the generated
kubernetes config files (or when our build pipeline applies them on your
behalf).</p>
<p>For &lsquo;real&rsquo; services, this is very unlikely to be a problem - most services have
distinct names, so namespace name conflicts are unlikely. But, if you are
creating a test/dummy namespace in order to learn how the platform works, it&rsquo;s
better to avoid generic names like &#39;dummy&rsquo;, &#39;test&rsquo; or &#39;example&rsquo;. Add something
unique (e.g. your name) to minimise the risk of trying to re-use a name by
mistake.</p>
<p>For <code>var.source_code_url</code>, this should be the URL of an application which is
&#39;cluster-ready&rsquo; to be deployed. If you do not have such an application ready to
go, you can use the reference application which the Cloud Platform team have
prepared <code>git@github.com:ministryofjustice/cloud-platform-reference-app.git</code></p>
<p>Note: The <code>source_code_url</code> is a descriptive label, used by the Cloud Platform
team in supporting your namespace. It does not set up an explicit link between
your namespace and your application&rsquo;s code repository.</p>
<p>At the final prompt &ldquo;Do you want to perform these actions?&rdquo;, enter &ldquo;yes&rdquo;</p>
<p>You can then access your namespace files under
<code>cloud-platform-environments/namespaces/live-1.cloud-platform.service.justice.gov.uk/&lt;your-namespace&gt;</code>,
if satisfied you can then push the changes to your branch and create a pull
request against the
<a href="https://github.com/ministryofjustice/cloud-platform-environments"><code>cloud-platform-environments</code></a>
master repo.</p>
<p>The cloud platform team will review the pull request when it gets opened.  As
soon as the pull request has been approved by the cloud platform team you can
then merge it into the master branch which will kick off the pipeline that
builds the environment. You can check whether the build succeeded or failed in
the <a href="https://mojdt.slack.com/messages/CA5MDLM34/"><code>#cp-build-notification</code></a>
slack channel. This can take about 5 minutes.</p>
<p>Please create <strong>one PR per namespace</strong></p>
<p>i.e. if you need namespaces &#39;myapp-dev&rsquo;, &#39;myapp-staging&rsquo; and &#39;myapp-prod&rsquo;,
please raise a separate PR for each one. This makes it a lot easier for the
cloud platform team to review your PRs.</p>
<h3 id="accessing-your-environments">Accessing your environments</h3><p>Once the pipeline has completed you will be able to check that your environment
is available by running:</p>
<p><code>$ kubectl get namespaces</code></p>
<p>This will return a list of the namespaces within the cluster, and you should
see yours in the list.</p>
<p>You can now run commands in your namespace by appending the <code>-n</code> or
<code>--namespace</code> flag to <code>kubectl</code>. So for instance, to see the pods running in
our <code>myenv-dev</code> namespace, we would run:</p>
<p><code>$ kubectl get pods -n myenv-dev</code> or</p>
<p><code>$ kubectl get pods --namespace myenv-dev</code></p>
<h3 id="next-steps">Next steps</h3><p><a href="/tasks.html#creating-an-ecr-repository">Create an ECR repository</a> to push your application docker image to.</p>
<p>Then you can try <a href="/tasks.html#deploying-a-39-hello-world-39-application-to-the-cloud-platform">deploying an app to Kubernetes manually</a>
by writing some custom YAML files or <a href="/archive.html#deploying-an-application-to-the-cloud-platform-with-helm">deploying an app with Helm</a>,
a Kubernetes <a href="https://helm.sh/">package manager</a>.</p>
<h3 id="more-information-on-environment-definition">More information on environment definition</h3><p>To set up an environment we create 5 files in that directory:</p>

<ul>
<li><a href="#00-namespace-yaml"><code>00-namespace.yaml</code></a></li>
<li><a href="#01-rbac-yaml"><code>01-rbac.yaml</code></a></li>
<li><a href="#02-limitrange-yaml"><code>02-limitrange.yaml</code></a></li>
<li><a href="#03-resourcequota-yaml"><code>03-resourcequota.yaml</code></a></li>
<li><a href="#04-networkpolicy-yaml"><code>04-networkpolicy.yaml</code></a></li>
</ul>
<p>These files define key elements of the namespace and restrictions we want to
place on it so that we have security and resource allocation properties. We
will use terraform to create these files from templates. We also describe each
of these files in more detail below in case you want to make changes.</p>
<h4 id="00-namespace-yaml"><code>00-namespace.yaml</code></h4><p>The <code>00-namespace.yaml</code> file defines the namespace so that the cluster
Kubernetes knows to create a namespace and what to call it.</p>
<div class="highlight"><pre class="highlight plaintext"><code>apiVersion: v1
kind: Namespace
metadata:
  name: myapp-dev ### This is where you will define your &lt;servicename-env&gt;
  labels:
    name: myapp-dev ### Also your &lt;servicename-env&gt;
</code></pre></div><h4 id="01-rbac-yaml"><code>01-rbac.yaml</code></h4><p>We will also create a <code>RoleBinding</code> resource by adding the <code>01-rbac.yaml</code> file.
This will provide us with <a href="https://kubernetes.io/docs/admin/authorization/rbac/">access
policies</a> on the
namespace we have created in the cluster.</p>
<p>A role binding resource grants the permissions defined in a role to a user or
set of users. A role can be another resource we can create but in this instance
we will reference a Kubernetes default role <code>ClusterRole - admin</code>.</p>
<p>This <code>RoleBinding</code> resource references the <code>ClusterRole - admin</code> to provide
admin permissions on the namespace to the set of users defined under
<code>subjects</code>. In this case, the <code>&lt;yourTeam&gt;</code> GitHub group will have admin access
to any resources within the namespace <code>myapp-dev</code>.</p>
<div class="highlight"><pre class="highlight plaintext"><code>kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: myapp-dev-admins  ### Your namespace with `-admin` e.g. `&lt;servicename-env&gt;-admin`
  namespace: myapp-dev ### Your namespace `&lt;servicename-env&gt;`
subjects:
  - kind: Group
    name: "github:&lt;yourTeam&gt;" ### Make this the name of the GitHub team you want to give access to
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: admin
  apiGroup: rbac.authorization.k8s.io
</code></pre></div><h4 id="02-limitrange-yaml"><code>02-limitrange.yaml</code></h4><p>As we are working on a shared Kubernetes cluster it is useful to put in place
limits on the resources that each namespace, pod and container can use. This
helps to stop us accidentally entering a situation where one service impacts
the performance of another through using more resources than are available.</p>
<p>The first Kubernetes limit we can use is a
<a href="https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">LimitRange</a>
which we define in <code>02-limitrange.yaml</code>.</p>
<p>The LimitRange object specifies two key resource limits on containers,
<code>defaultRequest</code> and <code>default</code>. <code>defaultRequest</code> is the memory and cpu a
container will request on startup. This is what the Kubernetes scheduler uses
to determine whether there is enough space on the cluster to run your
application and what your application will start up with when it is created.
<code>default</code> is the limit at which your application will be killed or throttled.</p>
<p>In <code>02-limitrange.yaml</code> you need to change the value of the <code>namespace</code> key to
match the name of your namespace in the form <code>&lt;service-env&gt;</code>. We have set
default values for the limits in the templates. As you learn more about the
behaviour of your applications on Kubernetes you can change them.</p>
<div class="highlight"><pre class="highlight plaintext"><code>apiVersion: v1
kind: LimitRange
metadata:
  name: limitrange
  namespace: myapp-dev ### Your namespace `&lt;servicename-env&gt;`
spec:
  limits:
  - default:
      cpu: 1000m
      memory: 2Gi
    defaultRequest:
      cpu: 100m
      memory: 128Mi
    type: Container
</code></pre></div><h4 id="03-resourcequota-yaml"><code>03-resourcequota.yaml</code></h4><p>The
<a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">ResourceQuota</a>
object allows us to set a total limit on the resources used in a namespace. As
with the LimitRange, the <code>requests.cpu</code> and <code>requests.memory</code> limits set how
much the namespace will request on creation. The <code>limits.cpu</code> and
<code>limits.memory</code> define the overall hard limits for the namespace.</p>
<p>In <code>03-resourcequota.yaml</code> you need to change the value of the <code>namespace</code> key
to match the name of your namespace in the form <code>&lt;service-env&gt;</code>. We have set
default values for the limits in the templates. As you learn more about the
behaviour of your applications on Kubernetes you can change them.</p>
<div class="highlight"><pre class="highlight plaintext"><code>apiVersion: v1
kind: ResourceQuota
metadata:
  name: namespace-quota
  namespace: myapp-dev ### Your namespace `&lt;servicename-env&gt;`
spec:
  hard:
    requests.cpu: 4000m
    requests.memory: 8Gi
    limits.cpu: 6000m
    limits.memory: 12Gi
</code></pre></div><h4 id="04-networkpolicy-yaml"><code>04-networkpolicy.yaml</code></h4><p>The
<a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">NetworkPolicy</a>
object defines how groups of pods are allowed to communicate with each other
and other network endpoints. By default pods are non-isolated, they accept
traffic from any source. We apply a network policy to restrict where traffic
can come from, allowing traffic only from the <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">ingress
controller</a>
and other pods in your namespace.</p>
<p>In <code>04-networkpolicy.yaml</code> you need to change the value of the <code>namespace</code> key
to match the name of your namespace in the form <code>&lt;service-env&gt;</code>.</p>
<div class="highlight"><pre class="highlight plaintext"><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default
  namespace: myapp-dev ### Your namespace `&lt;servicename-env&gt;`
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector: {}
---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-ingress-controllers
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          component: ingress-controllers
</code></pre></div>
<h2 id="creating-an-ecr-repository">Creating an ECR repository</h2><h3 id="creating-an-ecr-repository-introduction">Introduction</h3><p>This guide will guide you through the creation of an ECR (Elastic Container Registry) repository for your application&rsquo;s docker image.</p>
<p>AWS resources are provisioned through the <a href="https://github.com/ministryofjustice/cloud-platform-environments/">cloud-platform-environments</a> repository, per environment. Your application might be using multiple namespaces, however you typically only need one image repository and once created in any of them you can copy credentials for it to the others via <code>kubectl get/create secret</code>.</p>
<h4 id="creating-the-registry">Creating the registry</h4><p>1. In order to create the ECR Docker registry, git clone the repo and create a new branch.</p>
<div class="highlight"><pre class="highlight shell"><code>
  <span class="nv">$ </span>git clone git@github.com:ministryofjustice/cloud-platform-environments.git <span class="c">###git clone repo</span>

  <span class="nv">$ </span><span class="nb">cd </span>cloud-platform-environments <span class="c">### navigate into cloud-platform-environments directory.</span>

  <span class="nv">$ </span>git checkout <span class="nt">-b</span> add_ecr   <span class="c">### create and checkout new branch.</span>

</code></pre></div><p>2. You will need to navigate to your service&rsquo;s directory which is located in the namespaces directory. Create a directory named &ldquo;resources&rdquo;.</p>
<div class="highlight"><pre class="highlight shell"><code>
  <span class="nv">$ </span><span class="nb">cd </span>namespaces/live-1.cloud-platform.service.justice.gov.uk/<span class="nv">$your_service</span>  <span class="c">###navigate to your service's directory.</span>

  <span class="nv">$ </span>mkdir resources <span class="c">### make directory called resources</span>

  <span class="nv">$ </span><span class="nb">cd </span>resources

</code></pre></div><p>3. Create a terraform file within the resources directory.</p>
<div class="highlight"><pre class="highlight shell"><code>
  <span class="nv">$ </span>vi ecr.tf  <span class="c">###create a terraform file.</span>

</code></pre></div><p>4. Adapt the definition from the example described in the <a href="https://github.com/ministryofjustice/cloud-platform-terraform-ecr-credentials/tree/master/examples">cloud-platform-terraform-ecr-credentials module</a>; make sure you adjust the values of <code>team_name</code>, <code>repo_name</code> <code>name</code> and <code>namespace</code> to what is appropriate for your environment.</p>
<p>Note: A default ECR lifecycle policy is set, that will only keep the 40 most recent versions of an image. More infomation on the ECR lifecycle policy avalible <a href="/archive.html#ecr-lifecycle-policy">here.</a></p>
<p>5. git add, commit and push to your branch.</p>
<div class="highlight"><pre class="highlight shell"><code>
  <span class="nv">$ </span>git add ecr.tf

  <span class="nv">$ </span>git commit

  <span class="nv">$ </span>git push

</code></pre></div><p>6. Once your request has been approved and the branches are merged, it will trigger our build pipeline which will apply the Terraform module and create the resources.</p>
<p>For more information about the terraform module being used, please read the documentation <a href="https://github.com/ministryofjustice/cloud-platform-terraform-ecr-credentials">here</a>.</p>
<h3 id="accessing-the-credentials">Accessing the credentials</h3><p>The end result will be a kubernetes <code>Secret</code> inside your environment, called <code>example-team-ecr-credentials-output</code> (or whatever you changed that to); the secret holds IAM access keys to authenticate with the registry and the actual repository URL.</p>
<p>Note: For <code>example-team-ecr-credentials-output</code> you should put whatever you used as the value of the <code>name</code> property of the <code>kubernetes_secret</code> resource in the <code>ecr.tf</code> file you created previously.</p>
<p>To retrieve the credentials:
<code>
kubectl -n &lt;namespace_name&gt; get secret example-team-ecr-credentials-output -o yaml
</code></p>
<p>The values in kubernetes <code>Secrets</code> are always <code>base64</code> encoded so you will have to decode them before you can use them outside kubernetes. Inside the cluster, the nodes already have access to the ECR so you don&rsquo;t need to make any changes.</p>
<p>This can be done at the command line using the following:
<code>
echo QWxhZGRpbjpvcGVuIHNlc2FtZQ== | base64 --decode; echo
</code></p>
<h4 id="setting-up-circleci">Setting up CircleCI</h4><p>In your CircleCI project, go to the settings (the cog icon) and select &lsquo;AWS Permissions&rsquo; from the left hand menu. Fill in the IAM credentials and CircleCI will be able to use ECR images. For more information please see <a href="https://circleci.com/docs/2.0/private-images/">the official docs</a>.</p>
<h3 id="creating-an-ecr-repository-next-steps">Next steps</h3><p>Try <a href="/archive.html#deploying-an-application-to-the-cloud-platform-with-helm">deploying an app</a> with <a href="https://helm.sh/">Helm</a>, a Kubernetes package manager, or <a href="/tasks.html#deploying-a-39-hello-world-39-application-to-the-cloud-platform">deploying manually</a> by writing some custom YAML files.</p>

<h2 id="adding-aws-resources-to-your-environment">Adding AWS resources to your environment</h2><p>Through the <a href="https://github.com/ministryofjustice/cloud-platform-environments/">cloud-platform-environments</a> repository, you can provision AWS resources for your environments. This is done using terraform and more specifically, terraform modules we provide for use on the Cloud Platform.</p>
<p>The documentation for the modules lives in each module&rsquo;s repository and you can find a list of the available ones below.</p>
<h3 id="available-modules">Available modules</h3><p>The updated list of terraform modules provided by the MoJ are available here: <a href="https://github.com/ministryofjustice/cloud-platform#terraform-modules">Terraform Modules</a></p>
<div class="highlight"><pre class="highlight plaintext"><code>!!!  WARNING  !!!

Be aware that the latest versions of these modules will only be compatible
with Live-1.

If you are planing on deploying them against Live-0, please read their
respective READMEs carefully for instruction.

</code></pre></div><h3 id="usage">Usage</h3><p>In each terraform module repository, you will find a directory named <code>example</code> which includes sample configuration for use in Cloud Platform.</p>
<p>In your namespace&rsquo;s path in the <a href="https://github.com/ministryofjustice/cloud-platform-environments/">cloud-platform-environments</a> repository, create a directory called <code>resources</code> (if you have not created one already) and refer to the module&rsquo;s example to define your resources.</p>
<p>Each example will have some global configuration defined, however, this should only be declared once, regardless of the number of modules used:</p>
<div class="highlight"><pre class="highlight plaintext"><code>terraform {
  backend "s3" {}
}

provider "aws" {
  region = "eu-west-1"
}
</code></pre></div><p>Additionally, some example might define variables; again, these should only be declared once per namespace:</p>
<div class="highlight"><pre class="highlight plaintext"><code>variable "cluster_name" {}

variable "cluster_state_bucket" {}
</code></pre></div><p>The main README file of each module repository will list all the available configuration options that can be passed to the module.</p>
<h4 id="outputs">Outputs</h4><p>Each module will have its own outputs. These expose useful information, such as endpoints, credentials etc. The module examples all use a common approach: they employ the <code>kubernetes_secret</code> terraform resource to push the outputs straight into your environment in the form of a <code>Secret</code> which you could then extract information from or directly reference in <code>Pods</code>.</p>
<p>This is currently the only supported way of accessing terraform outputs.</p>
<h4 id="versioning">Versioning</h4><p>All modules are versioned. This allows us to implement changes without breaking existing resources. To use a specific version of a module you need to define it in the <code>source</code> attribute by specifying the <code>ref</code> attribute in the query string of the source URL:</p>
<div class="highlight"><pre class="highlight plaintext"><code>module "my_module" {
  source = "https://github.com/ministryofjustice/cloud-platform-terraform-ecr-credentials?ref=1.0"
}
</code></pre></div><p>Make sure that you have checked the releases page of a module and that you are using the latest of them in your configuration.</p>
<p>Upgrading to a new major version usually means that the configured resource will have to be re-created by terraform.</p>
<p>Refer to the <a href="http://terraform.io/docs/modules">terraform documentation on modules</a> for more information on usage.</p>


<h2 id="deploying-applications">Deploying Applications</h2>
<p><h3 id="deploying-a-39-hello-world-39-application-to-the-cloud-platform">Deploying a &lsquo;Hello World&rsquo; application to the Cloud Platform</h3><p><a href="../images/k8s-cluster-application-deployment-process.png" target="_blank" rel="noopener noreferrer"><img src="/images/k8s-cluster-application-deployment-process.png" alt="Deployment Process Diagram" /></a></p>
<h4 id="overview">Overview</h4><p>The aim of this guide is to walkthrough the process of deploying an application into the Cloud Platform.</p>
<p>This guide uses a pre-configured <a href="https://github.com/ministryofjustice/cloud-platform-helloworld-ruby-app">&ldquo;Hello World&rdquo; application</a> as an example of how to deploy your own. This application merely returns a static HTML response, and has no dependencies. Later examples will use more representative applications.</p>
<p>The process we will follow consists of the following stages:</p></p>

<ul>
<li>Build a docker image from the demo application</li>
<li>Tag the image and push it to your ECR</li>
<li>Edit kubernetes config files</li>
<li>Apply the config files to make the cluster run our application</li>
</ul>

<p>The process of building an image and pushing it to an ECR will normally be carried out by a build pipeline. For this initial walkthrough, we will go through these steps manually. Later we will go through an example of setting up a <a href="https://circleci.com">CircleCI</a> job to do this automatically. The steps are similar if you&rsquo;re using other CI/CD tools such as <a href="https://travis-ci.org">TravisCI</a>.</p>

<h4 id="prerequisites">Prerequisites</h4><p>This guide assumes the following:</p>

<ul>
<li><a href="https://www.docker.com">Docker</a> is installed and configured.</li>
<li>Kubectl is installed and configured (<code>brew install kubectl</code> on a Mac with <a href="https://brew.sh">Homebrew</a> installed)</li>
<li>AWS CLI is installed (<code>brew install awscli</code> on a Mac with <a href="https://brew.sh">Homebrew</a> installed)</li>
<li>You have <a href="/tasks.html#creating-a-cloud-platform-environment">created an environment for your application</a></li>
<li>You have <a href="/tasks.html#creating-an-ecr-repository">created an Amazon ECR</a> to host your docker image</li>
</ul>
<h4 id="step-1-build-your-docker-image">Step 1 - Build your docker image</h4>

<ul>
<li><p>Clone the <a href="https://github.com/ministryofjustice/cloud-platform-helloworld-ruby-app">demo application</a></p>
<p>git clone https://github.com/ministryofjustice/cloud-platform-helloworld-ruby-app
  cd cloud-platform-helloworld-ruby-app</p></li>
<li><p>Build the docker image</p>
<p>docker build -t [ECR Team Name]/[ECR Repository Name] .</p></li>
</ul>

<p>The <code>ECR Team Name</code> and <code>ECR Repository Name</code> must match the <code>team_name</code> and <code>repo_name</code> values you entered when you created the ECR via <a href="https://github.com/ministryofjustice/cloud-platform-environments">cloud-platform-environments</a> Github repository.</p>

<p>You can find them in the file <code>namespaces/live-1.cloud-platform.service.justice.gov.uk/[YOUR ENVIRONMENT]/resources/ecr.tf</code>.</p>
<p><h5 id="amazon-ecr-terminology">Amazon ECR Terminology</h5><p>Amazon ECR uses the terms <code>repository</code> and <code>image</code> in a rather confusing way. Normally, you would think of a docker image repository as holding multiple images, each with a different name, where each image can have multiple tags. Amazon ECR conflates the repository and image - i.e. you can only push images with the same name to a given ECR.</p>
<p>So, if you created your ECR using the team_name <code>davids-dummy-team</code> and repo_name <code>davids-dummy-app</code>, then you can only push images to the ECR if they are named <code>davids-dummy-team/davids-dummy-app:[something]</code>. You are free to change the tag of the image (shown as [something], here), and some teams overload the tag value as a way to store multiple completely different docker images in a single ECR.</p>
<h4 id="step-2-push-the-image-to-your-ecr">Step 2 - Push the image to your ECR</h4><h5 id="authenticating-to-your-docker-image-repository">Authenticating to your docker image repository</h5><p>You must authenticate to the docker image repository before you can push an image to it.</p>
<p>To authenticate to your ECR, you will need the <code>access_key_id</code> and <code>secret_access_key</code> which were created for you when you created your ECR. To retrieve these, see the <a href="/tasks.html#accessing-the-credentials">this section</a> of this guide.</p>
<p><em>tl;dr</em> use this command:</p>
<div class="highlight"><pre class="highlight plaintext"><code>  kubectl -n [namespace_name] get secret [name of your secret] -o yaml
</code></pre></div><p>Don&rsquo;t forget to base64 decode the <code>access_key_id</code> and <code>secret_access_key</code> values before using them.</p>
<div class="highlight"><pre class="highlight plaintext"><code>  echo &#39;your_access_key_id_value&#39; | base64 --decode
</code></pre></div><p>Once you have your <code>access_key_id</code> and <code>secret_access_key</code>, set up an AWS profile using the AWS cli tool.</p>
<div class="highlight"><pre class="highlight plaintext"><code>  aws configure
</code></pre></div><p>Supply your credentials when prompted.</p>
<p>This guide assumes you are using these credentials in your <code>default</code> AWS profile. If you have used a different name for this AWS profile, please add <code>--profile [YOUR PROFILE]</code> to all of the following AWS commands.</p>
<h5 id="authenticating-with-the-repository">Authenticating with the repository</h5><p>Use the following command to login to Amazon ECR</p>
<div class="highlight"><pre class="highlight plaintext"><code>  $(aws ecr get-login --no-include-email --region eu-west-2)
</code></pre></div><p>Note: The output of the <code>aws ecr...</code> command is a long <code>docker login...</code> command. Including the <code>$(...)</code> around the command executes this output in the context of the current shell</p>
<p>The output of the above should include <code>Login Succeeded</code> to confirm you have authenticated to the docker image repository.</p>
<p>These credential are valid for 12 hours. So, if you are working through this example over a longer period, you will have to login again, e.g. the following day.</p>
<h5 id="pushing-your-docker-image-to-the-ecr">Pushing your docker image to the ECR</h5><p>All of the MoJ Digital docker images are stored within the same Cloud Platform AWS account (cloud-platform-aws).</p>
<p>Your specific ECR will be:</p>
<div class="highlight"><pre class="highlight plaintext"><code>  754256621582.dkr.ecr.eu-west-2.amazonaws.com/[team_name]/[repo_name]
</code></pre></div><p>Where <code>team_name</code> and <code>repo_name</code> are the values from your <code>ecr.tf</code> file.</p>
<p>Ensure the Docker image for your application has been built and is stored locally on your machine.</p>
<div class="highlight"><pre class="highlight plaintext"><code>  docker build -t [team_name]/[repo_name] .
</code></pre></div><p>Now we need to tag the image so it can be pushed into the correct repository.</p>
<div class="highlight"><pre class="highlight plaintext"><code>  docker tag [team_name]/[repo_name]:latest 754256621582.dkr.ecr.eu-west-2.amazonaws.com/[team_name]/[repo_name]:latest
</code></pre></div><p>Finish by running the last command to push the image to your repository.</p>
<div class="highlight"><pre class="highlight plaintext"><code>docker push 754256621582.dkr.ecr.eu-west-2.amazonaws.com/[team_name]/[repo_name]:latest
</code></pre></div><h4 id="step-3-configure-your-namespace-in-the-kubernetes-cluster">Step 3 - Configure your namespace in the Kubernetes Cluster</h4><p>To deploy an application to the Cloud Platform, a number of deployment files must first be configured. You can find examples of these in the <code>kubectl_deploy</code> directory of the <a href="https://github.com/ministryofjustice/cloud-platform-helloworld-ruby-app">demo application</a>, but you will need to edit your copy to replace some of the values to use your kubernetes cluster environment and docker image.</p>
<p><em>Tip:</em> You can find more deployment config info <a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/">in the kubernetes developer documentation</a>.</p>
<h5 id="deployment-yaml">deployment.yaml</h5><div class="highlight"><pre class="highlight plaintext"><code>apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: helloworld-rubyapp
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: helloworld-rubyapp
    spec:
      containers:
      - name: rubyapp
        image: 754256621582.dkr.ecr.eu-west-2.amazonaws.com/davids-dummy-team/davids-dummy-app:latest
        ports:
        - containerPort: 4567
</code></pre></div><p>This file tells Kubernetes to run a single pod (<code>replicas: 1</code>) containing a single container based on a specific docker image from your ECR.</p>
<p>Change the image value to refer to the image you pushed to your ECR in the earlier step.</p>
<p>The <code>service.yaml</code> and <code>ingress.yaml</code> files make it possible to access your application from the outside world.</p>
<h5 id="service-yaml">service.yaml</h5><p>Service files are used to specify port and protocol information for your application and are also used to bundle together the set of pods created by the deployment.</p>
<p>This exposes port 4567 internally to your namespace. i.e. it enables pods and other objects within your namespace to connect to port 4567 of your container.</p>
<div class="highlight"><pre class="highlight plaintext"><code>apiVersion: v1
kind: Service
metadata:
  name: rubyapp-service
  labels:
    app: rubyapp-service
spec:
  ports:
  - port: 4567
    name: http
    targetPort: 4567
  selector:
    app: helloworld-rubyapp
</code></pre></div><p>The value of <code>spec/selector/app</code> must be the same as <code>spec/template/metadata/labels/app</code> in the <code>deployment.yml</code> file.</p>
<p><em>Tip:</em> You can find more info on service definition in the <a href="https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/">kubernetes docs</a>.</p>
<h5 id="ingress-yaml">ingress.yaml</h5><p>Ingress files are to use to define external access to the application.</p>
<p>This creates an <a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/">ingress controller</a> to enable network connections from outside of the cluster.</p>
<p>Note: Because we are specifying <code>http</code>, this ingress controller will expose port 80, and will redirect connections to port 4567 of the named service.</p>
<div class="highlight"><pre class="highlight plaintext"><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: helloworld-rubyapp-ingress
spec:
  tls:
  - hosts:
    - helloworld-rubyapp.apps.live-1.cloud-platform.service.justice.gov.uk
  rules:
  - host: helloworld-rubyapp.apps.live-1.cloud-platform.service.justice.gov.uk
    http:
      paths:
      - path: /
        backend:
          serviceName: rubyapp-service
          servicePort: 4567
</code></pre></div><p>The value of <code>serviceName</code> and <code>servicePort</code> must be the same as those specified in the <code>service.yml</code> file.</p>
<p>Change the <code>helloworld-rubyapp</code> prefix of the <code>host</code> string to the value you want to use as the hostname part of the URL on which your application will be available to the world (do not change the <code>.apps.live-1.cloud-platform...</code> part).</p>
<p><em>Tip:</em> You can find more info on ingress in the <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">kubernetes docs</a></p>
<h4 id="step-4-deploy-the-application">Step 4 - Deploy the application</h4><p>With all of the deployment files configured, you can now deploy your application to the Cloud Platform.</p>
<p>Start by listing the namespaces on the cluster you are connected to:</p>
<div class="highlight"><pre class="highlight plaintext"><code>  kubectl get namespaces
</code></pre></div><p>The list that gets returned should include the one you <a href="/tasks.html#creating-a-cloud-platform-environment">created earlier</a>, here we assume it is called <code>davids-dummy-dev</code>. Please change that to whatever your namespace (environment) is called, in all of the following commands.</p>
<p>To deploy your application run the following command. This command assumes that the current directory is the root directory of your working copy of the <a href="https://github.com/ministryofjustice/cloud-platform-helloworld-ruby-app">demo application</a>. i.e. <code>kubectl_deploy</code> points to the directory where the deployment files are stored.</p>
<div class="highlight"><pre class="highlight plaintext"><code>  kubectl create --filename kubectl_deploy --namespace davids-dummy-dev
</code></pre></div><p>You have to specify the namespace you want to deploy to, this should be the namespace of the environment you created.</p>
<p>Confirm the deployment with:</p>
<div class="highlight"><pre class="highlight plaintext"><code>  kubectl get pods --namespace davids-dummy-dev
</code></pre></div><h4 id="interacting-with-the-application">Interacting with the application</h4><p>With the application deployed into the Cloud Platform, there are a few ways of managing it:</p></p>

<ul>
<li><strong>View pods</strong> - <code>kubectl get pods --namespace davids-dummy-dev</code></li>
<li><strong>Check host</strong> - <code>kubectl get ingress --namespace davids-dummy-dev</code></li>
<li><strong>Delete application</strong> - <code>kubectl delete --filename kubectl_deploy --namespace davids-dummy-dev</code></li>
<li><strong>Shell into container</strong> - <code>kubectl exec --stdin --tty --namespace davids-dummy-dev [POD-NAME] -- /bin/sh</code></li>
</ul>

<p>For <code>[POD-NAME]</code> use the value returned by the <code>kubectl get pods...</code> command</p>

<p><em>Tip:</em> You can find more about the <code>kubectl</code> command <a href="https://kubernetes.io/docs/reference/kubectl/overview/">here</a></p>

<p>You should be able to view the app. at the following URL:</p>

<div class="highlight"><pre class="highlight plaintext"><code>  curl -L https://helloworld-rubyapp.apps.live-1.cloud-platform.service.justice.gov.uk
</code></pre></div><p>Don&rsquo;t forget to change <code>helloworld-rubyapp</code> to whatever hostname you chose earlier.</p>
<p>You need the <code>-L</code> flag to make curl follow the 308 redirect response that it will receive from the ingress controller. If you view the URL in a web browser, it should just work.</p>
<p>If you are wondering why https &#39;just works&rsquo;, there is some magic behind the scenes whereby a LetsEncrypt SSL certificate is created for you, and applied to your ingress. A future user guide article will describe this in more detail.</p>

<h3 id="deploying-a-multi-container-application-to-the-cloud-platform">Deploying a multi-container application to the Cloud Platform</h3><h4 id="deploying-a-multi-container-application-to-the-cloud-platform-overview">Overview</h4><p>This section goes through the process of deploying a <a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app">demo application</a> consisting of several components, each running in its own container.</p>
<p>Please see the <a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app#multi-container-demo-application">application README</a> for a description of the different components, and how they connect. You can also run the application locally via docker-compose to confirm that it works as it should.</p>
<h4 id="running-in-the-kubernetes-cluster">Running in the Kubernetes Cluster</h4><p>In the <a href="https://github.com/ministryofjustice/cloud-platform">Cloud Platform</a> kubernetes cluster, the application will be set up like this:</p>
<p><a href="../images/multi-container-k8s.png" target="_blank" rel="noopener noreferrer"><img src="/images/multi-container-k8s.png" alt="Multi-container architecture diagram" /></a></p>
<p>Each container needs a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a> which will contain a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/">Pod</a>. <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services</a> make pods available on the cluster&rsquo;s internal network, and an <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress</a> exposes one or more services to the outside world.</p>
<h4 id="create-an-rds-instance">Create an RDS instance</h4><p>The application database will be an Amazon RDS instance. To create this, refer to the <a href="https://github.com/ministryofjustice/cloud-platform-terraform-rds-instance">cloud platform RDS</a> repository, and create a terraform file in your sub-directory of the <a href="https://github.com/ministryofjustice/cloud-platform-environments">cloud platform environments</a> repository (you will need to raise a PR for this, and get the cloud platform team to approve it).</p>
<p>For more information see <a href="/tasks.html#adding-aws-resources-to-your-environment">Adding AWS resources to your environment</a>.</p>
<p>The <a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app">demo application</a>, and this guide, assumes a DATABASE_URL environment variable, exported by the terraform RDS module as follows:</p>
<div class="highlight"><pre class="highlight plaintext"><code># rds.tf
...
data {
  url = postgres://${module.module_name.database_username}:${module.module_name.database_password}@${module.module_name.rds_instance_endpoint}/${module.module_name.database_name}
}
...
</code></pre></div><p>Please ensure that your <code>rds.tf</code> file exports a database <code>url</code> value in this way (changing <code>module_name</code> to match the name you use in your <code>rds.tf</code> file).</p>
<h4 id="build-docker-images-and-pushing-to-ecr">Build docker images and pushing to ECR</h4><p>As before, we need to build docker images which we will push to our <a href="https://aws.amazon.com/ecr/">Amazon ECR</a>.</p>
<p>Please carry out the following steps on your own working copy of the <a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app">demo application</a>.</p>
<p>For <code>team_name</code> and <code>repo_name</code> please use the values from your <code>ecr.tf</code> file, when you <a href="/tasks.html#creating-an-ecr-repository">created your ECR</a>.</p>
<div class="highlight"><pre class="highlight plaintext"><code>cd rails-app
docker build -t [team_name]/[repo_name]:rails-app .
docker tag [team_name]/[repo_name]:rails-app 754256621582.dkr.ecr.eu-west-2.amazonaws.com/[team_name]/[repo_name]:rails-app-1.0
docker push 754256621582.dkr.ecr.eu-west-2.amazonaws.com/[team_name]/[repo_name]:rails-app-1.0
</code></pre></div><p>Note that we are overloading the tag value to push multiple different containers to a single Amazon ECR. This is because of a quirk in the way Amazon ECR refers to <code>image repositories</code> and <code>images</code>.</p>
<p>Repeat the steps above for the <code>content-api</code> and <code>worker</code> sub-directories (changing <code>rails-app</code> as appropriate, in the commands).</p>
<p>The <code>makefile</code> in the <a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app">demo application</a> contains commands to make this process easier. Don&rsquo;t forget to edit the values for <code>TEAM_NAME</code>, <code>REPO_NAME</code> and <code>VERSION</code> appropriately.</p>
<h4 id="kubernetes-configuration">Kubernetes configuration</h4><p>As per the diagram, we need to configure six objects in kubernetes - 3 deployments, 2 services and 1 ingress.</p>
<p>You can see these YAML config files in the <code>kubernetes_deploy</code> directory of the <a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app">demo application</a>.</p>
<p>Note: The yaml files in the github repository have the namespace name <code>davids-dummy-dev</code>, team name <code>davids-dummy-team</code> and application name <code>davids-dummy-app</code>. You will need to change these to the corresponding values for your situation, and also the full names of your docker images.</p>
<p>You may also need to change the <code>host</code> entry in the <code>ingress.yaml</code> file, if someone else has deployed an instance of the demo application using the same hostname.</p>
<p>In <code>rails-app-deployment.yaml</code> and <code>worker-deployment.yaml</code> you can see the configuration for two environment variables:</p>

<ul>
<li><code>DATABASE_URL</code> is retrieved from the kubernetes secret which was created when the RDS instance was set up</li>
<li><code>CONTENT_API_URL</code> uses the name and port defined in <code>content-api-service.yaml</code></li>
</ul>
<h4 id="deploying-to-the-cluster">Deploying to the cluster</h4><p>After you have built and pushed your docker images, and made the corresponding changes to the <code>kubernetes_deploy/*.yaml</code> files, you can apply the configuration to your namespace in the kubernetes cluster:</p>
<div class="highlight"><pre class="highlight plaintext"><code>  kubectl apply --filename kubernetes_deploy --namespace [your namespace]
</code></pre></div><h4 id="deploying-a-multi-container-application-to-the-cloud-platform-interacting-with-the-application">Interacting with the application</h4><p>You should be able to view the application in your browser at:</p>
<div class="highlight"><pre class="highlight plaintext"><code>  https://multi-container-demo.apps.live-1.cloud-platform.service.justice.gov.uk/
</code></pre></div><p>It should behave in the same way as when you were running it locally via docker-compose.</p>

<h2 id="how-do-i-run-rails-database-migrations">How do I run Rails database migrations?</h2><p>The short and naive answer is that you create a Kubernetes &lsquo;Job&rsquo; yaml file, using your rails application Docker image, and run <code>rails db:migrate</code></p>
<p>Here is an <a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app/blob/master/kubernetes_deploy/rails-migrations-job.yaml">example</a></p>
<p>This approach may work for your service, but there are potential issues that
you should be aware of. In particular, kubernetes makes no guarantees about
when your job will run, in relation to when the rest of your service starts.</p>
<p>So, it is possible for your application containers to be replaced with the
latest version <strong>before</strong> your migration job has run. In this case, there will
be a period of time when your updated application code may try to interact with
the database while it is still in the &#39;old&rsquo; state (e.g. before your migration
added a new column). This might cause users to see errors.</p>
<p>Alternatively, it is possible that your migration job will complete before all
of your application pods have been replaced with pods running the latest code.
So, you might have application code which expects to see the old database
structure, but which are running against an updated database. If your migration
removed a column, or otherwise updated the database in ways which are
incompatible with the old version of the application code, then this could
cause users to see errors.</p>
<p>These problems are not unique to Kubernetes. They occur in any scenario where a
mismatch between the state of your database and the state of your application
code can cause errors.</p>
<p>In the majority of cases, the window during which errors might occur is likely
to be so brief that no users will be affected, but it is worth being aware of,
even if you decide to just accept that it <strong>might</strong> happen.</p>
<p>Some strategies to protect against this include:</p>

<ul>
<li>If planned downtime is a possibility, put the service into maintenance mode
before the migration, and bring it back into service when you are confident
that the application code and the database are both in the appropriate state.</li>
<li>Create a healthcheck endpoint in your application code which tests the
state of the database and fails if the database structure is not as
expected, so that kubernetes does not start any new application pods until
after the migration has completed (although this will not prevent problems if
the new database structure is incompatible with the old application code).</li>
<li>Break your migration into several stages such that, at every stage, your
application code works with both the current and next/previous state of the
database.</li>
</ul>
<h3 id="do-not-run-migrations-on-container-startup">Do not run migrations on container startup</h3><p>A pattern to avoid is having your application container start up using a
command like this:</p>
<div class="highlight"><pre class="highlight plaintext"><code>bundle exec rails db:migrate &amp;&amp; bundle exec rails server
</code></pre></div><p>In general, you should avoid overloading container startup in this way. If your
container takes a long time to start up (e.g. if the migration task takes a
long time to complete, in this example) then the cluster might assume your pod
has failed, and it will kill it and start a new one. In the worst cases, this
can cause your application to go into a crash loop such that it never starts at
all.</p>
<p>Keep your containers dedicated to a single purpose and, if you need to run
one-off jobs, use a dedicated job or other kubernetes object to do so.</p>
<h3 id="further-reading">Further reading</h3><p>The following StackOverflow threads discuss these issues:</p>

<ul>
<li><a href="https://stackoverflow.com/questions/50218376/managing-db-migrations-on-kubernetes-cluster">Managing DB migrations on Kubernetes cluster</a></li>
<li><a href="https://stackoverflow.com/questions/48877182/kubernetes-rolling-deployments-and-database-migrations">Kubernetes rolling deployments and database migrations</a></li>
<li><a href="https://stackoverflow.com/questions/37058812/how-best-to-run-one-off-migration-tasks-in-a-kubernetes-cluster">How best to run one-off migration tasks in a kubernetes cluster</a></li>
</ul>


<h2 id="adding-a-secret-to-an-application">Adding a secret to an application</h2><h3 id="adding-a-secret-to-an-application-overview">Overview</h3><p>The aim of this guide is to walkthrough the process of adding a secret (in this example for aws access-key credentials) to a previously deployed application in the Cloud Platform.</p>
<h3 id="adding-a-secret-to-an-application-prerequisites">Prerequisites</h3><p>This guide assumes the following:</p>

<ul>
<li>You have previously set up an env. See <a href="/tasks.html#creating-a-cloud-platform-environment">Creating a Cloud Platform Environment</a></li>
<li>You have previously deployed your application. See <a href="/tasks.html#deploying-a-39-hello-world-39-application-to-the-cloud-platform">Deploying an application to the Cloud-Platform</a></li>
<li>Check your deployment is running. See <a href="/tasks.html#interacting-with-the-application">Interacting with the application</a></li>
</ul>
<h3 id="configuring-secrets">Configuring secrets</h3><p>The following is an example of encoding (configuring) aws access-key credentials in your deployment.
See <a href="https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables">kuberenetes using secrets as environment variables</a></p>
<p>for detailed information regarding providing base64 values in secret objects to Kuberenetes pods)</p>
<p>Create your AWS Credentials access key (making a note of the aws_access_key_id and aws_secret_access_key)</p>
<h4 id="base64-encode-your-secret-as-follows">base64-encode your secret as follows:</h4><p>In this example  aws_access_key_id is &lsquo;AKIAFTKSAW15HJLOGD&rsquo;. Issue the following command to base64-encode:</p>
<div class="highlight"><pre class="highlight plaintext"><code>echo -n 'AKIAFTKSAW15HJLOGD' | base64 -b0
</code></pre></div><p>This will return the encoded id &#39;QUtJQUZUS1NBVzE1SEpMT0dE&rsquo;</p>
<p>In this example the is aws_secret_access_key &#39;g8hjpmhvgfhk4547gfdshhjj&rsquo;. Issue the following command to base64-encode:</p>
<div class="highlight"><pre class="highlight plaintext"><code>echo -n 'QUtJQUZUS1NBVzE1SEpMT0dE' | base64 -b0
</code></pre></div><p>This will return the encoded secret &#39;UVV0SlFVWlVTMU5CVnpFMVNFcE1UMGRF&rsquo;</p>
<h3 id="creating-the-secret">Creating the secret</h3><p>Create a secrets.yaml file similar to:</p>
<div class="highlight"><pre class="highlight plaintext"><code>apiVersion: v1
kind: Secret
metadata:
  name: demosecret
type: Opaque
data:
  aws_access_key_id: QUtJQUZUS1NBVzE1SEpMT0dE
  aws_secret_access_key: UVV0SlFVWlVTMU5CVnpFMVNFcE1UMGRF
</code></pre></div><p>issue the following command:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ kubectl apply -f secrets.yaml
secret "demosecret" created
</code></pre></div><p>To see the secrets:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ kubectl get secrets
NAME                                          TYPE                                  DATA      AGE
calico-zebu-external-dns-token-pldjb          kubernetes.io/service-account-token   3         16d
dandy-bumblebee-nginx-ingress-token-bspl6     kubernetes.io/service-account-token   3         14d
default-token-hz7z7                           kubernetes.io/service-account-token   3         26d
demosecret                                    Opaque                                2         5d
</code></pre></div><p>Decoding the Secret
Secrets can be retrieved via the kubectl get secret command. For example, to retrieve the secret you created:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ kubectl get secret demosecret  -o yaml
apiVersion: v1
data:
  aws_access_key_id: dGVzdCBrZXk=
  aws_secret_access_key: dGVzdCBrZXk=
kind: Secret
metadata:
  creationTimestamp: 2018-05-15T12:24:33Z
  name: demosecret
</code></pre></div><p>Add the AWS_ACCESS_KEY_ID referencing &#39;aws_access_key_id&rsquo; and AWS_SECRET_ACCESS_KEY referencing &#39;aws_secret_access_key&rsquo; (as previously set) to the containers env in <code>deployment-files/deployment.yaml</code></p>
<div class="highlight"><pre class="highlight plaintext"><code>    spec:
      containers:
        - name: django-demo-container
          image: 754256621582.dkr.ecr.eu-west-2.amazonaws.com/cloud-platform-reference-app:django
          ports:
            - containerPort: 8000
          env:
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: demosecret
                  key: aws_access_key_id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: demosecret
                  key: aws_secret_access_key
</code></pre></div>

<h2 id="creating-pingdom-checks">Creating Pingdom checks</h2><h3 id="creating-pingdom-checks-overview">Overview</h3><p><a href="https://my.pingdom.com">Pingdom</a> is a global performance and availability monitor for your web application. The aim of this document is to provide you with the necessary information to create Pingom checks via the <a href="https://github.com/ministryofjustice/cloud-platform-environments">cloud-platform-environments</a> pipeline, and then send failing checks to a Slack channel of your choosing.</p>
<h3 id="creating-pingdom-checks-prerequisites">Prerequisites</h3><p>This guide assumes the following:</p>

<ul>
<li>You have <a href="/tasks.html#creating-a-cloud-platform-environment">created a namespace for your application</a></li>
<li>You have a slack channel to send alerts to</li>
</ul>
<h3 id="create-a-pingdom-check">Create a Pingdom check</h3><p>To create a Pingdom check simply add a <code>pingdom.tf</code> file in the resources directory of your namespace in your <a href="https://github.com/ministryofjustice/cloud-platform-environments/tree/master/namespaces/live-1.cloud-platform.service.justice.gov.uk">cloud-platform-environments</a> repository. You can define the conditions of your check using the resources outlined in the <a href="https://github.com/russellcardullo/terraform-provider-pingdom">Terraform community provider</a>. Here&rsquo;s a working example of a <a href="https://github.com/ministryofjustice/cloud-platform-environments/tree/master/namespaces/cloud-platform-live-0.k8s.integration.dsd.io/monitoring/resources">basic check</a>.</p>
<div class="highlight"><pre class="highlight yaml"><code><span class="s">terraform {</span>
  <span class="s">backend "s3" {}</span>
<span class="err">}</span>

<span class="s">provider "pingdom" {}</span>

<span class="s">resource "pingdom_check" "cloud-platform-prometheus-live-0-healthcheck" {</span>
   <span class="s">type                     = "http"</span>
   <span class="s">name                     = "Prometheus - live-0 - cloud-platform - Healthcheck"</span>
   <span class="s">host                     = "prometheus.apps.cloud-platform-live-0.k8s.integration.dsd.io"</span>
   <span class="s">resolution               = 1</span>
   <span class="s">notifywhenbackup         = </span><span class="no">true</span>
   <span class="s">sendnotificationwhendown = 6</span>
   <span class="s">notifyagainevery         = 0</span>
   <span class="s">url                      = "/-/healthy"</span>
   <span class="s">encryption               = </span><span class="no">true</span>
   <span class="s">port                     = 443</span>
   <span class="s">tags                     = "businessunit_platforms,application_prometheus,component_healthcheck,isproduction_true,environment_prod,infrastructuresupport_platforms"</span>
   <span class="s">probefilters             = "region:EU"</span>
   <span class="s">publicreport             = "true"</span>
 <span class="s">}</span>
</code></pre></div><p><strong>Note</strong>: You&rsquo;ll need to include the <code>provider &quot;pingdom&quot;</code> and <code>terraform</code> blocks either in this file or in a <code>main.tf</code> file.</p>
<p>This basic check simply checks that the host/url (in this case; https://prometheus.apps.cloud-platform-live-0.k8s.integration.dsd.io/-/healthy) returns a 200 every minute (resolution = 1 minute). When six (sendnotificationwhendown = 6) consecutive checks fail it triggers an alarm. As publicreport is set to true, you can view the status of your check by visiting the <a href="http://pingdom.service.dsd.io">public status page</a>, where this check would appear with the name &ldquo;Prometheus - live-0 - cloud-platform - Healthcheck&rdquo;.</p>
<p><a href="https://github.com/russellcardullo/terraform-provider-pingdom#pingdom-check">This</a> page explains all the attributes used in the check.</p>
<p>All resources, including Pingdom checks <strong>must</strong> be tagged and adhere to the technical guidance outlined <a href="https://github.com/ministryofjustice/technical-guidance/blob/master/standards/documenting-infrastructure-owners.md">here</a>. Ensure your check has appropriate tags before submitting a pull request.</p>
<p>Once reviewed and merged to master, the pipeline will create your check in the MoJ Pingdom account.</p>
<h4 id="adding-slack-notification">Adding Slack notification</h4><p>You can enable the option to send a failing alert to Slack via a webhook by simply adding Pingdom integration id. You need administrator permissions to manage the mojdt <a href="https://mojdt.slack.com/apps/A0F814AV7-pingdom?next_id=0">Pingdom Slack</a> webhook and then <a href="https://my.pingdom.com">Pingdom</a> to create the integration id.</p>
<p>The Cloud Platform team can do this on your behalf. Create a ticket requesting a Pingdom integration id with the following information:</p>

<ul>
<li>team name</li>
<li>application name</li>
<li>slack channel</li>
</ul>
<p>The team will provide you with an integration id, following the steps outlined <a href="https://github.com/ministryofjustice/cloud-platform-environments/blob/master/docs/creating-pingdom-webhook.md">here</a>.</p>
<p>You can now add <code>integrationids</code> to your <code>pingdom.tf</code>. Appending the example above, your check will now appear as follows (assuming you were given 1000 as the integration id):</p>
<div class="highlight"><pre class="highlight yaml"><code><span class="s">terraform {</span>
   <span class="s">backend "s3" {}</span>
 <span class="s">}</span>

 <span class="s">provider "pingdom" {}</span>

 <span class="s">resource "pingdom_check" "cloud-platform-prometheus-live-0-healthcheck" {</span>
    <span class="s">type                     = "http"</span>
    <span class="s">name                     = "Prometheus - live-0 - cloud-platform - Healthcheck"</span>
    <span class="s">host                     = "prometheus.apps.cloud-platform-live-0.k8s.integration.dsd.io"</span>
    <span class="s">resolution               = 1</span>
    <span class="s">notifywhenbackup         = </span><span class="no">true</span>
    <span class="s">sendnotificationwhendown = 6</span>
    <span class="s">notifyagainevery         = 0</span>
    <span class="s">url                      = "/-/healthy"</span>
    <span class="s">encryption               = </span><span class="no">true</span>
    <span class="s">port                     = 443</span>
    <span class="s">tags                     = "businessunit_platforms,application_prometheus,component_healthcheck,isproduction_true,environment_prod,infrastructuresupport_platforms"</span>
    <span class="s">probefilters             = "region:EU"</span>
    <span class="s">publicreport             = "true"</span>
    <span class="s">integrationids           = [1000]</span>
  <span class="s">}</span>

</code></pre></div>
<h2 id="cleaning-up">Cleaning up</h2><p>When you have finished with a namespace, please clean it up, along with any
additional AWS resources you created. This helps to keep the cloud platform
repositories well-organised, and speeds up deployments and changes to the
cluster (because the build process doesn&rsquo;t have to spend time managing
unnecessary resources). It also helps to keep our hosting costs down.</p>
<p>The resources to be removed are:</p>

<ul>
<li>The AWS ECR which stores your docker images</li>
<li>Your namespace in the cluster. This contains all of the pods, containers and
other cluster resources for your application.</li>
<li>Any other AWS resources (e.g. RDS/Elasticache instances, S3 buckets, etc.)</li>
</ul>
<p>Cleaning up a namespace is a multi-stage process, as follows:</p>

<ol>
<li>Delete any CI/CD pipeline you have created, which deploys into your
namespace. This should be done first, so that anything you delete is not
immediately recreated by your build pipeline.</li>
<li>Tell terraform to delete the AWS resources it created for you.</li>
<li>Remove your namespace code from the <a href="https://github.com/ministryofjustice/cloud-platform-environments">cloud-platform-environments</a> repository.</li>
<li>Delete all of the kubernetes resources inside your namespace.</li>
<li>Delete your namespace from the cluster.</li>
</ol>
<p>The first step depends on how you have set up your CI/CD pipeline, and is not
covered here.</p>
<h3 id="2-tell-terraform-to-delete-your-aws-resources">2. Tell terraform to delete your AWS resources</h3><p>AWS resources are created by adding terraform code to the <code>resources</code> directory
in your namespace&rsquo;s folder in the <a href="https://github.com/ministryofjustice/cloud-platform-environments">cloud-platform-environments</a> repository:</p>
<div class="highlight"><pre class="highlight plaintext"><code> cloud-platform-environments/namespaces/live-1.cloud-platform.service.justice.gov.uk/[your namespace]/resources/
</code></pre></div><p>To get terraform to delete the resources it created, you need to delete all the
<code>\*.tf</code> files in this directory <strong>except <code>main.tf</code></strong></p>
<p>If you delete <code>main.tf</code> at this point, terraform has no way of knowing it is
responsible for managing any resources in the namespace, so it will not delete
anything. By leaving <code>main.tf</code> but nothing else, you are telling terraform that
it should manage resources for this namespace, but that there should be no
resources, so terraform will delete any resources that do exist.</p>
<p>Once you have deleted the other <code>*.tf</code> files from your namespace&rsquo;s resources
folder, raise a <a href="https://help.github.com/en/articles/about-pull-requests">PR</a> to get your changes merged. As soon as this happens, the
cloud platform build pipeline will run, and your AWS resources will be deleted.</p>
<h3 id="3-remove-your-namespace-code-from-the-cloud-platform-environments-repository">3. Remove your namespace code from the cloud-platform-environments repository</h3><p>After your change to delete all the <code>\*.tf</code> files except <code>main.tf</code> has been
merged, please raise an additional <a href="https://help.github.com/en/articles/about-pull-requests">PR</a> removing the whole of your namespace code
from the <a href="https://github.com/ministryofjustice/cloud-platform-environments">cloud-platform-environments</a> repository.</p>
<p>i.e. deleting the whole of the directory:</p>
<div class="highlight"><pre class="highlight plaintext"><code> cloud-platform-environments/namespaces/live-1.cloud-platform.service.justice.gov.uk/[your namespace]
</code></pre></div><p>Merging this <a href="https://help.github.com/en/articles/about-pull-requests">PR</a> will prevent the cloud platform build pipeline from recreating
your namespace, after it is deleted.</p>
<h3 id="4-delete-all-of-the-kubernetes-resources-inside-your-namespace">4. Delete all of the kubernetes resources inside your namespace.</h3><p>In your working copy of your application code, you can use the kubernetes
deployment yaml files to delete your namespace and all the kubernetes resources
within it.</p>
<p>Assuming your current working directory is a working copy of your application,
and that your kubernetes deployment yaml files are in a directory called
<code>kubernetes_deploy</code>, immediately below your current working directory, you
would run the following command to delete everything within your namespace.</p>
<div class="highlight"><pre class="highlight plaintext"><code>kubectl delete --filename kubernetes_deploy --namespace [your namespace]
</code></pre></div><p>This is analogous to using <code>kubectl apply</code> to create the resources from your
YAML files, but it will delete all the named resources.</p>
<p>If you are using <a href="https://helm.sh">Helm</a>, the equivalent command is:</p>
<div class="highlight"><pre class="highlight plaintext"><code>helm delete --purge
</code></pre></div><h3 id="5-delete-your-namespace-from-the-cluster">5. Delete your namespace from the cluster.</h3><p>Deleting a namespace requires admin access to the cluster.</p>
<p>Please raise a <a href="https://help.github.com/en/articles/about-pull-requests">PR</a> against the <a href="https://github.com/ministryofjustice/cloud-platform-environments">cloud-platform-environments</a> repository,
specifying the namespace you would like the team to delete.</p>
<h3 id="summary">Summary</h3><p>Removing your namespace and associated resources is a multi-stage process:</p>

<ol>
<li>Stop any CI/CD process from recreating everything</li>
<li>Get terraform to delete AWS resources</li>
<li>Remove the code that defines your namespace</li>
<li>Remove everything inside the namespace</li>
<li>Tell the cloud platform team to delete the namespace</li>
</ol>


<h2 id="migrating-from-live-0-to-live-1">Migrating from Live-0 to Live-1</h2><h3 id="migrating-from-live-0-to-live-1-overview">Overview</h3><p>After some long consideration of possible options, the decision has been made to migrate from the <code>live-0</code> cluster to the new <code>live-1</code> cluster.</p>
<p>The reason behind this decision is based on the need to move to a dedicated AWS account, which will be much easier to support, and the need to move away from the Ireland (EU) region to the London (UK) region.</p>
<p>The purpose of this document is to aid development teams in migrating their existing applications from <code>live-0</code> to <code>live-1</code>.</p>
<p>The migration steps that need to be taken may differ for individual applications.</p>
<p>The following steps are for an application that is considered to be fairly normal and deployed through CircleCI.</p>
<p>Appending these steps are a few extra consideration points, that are not covered in the example, but may apply to your application.</p>
<h3 id="accessing-the-live-1-cluster">Accessing the Live-1 cluster</h3><p>To access the <code>live-1</code> cluster, follow the steps in the <a href="/tasks.html#authentication">authentication</a> section of this guide, and download your Kube config file.</p>
<p>Kubernetes provides a <a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#set-the-kubeconfig-environment-variable">brief guide</a> on how to set up <code>kubectl</code> to use multiple config files simultaneously.</p>
<p>You should now be able to switch contexts between the <code>live-0</code> and <code>live-1</code> clusters.</p>
<h3 id="generating-a-new-environment">Generating a new environment</h3><p>Start by following the guide to generate a new environment, this follows the same process as was followed for <code>live-0</code>, and you should use the same details as you did for your environment then.</p>
<p><a href="/tasks.html#create-an-environment">Environment generation guide.</a></p>
<p>Run a <code>kubectl get namespaces</code> to check your environment has been successfully created.</p>
<h3 id="generating-a-new-ecr-repository">Generating a new ECR repository</h3><p>Once you&rsquo;ve generated a new environment in the <code>live-1</code> cluster, you will need to generate a new ECR repository for your application to be pushed to.</p>
<p>The reason why the previous ECR repo can&rsquo;t be used is due to the new <code>live-1</code> cluster being hosted in a separate AWS account.</p>
<p>If you need reminding of the ECR creation process, please see the <a href="/tasks.html#creating-an-ecr-repository">user documentation</a>.</p>
<h3 id="changing-the-circleci-environment-variables">Changing the CircleCI environment variables</h3><p>Now that you have a new empty environment and ECR repository set-up, the next step is to point your existing CircleCI pipeline away from the <code>live-0</code> environment, to your new <code>live-1</code> environment.</p>
<p>This is done by replacing the CircleCI environment variables with the ones generated for your <code>live-1</code> environment and then rerunning the pipeline.</p>
<p>Our helper script expects environment variables to be named according to the list below where <code>&lt;ENVIRONMENT&gt;</code> should be replaced by some identifier of your choosing (eg.: <code>STAGING</code>, <code>PRODUCTION</code>).</p>
<p>The environment variables you will need to replace are as follows:</p>

<div style="height:1px;font-size:1px;">&nbsp;</div>
<div class="table-container">
        <table>
          <tr>
<th>Variable</th>
<th style="text-align: left"></th>
</tr>
<tr>
<td><code>AWS_DEFAULT_REGION</code></td>
<td style="text-align: left">The default region will now be <code>eu-west-2</code>.</td>
</tr>
<tr>
<td><code>AWS_ACCESS_KEY_ID</code></td>
<td style="text-align: left">The access key can be found in the secret created by the ECR generation. This requires base64 decoding.</td>
</tr>
<tr>
<td><code>AWS_SECRET_ACCESS_KEY</code></td>
<td style="text-align: left">The secret key can be found in the secret created by the ECR generation. This requires base64 decoding.</td>
</tr>
<tr>
<td><code>ECR_ENDPOINT</code></td>
<td style="text-align: left">The ECR endpoint for all repos in <code>live-1</code> is <code>754256621582.dkr.ecr.eu-west-2.amazonaws.com</code></td>
</tr>
<tr>
<td><code>K8S_&lt;ENVIRONMENT&gt;_CLUSTER_CERT</code></td>
<td style="text-align: left">The cert is an attribute found in the <code>default-token</code> secret and does not need base64 decoding.</td>
</tr>
<tr>
<td><code>K8S_&lt;ENVIRONMENT&gt;_CLUSTER_NAME</code></td>
<td style="text-align: left">The cluster name is <code>live-1.cloud-platform.service.justice.gov.uk</code></td>
</tr>
<tr>
<td><code>K8S_&lt;ENVIRONMENT&gt;_NAMESPACE</code></td>
<td style="text-align: left">This variable should be equal to the name of your namespace.</td>
</tr>
<tr>
<td><code>K8S_&lt;ENVIRONMENT&gt;_TOKEN</code></td>
<td style="text-align: left">The token is another attribute found in the <code>default-token</code> secret and needs base64 decoding.</td>
</tr>

        </table>
      </div>
<div style="height:1px;font-size:1px;">&nbsp;</div>
<p>After triggering the CircleCI pipeline, your application should now deploy into your new environment.</p>
<h3 id="deleting-your-live-0-deployment">Deleting your Live-0 deployment</h3><p>The last thing you will need to do is to delete your application from the <code>live-0</code> cluster.</p>
<p>Please see the documentation on <a href="/tasks.html#cleaning-up">cleaning up within the Cloud Platform</a>.</p>
<h3 id="other-considerations">Other considerations</h3><h4 id="podsecuritypolicy">PodSecurityPolicy</h4><p>In the <code>live-1</code> cluster we are introducing a restrictive <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/"><code>PodSecurityPolicy</code></a> as part of our effort to harden the cluster and provide a stable, secure and reliable service.</p>
<p>The major change this brings is that root containers are not allowed to run. What this means is that the containers that run on the platform need to run as a non-root user. The <a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app/blob/9ad6caf101cc21117742e5ab2cbe5507efd54efd/rails-app/Dockerfile">solution</a> is straightforward for images we build: by using the <code>USER</code> directive in the <code>Dockerfile</code> with a <strong>numeric uid</strong>.</p>
<p>Sometimes we use images built by third parties which may or may not have taken the steps to build them as non-root images. One such very common example is nginx from the dockerhub library. If you need to run an nginx container we recommend that you use the <a href="https://github.com/bitnami/bitnami-docker-nginx"><code>bitnami/nginx</code></a> image.</p>
<p>See <a href="https://docs.bitnami.com/containers/how-to/work-with-non-root-containers">here</a> for more information on non-root containers.</p>


<h2 id="other-topics">Other Topics</h2>
<p><h3 id="git-crypt">Git-Crypt</h3><p>We use <code>git-crypt</code> to ensure that application secrets are encrypted at rest in git.</p>
<h4 id="git-crypt-prerequisites">Prerequisites</h4>
<ol>
<li>Install <a href="https://gnupg.org/">GPG</a></li>
<li>Install <a href="https://www.agwa.name/projects/git-crypt/">git-crypt</a></li>
<li>Generate a key pair, if you don&rsquo;t have one already. The <a href="https://help.github.com/articles/generating-a-new-gpg-key/">GitHub documentation</a> is a good reference.</li>
<li>Push your public key to a key server: <code>gpg --send-keys PUBKEYID</code></li>
<li>Add the pubkey to your GitHub account, again, following <a href="https://help.github.com/articles/adding-a-new-gpg-key-to-your-github-account/">the documentation</a></li>
</ol>
<h4 id="setup">Setup</h4>
<ul>
<li>If the repository has not been setup before, please follow the <a href="https://github.com/AGWA/git-crypt##using-git-crypt">git-crypt documentation</a> to do so.</li>
</ul>
<p>otherwise,</p></p>

<ul>
<li>Share your <code>PUBKEYID</code> with an existing member of the CloudPlatforms team. They will need to trust your key and add you to the repository (see git-crypt documentation above).</li>
</ul>
<p><h4 id="git-crypt-usage">Usage</h4><p>Once the above has been setup, update your local repository clone and unlock the secrets:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ git pull
$ git-crypt unlock
</code></pre></div><p>From this point on, <code>git-crypt</code> operates transparently.</p>
<p>You can verify the status of files by using <code>git-crypt status</code>.</p></p>
<p><h3 id="secrets-overview">Secrets overview</h3><p>We identify secrets as one of three kinds:
- user secrets
- system secrets
- application secrets</p>
<h4 id="user-secrets">User Secrets</h4><p>They are essentially any kind of secret that is owned by a specific user (eg. GitHub or AWS credentials). The user is responsible for securely managing these secrets, typically using a password manager and they should not be shared with other individuals or used in applications.</p>
<h4 id="system-secrets">System Secrets</h4><p>They are secrets used in system components, usually configured by someone who manages, configures or supports the system. For example, when setting up a CI pipeline, the credentials it uses to fetch the source code and push the produced artifacts to a repository are considered system secrets.</p>
<p>These should not be tied to an individual user but machine users should be employed. The responsibility of managing these secrets securely lies with the owner of the system.</p>
<h4 id="application-secrets">Application Secrets</h4><p>These are the secrets that the application requires at runtime. Some examples are: API keys for third-party services, keys that applications might use to communicate with each other, database credentials, cookie encryption keys and so on.</p>
<p>This kind of secrets falls under the shared responsibility model:</p></p>

<ul>
<li><p>the owners of the application are responsible for securely managing the secrets at rest (eg. using <a href="/tasks.html#git-crypt">git-crypt</a> to encrypt them alongside the source code) and also for managing access to the secrets once they&rsquo;ve been added to an environment,</p></li>
<li><p>the Cloud Platform team, on the other hand, is responsible for ensuring the secrets remain secure inside the environment.</p></li>
</ul>
<p><h3 id="kubectl-quick-reference">Kubectl quick reference</h3><p>This document acts as a quick reference to <code>kubectl</code>, listing some of the most common operations.</p>
<p>The examples here only address the most basic approach to these operations. For more options, please refer to the command-line help of <code>kubectl</code> subcommands.</p>
<p>There is also a more detailed <a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/">cheatsheet</a> in the official kubernetes documentation.</p>
<h6 id="inspecting-running-instances-of-the-application">Inspecting running instances of the application</h6><p>To list running <code>Pods</code>:
<code>
$ kubectl -n &lt;namespace&gt; get pods
</code>
To view details for a <code>Pod</code>:
<code>
$ kubectl -n &lt;namespace&gt; describe pod &lt;pod&gt;
</code></p>
<h6 id="viewing-logs">Viewing logs</h6><p>To access the logs of a running container:
<code>
$ kubectl -n &lt;namespace&gt; logs &lt;pod&gt;
</code></p>
<h6 id="viewing-kubernetes-events">Viewing kubernetes events</h6><p>To see kubernetes events, which can help debugging:
<code>
$ kubectl -n &lt;namespace&gt; get events
</code></p>
<h6 id="container-shell">Container shell</h6><p>You can get a shell inside a running container:
<code>
$ kubectl -n &lt;namespace&gt; exec -it &lt;pod&gt; sh
</code></p>
<p>For more information, click <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/">here</a></p>
<h6 id="pod-port-forwarding">Pod port-forwarding</h6><p>To forward port <code>5000</code> on <code>localhost</code> to port <code>5001</code> in the <code>Pod</code>:
<code>
$ kubectl -n &lt;namespace&gt; port-forward &lt;pod&gt; 5000:5001
</code></p>
<p>For more information, click <a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/">here</a></p></p>
<p><h3 id="cloud-platform-support">Cloud Platform Support</h3><p>This google document describes the Cloud Platform Support process, along with a description of what is/is not supported.</p>
<p><a href="https://docs.google.com/document/d/1M89TXQJIQAqu8yb2wzatlI8PgsXrgOi-GxLtB76XgjU/edit?usp=sharing">https://docs.google.com/document/d/1M89TXQJIQAqu8yb2wzatlI8PgsXrgOi-GxLtB76XgjU/edit?usp=sharing</a></p></p>
<p><h3 id="zero-downtime-deployments">Zero Downtime Deployments</h3><h4 id="zero-downtime-deployments-introduction">Introduction</h4><p>Zero Downtime Deployments are a significant feature of the Cloud Platform, that bring a host of advantages.</p>
<h4 id="rolling-updates">Rolling Updates</h4><p>Rolling updates introduce the ability to update an application without any downtime.</p>
<p>A rolling update works by ensuring there is always one extra Pod than the maximum number stated in the deployment.</p>
<p>The new deployment is applied incrementally, usually one to two Pods at a time until all Pods are running the latest version. How the deployment is rolled out, can be configured by the user. More information about configuring a rolling update can be found <a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/">here</a>.</p>
<p>If an application is exposed publicly, traffic will only be routed to the available Pods. However, this is only the case if the user configured <code>readinessProbes</code> correctly. This is a large topic, so more information can be found <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">here</a>.</p>
<h5 id="the-major-advantage">The Major Advantage</h5><p>Where rolling updates really brings benefit is the enabling of Continuous Integration and Continuous Delivery.</p>
<p>The ability to constantly update your application, with zero downtime, brings a host of benefits.</p>
<h5 id="rolling-updates-further-reading">Further Reading</h5><p>If you&rsquo;d like to read more in-depth into rolling updates, then Kubernetes has some great documentation <a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/">here</a>.</p>
<h5 id="readiness-liveness-probes-and-ssl-in-rails-applications">Readiness/Liveness probes and SSL in Rails applications</h5><p>For zero downtime deployments, you will need a readiness probe in your application, so that the cluster knows when your container is ready to receive traffic. You will also need a liveness probe, so that the cluster knows if it needs to restart your container after a crash.</p>
<p>SSL will be terminated outside of your pod, so your probes will need to respond to HTTP requests. However, Ruby on Rails applications are sometimes configured to only respond to HTTPS traffic (by adding <code>config.force_ssl = true</code> in the <code>config/environments/production.rb</code> file).</p>
<p>In this case, the application will respond to any HTTP request with a redirect status code, asking the requester to resend the request to the equivalent HTTPS URL. This will cause your probes to fail, because the redirect will not be followed.</p>
<p>To fix this, the probe needs to tell the application that it is an HTTPS request, even though it isn&rsquo;t, so that the application will process the request rather than sending a redirect response. You can do this by adding some HTTP headers to your probe definitions like this:</p>
<div class="highlight"><pre class="highlight plaintext"><code>readinessProbe:
  httpGet:
    path: /ping.json
    port: 3000
    httpHeaders:
      - name: X-Forwarded-Proto
        value: https
      - name: X-Forwarded-Ssl
        value: &quot;on&quot;
</code></pre></div><p>This will send an HTTP request to <code>/ping.json</code>, but the rails application will respond as if it is HTTPS, and your probe should work as expected. This works for both readiness and liveness probes.</p></p>
<p><h3 id="using-an-externally-managed-hostname">Using an externally-managed hostname</h3><h4 id="background">Background</h4><p>Every application running on Cloud Platform is able to use a hostname for their
HTTP endpoints, under a pre-defined DNS zone. For example, on the <code>live-1</code>
cluster, this would be <code>*.apps.live-1.cloud-platform.service.justice.gov.uk</code>. As
long as it is defined on the <code>Ingress</code> resource, it works automatically with a
wildcard TLS certificate.</p>
<p>However, most applications will typically need to be served on a <code>gov.uk</code>
hostname. These hostnames (or usually, DNS zones) are managed externally,
relative to the cluster, and there is a number of actions in order to set them
up for usage.</p>
<h4 id="using-an-externally-managed-hostname-setup">Setup</h4><h5 id="you-do-not-have-a-dns-zone-for-the-desired-hostname">You do not have a DNS zone for the desired hostname</h5><p>Please <a href="http://goo.gl/msfGiS">create a support ticket</a> providing as much
information as possible.</p>
<p>Once the zone is ready, you can continue with the rest of this guide.</p>
<h5 id="you-already-have-a-dns-zone-for-the-desired-hostname">You already have a DNS zone for the desired hostname</h5><h6 id="gt-it-39-s-a-route53-zone">&gt; It&rsquo;s a Route53 zone</h6>
<ol>
<li><p><a href="http://goo.gl/msfGiS">Create a support ticket</a> requesting the <code>provider</code>
name for your <code>Certificate</code> (see the next step).</p></li>
<li><p>Create the <code>Certificate</code>, using the <code>provider</code> name from the previous step.
The <code>secretName</code> attribute defines the <code>Secret</code> where the certificate and key
material will be stored, in your namespace.</p></li>
</ol>
<div class="highlight"><pre class="highlight plaintext"><code>   ---
   apiVersion: certmanager.k8s.io/v1alpha1
   kind: Certificate
   metadata:
     name: &lt;my-cert&gt;
     namespace: &lt;my-namespace&gt;
   spec:
     secretName: &lt;my-cert-secret&gt;
     issuerRef:
       name: letsencrypt-production
       kind: ClusterIssuer
     commonName: &#39;&lt;hostname&gt;&#39;
     acme:
       config:
       - domains:
         - &#39;&lt;hostname&gt;&#39;
         dns01:
           provider: &lt;provider&gt;
</code></pre></div>
<ol>
<li>Make sure the certificate has been issued correctly, by checking its <code>Status</code>:</li>
</ol>
<div class="highlight"><pre class="highlight plaintext"><code>   $ kubectl describe certificate &lt;my-cert&gt;
</code></pre></div>
<ol>
<li>You will need to update your <code>Ingress</code> spec to include the new hostname.</li>
</ol>
<p><strong>Once your host is defined here, the cluster will take control of the DNS record and automatically adjust to point to the cluster.</strong></p>
<p>If this does not happen, please get in touch with us in #ask-cloud-platform. Depending on your setup, we might need to
   intervene manually to allow <code>external-dns</code> to assume ownership of the DNS record.</p>
<div class="highlight"><pre class="highlight plaintext"><code>     apiVersion: extensions/v1beta1
     kind: Ingress
     metadata:
       name: &lt;my-ingress&gt;
       namespace: &lt;my-namespace&gt;
     spec:
       tls:
       - hosts:
         - my-app.apps.live-1.cloud-platform.service.justice.gov.uk
   +   - hosts:
   +     - &lt;hostname&gt;
   +     secretName: &lt;my-cert-secret&gt;
       rules:
       - host: my-app.apps.live-1.cloud-platform.service.justice.gov.uk
         http:
           paths:
           - path: /
             backend:
               serviceName: &lt;my-svc&gt;
               servicePort: 80
   +   - host: &lt;hostname&gt;
   +     http:
   +       paths:
   +       - path: /
   +         backend:
   +           serviceName: &lt;my-svc&gt;
   +           servicePort: 80
</code></pre></div><h6 id="gt-it-39-s-a-dns-zone-hosted-with-another-provider">&gt; It&rsquo;s a DNS zone hosted with another provider</h6><p>For the time being, we only support Route53 natively. Depending on the provider
we might be able to accommodate you or we might need to handle this manually, if possible.
Please <a href="http://goo.gl/msfGiS">create a support ticket</a>.</p></p>
<p><h3 id="ip-whitelisting">IP Whitelisting</h3><h4 id="inbound-ip-whitelisting">Inbound IP Whitelisting</h4><p>Allowed client IP source ranges can be specified using the nginx.ingress.kubernetes.io/whitelist-source-range annotation. The value is a comma separated list of CIDRs, e.g. 1.1.1.1/24,10.0.0.0/24.</p>
<p><a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#whitelist-source-range">Kubernetes official documentation on whitelisting source ranges</a>.</p>
<p>An example configuration using &ldquo;nginx.ingress.kubernetes.io/whitelist-source-range: 1.1.1.1/24,10.0.0.0/24&rdquo;</p>
<div class="highlight"><pre class="highlight plaintext"><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
          {&quot;apiVersion&quot;:&quot;extensions/v1beta1&quot;,&quot;kind&quot;:&quot;Ingress&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{&quot;kubernetes.io/ingress.class&quot;:&quot;nginx&quot;,&quot;nginx.ingress.kubernetes.io/whitelist-source-range&quot;:&quot;1.1.1.1/24,10.0.0.0/24&quot;},&quot;name&quot;:&quot;&lt;my-ingress&gt;&quot;,&quot;namespace&quot;:&quot;&lt;my-namespace&gt;&quot;},&quot;spec&quot;:{&quot;rules&quot;:[{&quot;host&quot;:&quot;my-app.apps.live-1.cloud-platform.service.justice.gov.uk&quot;,&quot;http&quot;:{&quot;paths&quot;:[{&quot;backend&quot;:{&quot;serviceName&quot;:&quot;&lt;my-svc&gt;&quot;,&quot;servicePort&quot;:3000},&quot;path&quot;:&quot;/&quot;}]}}],&quot;tls&quot;:[{&quot;hosts&quot;:[&quot;my-app.apps.live-1.cloud-platform.service.justice.gov.uk&quot;]}]}}
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/whitelist-source-range: 1.1.1.1/24,10.0.0.0/24
  creationTimestamp: 2019-03-21T14:26:31Z
  generation: 1
  name: &lt;my-ingress&gt;
  namespace: &lt;my-namespace&gt;
spec:
  rules:
  - host: my-app.apps.live-1.cloud-platform.service.justice.gov.uk
    http:
      paths:
      - path: /
      - backend:
          serviceName: &lt;my-svc&gt;
          servicePort: 3000
  tls:
  - hosts:
    - my-app.apps.live-1.cloud-platform.service.justice.gov.uk
status:
  loadBalancer:
    ingress:
    - hostname: &lt;hostname&gt;
</code></pre></div><p>Testing with the annotation set:</p>
<div class="highlight"><pre class="highlight plaintext"><code>curl -v -H &quot;Host: my-app.apps.live-1.cloud-platform.service.justice.gov.uk&quot; &lt;HOST-IP&gt;
</code></pre></div><p>Will return a &ldquo;403 forbidden&rdquo; status</p>
<h4 id="outbound-ip-whitelisting">Outbound IP Whitelisting</h4><h5 id="nat-gateways">NAT Gateways</h5><p>Many applications use third-party tools for a variety of reasons, and many of these tools require IP Whitelisting.</p>
<p>The Cloud Platform uses NAT Gateways as its external IP Endpoints.</p>
<p>The IP addresses for the clusters are as follows:</p>
<h6 id="live-0-cluster">Live-0 Cluster</h6><div class="highlight"><pre class="highlight plaintext"><code>52.17.133.167
34.251.93.81
34.247.134.240
</code></pre></div><h6 id="live-1-cluster">Live-1 Cluster</h6><div class="highlight"><pre class="highlight plaintext"><code>35.178.209.113
3.8.51.207
35.177.252.54
</code></pre></div>
<h3 id="ecr-lifecycle-policy">ECR Lifecycle Policy</h3><h4 id="ecr-lifecycle-policy-overview">Overview</h4><p>ECR repositories created for use in the Cloud Platform will have a default lifecycle policy applied.</p>
<p>Due to some applications having a constant rate of images being pushed to their ECR repo, we found that the AWS limit of 1000 images was being hit by some teams.</p>
<p>After consulting with application teams, we decided to implement a lifecycle policy of <em>40 images</em> per ECR repo.</p>
<p>If you feel that you have a need to archive more than the last 40 images, please contact the Cloud Platform team, who can adjust or remove the lifecycle policy.</p></p>
<p><h3 id="applying-a-maintenance-page">Applying a Maintenance Page</h3><h4 id="applying-a-maintenance-page-overview">Overview</h4><p>This document will serve as a guide on how to apply a default
maintenance page to your application on the Cloud Platform.</p>
<h4 id="deploying-the-page">Deploying the page</h4><p>A repository has been created to store all the files related to the
maintenance page:</p>
<p><a href="https://github.com/ministryofjustice/cloud-platform-maintenance-page">cloud-platform-maintenance-page</a></p>
<p>The repository contains the manifest files needed to deploy a
standard maintenance page into your namespace. This directory also
contains the maintenance page HTML file, along with a DockerFile
to build an image to serve the maintenance page.</p>
<p>Within the <code>Kubectl_deploy</code> directory, there are 3 simple manifest
files that make up the deployment, <code>deploy.yaml</code>, <code>ingress.yaml</code>
and <code>service.yaml</code>.</p>
<p>To use this example, copy the files into your namespace and make
any changes you require, to tailor the maintenance page to your
service.</p>
<p>Once you have done this, the maintenance page will be deployed
into your namespace, ready for you to use as and when you need
it.</p>
<h5 id="maintenance-deploy-yaml">maintenance-deploy.yaml</h5><p>A notable part of this file is the <code>image</code> line, which points to
the ECR URI.</p>
<p>If you wish to customize the maintenance page, you must edit and
build the image and update the <code>image</code> value.</p>
<h5 id="maintenance-ingress-yaml">maintenance-ingress.yaml</h5><p>In this file, change the example <code>host</code> field to your applications
URL.</p>
<p>Rather than using this file, you may prefer to change your existing
<code>Ingress</code> so that the <code>backend</code> points to your maintenance page,
whenever you need to replace your service with the maintenance page.</p>
<p>This will ensure that you do not incur any downtime (by deleting
the previous ingress and creating a new one).</p></p>


            
          </main>

            <ul class="contribution-banner">
              <li><a href="https://github.com/ministryofjustice/cloud-platform-user-guide/blob/master/source/tasks.html.md.erb">View source</a></li>
              <li><a href="https://github.com/ministryofjustice/cloud-platform-user-guide/issues/new?labels=bug&amp;title=Re:%20'Tasks'&amp;body=Problem%20with%20'Tasks'%20(https://user-guide.cloud-platform.service.justice.gov.uk/tasks.html)">Report problem</a></li>
              <li><a href="https://github.com/ministryofjustice/cloud-platform-user-guide">GitHub Repo</a></li>
            </ul>

          <footer class="footer">
  <div class="footer__licence">
    <a class="footer__licence-logo" href="https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/" rel="license">Open Government Licence</a>
    <p class="footer__licence-description">All content is available under the <a href="https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/" rel="license">Open Government Licence v3.0</a>, except where otherwise stated</p>
  </div>

  <div class="footer__copyright">
    <a class="footer__copyright-logo" href="http://www.nationalarchives.gov.uk/information-management/re-using-public-sector-information/copyright-and-re-use/crown-copyright/">© Crown copyright</a>
  </div>
</footer>

        </div>
      </div>
    </div>

    
  </body>
</html>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-138188246-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-138188246-1');
</script>

<script>
  // Add the current window.location to the body of the
  // feedback email message.
  function sendFeedback(mailto, event) {
    event.preventDefault();

    var body = "Feedback/Problem on page: " + window.location + "\n";
    var href = mailto + '&body=' + encodeURIComponent(body);

    window.location = href;
  }

  $(document).ready(function() {
    var feedbackLink = $('a[href^="mailto:"]:contains("Feedback")')[0];
    var mailto = feedbackLink.href;
    $(feedbackLink).on('click', function(event) { sendFeedback(mailto, event); });
  });
</script>
