<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta content="IE=edge" http-equiv="X-UA-Compatible">
    <meta charset="utf-8">
    <meta content="width=device-width,initial-scale=1.0,minimum-scale=1.0,maximum-scale=1.0,user-scalable=no" name="viewport">
      <meta name="robots" content="noindex">

    <title>Tasks | Cloud Platform User Guide</title>

    <!--[if gt IE 8]><!--><link href="/stylesheets/screen.css" rel="stylesheet" media="screen" /><!--<![endif]-->
    <!--[if lte IE 8]><link href="/stylesheets/screen-old-ie.css" rel="stylesheet" media="screen" /><![endif]-->

    <link rel="canonical" href="https://user-guide.cloud-platform.service.justice.gov.uk/tasks.html">


    <link href="/stylesheets/print.css" rel="stylesheet" media="print" />
    <script src="/javascripts/application.js"></script>

      <meta property="og:image" content="https://user-guide.cloud-platform.service.justice.gov.uk/images/govuk-large.png" />
      <meta property="og:site_name" content="Cloud Platform User Guide" />
      <meta property="og:title" content="Tasks" />
      <meta property="og:type" content="object" />
      <meta property="og:url" content="https://user-guide.cloud-platform.service.justice.gov.uk/tasks.html" />
      <meta property="twitter:card" content="summary" />
      <meta property="twitter:domain" content="user-guide.cloud-platform.service.justice.gov.uk" />
      <meta property="twitter:image" content="https://user-guide.cloud-platform.service.justice.gov.uk/images/govuk-large.png" />
      <meta property="twitter:title" content="Tasks | Cloud Platform User Guide" />
      <meta property="twitter:url" content="https://user-guide.cloud-platform.service.justice.gov.uk/tasks.html" />

    
  </head>

  <body>
    <div class="app-pane">
      <div class="app-pane__header toc-open-disabled">
        <a href="#content" class="skip-link">Skip to main content</a>

        <header class="header header--full-width">
  <div class="header__container">
    <div class="header__brand">
        <a href="/">
        <span class="header__title">
          Cloud Platform User Guide
            <span class="phase-banner">INTERNAL</span>
        </span>
        </a>
    </div>

      <div data-module="navigation">
        <button type="button" class="header__navigation-toggle js-nav-toggle" aria-controls="navigation" aria-label="Show or hide top level navigation">Menu</button>

        <nav id="navigation" class="header__navigation js-nav" aria-label="Top Level Navigation" aria-hidden="true">
          <ul>
              <li>
                <a href="mailto:platforms+user-guide@digital.justice.gov.uk?subject=User+guide+feedback">Feedback / Report a problem</a>
              </li>
              <li>
                <a href="/">Documentation</a>
              </li>
              <li>
                <a href="https://github.com/ministryofjustice/cloud-platform-user-guide">GitHub</a>
              </li>
          </ul>
        </nav>
      </div>
  </div>
</header>

      </div>

        <div id="toc-heading" class="toc-show fixedsticky">
          <a href="#toc" class="toc-show__label js-toc-show" aria-controls="toc">
            Table of contents <span class="toc-show__icon"></span>
          </a>
        </div>

      <div class="app-pane__body" data-module="in-page-navigation">
          <div class="app-pane__toc">
            <div class="toc" data-module="table-of-contents">
              <div class="search" data-module="search">
  <form action="https://www.google.co.uk/search" method="get" role="search">
    <input type="hidden" name="as_sitesearch" value="https://user-guide.cloud-platform.service.justice.gov.uk"/>
    <label for="search"  class="search__label">Search (via Google)</label>
    <input type="text" id="search" name="q" placeholder="Search" aria-controls="search-results" class="form-control" />
  </form>
  <div id="search-results" class="search-results" aria-hidden="true" role="dialog" aria-labelledby="search-results-title">
    <div class="search-results__inner">
      <button class="search-results__close">Close<span class="search-results__close-label"> search results</span></button>
      <h2 id="search-results-title" class="search-results__title" aria-live="polite">Results</h2>
      <div class="search-results__content"></div>
    </div>
  </div>
</div>

              <a href="#" class="toc__close js-toc-close" aria-controls="toc" aria-label="Hide table of contents"></a>
              <nav id="toc" class="js-toc-list toc__list" aria-labelledby="toc-heading" data-module="collapsible-navigation">
                      <ul>
  <li>
    <a href="/#cloud-platform-user-guide">Cloud platform user guide</a>
    <ul>
      <li>
        <ul>
          <li>
          </li>
          <li>
            <a href="/#who-is-the-platform-for">Who is the platform for</a>
          </li>
          <li>
            <a href="/#what-do-we-currently-support">What do we currently support</a>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<ul>
  <li>
    <a href="/concepts.html#concepts">Concepts</a>
    <ul>
      <li>
        <a href="/concepts.html#kubernetes">Kubernetes</a>
        <ul>
          <li>
            <a href="/concepts.html#resources">Resources</a>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<ul>
  <li>
    <a href="/tasks.html#tasks">Tasks</a>
    <ul>
      <li>
        <a href="/tasks.html#how-to-use-kubectl-to-connect-to-the-cluster">How to use kubectl to connect to the cluster</a>
        <ul>
          <li>
            <a href="/tasks.html#installation">Installation</a>
          </li>
          <li>
            <a href="/tasks.html#authentication">Authentication</a>
          </li>
          <li>
            <a href="/tasks.html#where-to-go-from-here">Where to go from here?</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#creating-a-cloud-platform-environment">Creating a Cloud Platform Environment</a>
        <ul>
          <li>
            <a href="/tasks.html#introduction">Introduction</a>
          </li>
          <li>
            <a href="/tasks.html#objective">Objective</a>
          </li>
          <li>
            <a href="/tasks.html#create-an-environment">Create an environment</a>
          </li>
          <li>
            <a href="/tasks.html#accessing-your-environments">Accessing your environments</a>
          </li>
          <li>
            <a href="/tasks.html#next-steps">Next steps</a>
          </li>
          <li>
            <a href="/tasks.html#more-information-on-environment-definition">More information on environment definition</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#creating-an-ecr-repository">Creating an ECR repository</a>
        <ul>
          <li>
            <a href="/tasks.html#creating-an-ecr-repository-introduction">Introduction</a>
          </li>
          <li>
            <a href="/tasks.html#accessing-the-credentials">Accessing the credentials</a>
          </li>
          <li>
            <a href="/tasks.html#creating-an-ecr-repository-next-steps">Next steps</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#creating-a-route-53-hosted-zone">Creating a Route 53 Hosted Zone</a>
        <ul>
          <li>
            <a href="/tasks.html#overview">Overview</a>
          </li>
          <li>
            <a href="/tasks.html#pre-requisites">Pre-Requisites</a>
          </li>
          <li>
            <a href="/tasks.html#terraform-files">Terraform files</a>
          </li>
          <li>
            <a href="/tasks.html#creating-the-resource">Creating the resource</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#adding-aws-resources-to-your-environment">Adding AWS resources to your environment</a>
        <ul>
          <li>
            <a href="/tasks.html#available-modules">Available modules</a>
          </li>
          <li>
            <a href="/tasks.html#usage">Usage</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#deploying-a-39-hello-world-39-application-to-the-cloud-platform">Deploying a ‘Hello World’ application to the Cloud Platform</a>
        <ul>
          <li>
            <a href="/tasks.html#deploying-a-39-hello-world-39-application-to-the-cloud-platform-overview">Overview</a>
          </li>
          <li>
            <a href="/tasks.html#prerequisites">Prerequisites</a>
          </li>
          <li>
            <a href="/tasks.html#step-1-build-your-docker-image">Step 1 - Build your docker image</a>
          </li>
          <li>
            <a href="/tasks.html#step-2-push-the-image-to-your-ecr">Step 2 - Push the image to your ECR</a>
          </li>
          <li>
            <a href="/tasks.html#step-3-configure-your-namespace-in-the-kubernetes-cluster">Step 3 - Configure your namespace in the Kubernetes Cluster</a>
          </li>
          <li>
            <a href="/tasks.html#step-4-deploy-the-application">Step 4 - Deploy the application</a>
          </li>
          <li>
            <a href="/tasks.html#interacting-with-the-application">Interacting with the application</a>
          </li>
          <li>
            <a href="/tasks.html#add-http-basic-authentication">Add HTTP Basic Authentication</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#deploying-a-multi-container-application-to-the-cloud-platform">Deploying a multi-container application to the Cloud Platform</a>
        <ul>
          <li>
            <a href="/tasks.html#deploying-a-multi-container-application-to-the-cloud-platform-overview">Overview</a>
          </li>
          <li>
            <a href="/tasks.html#running-in-the-kubernetes-cluster">Running in the Kubernetes Cluster</a>
          </li>
          <li>
            <a href="/tasks.html#create-an-rds-instance">Create an RDS instance</a>
          </li>
          <li>
            <a href="/tasks.html#build-docker-images-and-pushing-to-ecr">Build docker images and pushing to ECR</a>
          </li>
          <li>
            <a href="/tasks.html#kubernetes-configuration">Kubernetes configuration</a>
          </li>
          <li>
            <a href="/tasks.html#deploying-to-the-cluster">Deploying to the cluster</a>
          </li>
          <li>
            <a href="/tasks.html#deploying-a-multi-container-application-to-the-cloud-platform-interacting-with-the-application">Interacting with the application</a>
          </li>
          <li>
            <a href="/tasks.html#further-development">Further Development</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#deploying-with-helm-and-circleci">Deploying with Helm and CircleCI</a>
        <ul>
          <li>
            <a href="/tasks.html#prerequisite-for-live-1-deployment">Prerequisite for Live-1 deployment</a>
          </li>
          <li>
            <a href="/tasks.html#deploying-an-application-to-the-cloud-platform-with-helm">Deploying an application to the Cloud Platform with Helm</a>
          </li>
          <li>
            <a href="/tasks.html#continuous-deployment-of-an-application-using-circleci-and-helm">Continuous Deployment of an application using CircleCI and Helm</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#how-do-i-run-rails-database-migrations">How do I run Rails database migrations?</a>
        <ul>
          <li>
            <a href="/tasks.html#do-not-run-migrations-on-container-startup">Do not run migrations on container startup</a>
          </li>
          <li>
            <a href="/tasks.html#further-reading">Further reading</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#adding-a-secret-to-an-application">Adding a secret to an application</a>
        <ul>
          <li>
            <a href="/tasks.html#adding-a-secret-to-an-application-overview">Overview</a>
          </li>
          <li>
            <a href="/tasks.html#adding-a-secret-to-an-application-prerequisites">Prerequisites</a>
          </li>
          <li>
            <a href="/tasks.html#configuring-secrets">Configuring secrets</a>
          </li>
          <li>
            <a href="/tasks.html#creating-the-secret">Creating the secret</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#creating-pingdom-checks">Creating Pingdom checks</a>
        <ul>
          <li>
            <a href="/tasks.html#creating-pingdom-checks-overview">Overview</a>
          </li>
          <li>
            <a href="/tasks.html#creating-pingdom-checks-prerequisites">Prerequisites</a>
          </li>
          <li>
            <a href="/tasks.html#create-a-pingdom-check">Create a Pingdom check</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#cleaning-up">Cleaning up</a>
        <ul>
          <li>
            <a href="/tasks.html#2-tell-terraform-to-delete-your-aws-resources">2. Tell terraform to delete your AWS resources</a>
          </li>
          <li>
            <a href="/tasks.html#3-remove-your-namespace-code-from-the-cloud-platform-environments-repository">3. Remove your namespace code from the cloud-platform-environments repository</a>
          </li>
          <li>
            <a href="/tasks.html#4-delete-all-of-the-kubernetes-resources-inside-your-namespace">4. Delete all of the kubernetes resources inside your namespace.</a>
          </li>
          <li>
            <a href="/tasks.html#5-delete-your-namespace-from-the-cluster">5. Delete your namespace from the cluster.</a>
          </li>
          <li>
            <a href="/tasks.html#summary">Summary</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#monitoring-applications">Monitoring Applications</a>
        <ul>
          <li>
            <a href="/tasks.html#using-the-cloud-platform-prometheus-alertmanager-and-grafana">Using the Cloud Platform Prometheus, AlertManager and Grafana</a>
          </li>
          <li>
            <a href="/tasks.html#creating-your-own-custom-alerts">Creating your own custom alerts</a>
          </li>
          <li>
            <a href="/tasks.html#getting-application-metrics-into-prometheus">Getting Application Metrics into Prometheus</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#application-logging">Application Logging</a>
        <ul>
          <li>
            <a href="/tasks.html#application-log-collection-and-storage">Application Log Collection and Storage</a>
          </li>
          <li>
            <a href="/tasks.html#accessing-application-log-data">Accessing Application Log Data</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#migrating-from-live-0-to-live-1">Migrating from Live-0 to Live-1</a>
        <ul>
          <li>
            <a href="/tasks.html#migrating-from-live-0-to-live-1-overview">Overview</a>
          </li>
          <li>
            <a href="/tasks.html#accessing-the-live-1-cluster">Accessing the Live-1 cluster</a>
          </li>
          <li>
            <a href="/tasks.html#generating-a-new-environment">Generating a new environment</a>
          </li>
          <li>
            <a href="/tasks.html#generating-a-new-ecr-repository">Generating a new ECR repository</a>
          </li>
          <li>
            <a href="/tasks.html#changing-the-circleci-environment-variables">Changing the CircleCI environment variables</a>
          </li>
          <li>
            <a href="/tasks.html#deleting-your-live-0-deployment">Deleting your Live-0 deployment</a>
          </li>
          <li>
            <a href="/tasks.html#other-considerations">Other considerations</a>
          </li>
        </ul>
      </li>
      <li>
        <a href="/tasks.html#other-topics">Other Topics</a>
        <ul>
          <li>
            <a href="/tasks.html#git-crypt">Git-Crypt</a>
          </li>
          <li>
            <a href="/tasks.html#secrets-overview">Secrets overview</a>
          </li>
          <li>
            <a href="/tasks.html#migrating-an-rds-instance">Migrating an RDS instance</a>
          </li>
          <li>
            <a href="/tasks.html#kubectl-quick-reference">Kubectl quick reference</a>
          </li>
          <li>
            <a href="/tasks.html#cloud-platform-support">Cloud Platform Support</a>
          </li>
          <li>
            <a href="/tasks.html#zero-downtime-deployments">Zero Downtime Deployments</a>
          </li>
          <li>
            <a href="/tasks.html#using-a-custom-domain">Using a custom domain</a>
          </li>
          <li>
            <a href="/tasks.html#ip-whitelisting">IP Whitelisting</a>
          </li>
          <li>
            <a href="/tasks.html#setup-postgres-container">Setup Postgres container</a>
          </li>
          <li>
            <a href="/tasks.html#applying-a-maintenance-page">Applying a Maintenance Page</a>
          </li>
          <li>
            <a href="/tasks.html#how-to-decommission-unused-template-deploy-services">How to decommission unused template deploy services</a>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <a href="/tasks.html#ssl-connections-with-rds">SSL connections with RDS</a>
    <ul>
      <li>
        <a href="/tasks.html#full-verification-of-certificates">Full verification of certificates</a>
      </li>
      <li>
        <a href="/tasks.html#force-ssl-connections">Force SSL connections</a>
      </li>
    </ul>
  </li>
</ul>
<ul>
  <li>
    <a href="/reference.html#reference">Reference</a>
    <ul>
      <li>
        <a href="/reference.html#kubernetes-resources">Kubernetes resources</a>
        <ul>
          <li>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<ul>
  <li>
    <a href="/getting-help.html#getting-help">Getting Help</a>
    <ul>
      <li>
        <a href="/getting-help.html#slack-channel">Slack Channel</a>
      </li>
      <li>
        <a href="/getting-help.html#raise-a-support-ticket">Raise a support ticket</a>
      </li>
    </ul>
  </li>
</ul>


              </nav>
            </div>
          </div>

        <div class="app-pane__content toc-open-disabled">
          <main id="content" class="technical-documentation" data-module="anchored-headings">
              <h1 id="tasks">Tasks</h1>
<h2 id="how-to-use-kubectl-to-connect-to-the-cluster">How to use <code>kubectl</code> to connect to the cluster</h2><p>The aim of this guide is to provide a walkthrough of the installation of the <code>kubectl</code> command-line tool, the official command-line tool for Kubernetes, as well as setting it up by authenticating with the Cloud Platform cluster.</p>
<p><code>kubectl</code> is used to deploy and manage applications on Kubernetes and is fundamental for anyone who wants to interact with the cluster.</p>
<p>When complete you should have access to perform API calls via a tool called <code>kubectl</code>.</p>
<h3 id="installation">Installation</h3><p>Please read the <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl">official documentation</a> on how to install <code>kubectl</code>.</p>
<h3 id="authentication">Authentication</h3><p>You must have a GitHub account and be a member of the Ministry of Justice Organisation.</p>
<p>For our use case, we want authentication and identity to be handled by Github, and to derive all cluster access control from Github teams - projects will be deployed into namespaces (e.g. <code>pvb-production</code>, <code>cla-staging</code>), and access to resources in those namespaces should be available to the appropriate teams only (e.g. <code>PVB</code> and <code>CLA</code> teams).</p>
<p>Kubernetes supports authentication from external identity providers, including group definition, via <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#openid-connect-tokens">OIDC</a>. Github however only support OAuth2 as an authentication method, so an identity broker is required to translate from OAuth2 to OIDC.</p>
<p>As work on MOJ&rsquo;s identity service is ongoing, a development <a href="https://www.auth0.com">Auth0</a> account has been created to act as a standin in the meantime.</p>
<h4 id="live-clusters">Live clusters</h4><p>Live clusters are those available to users:</p>

<div style="height:1px;font-size:1px;">&nbsp;</div>
<div class="table-container">
        <table>
          <tr>
<th>Cluster Name</th>
<th>Login page</th>
</tr>
<tr>
<td><code>cloud-platform-live-0</code></td>
<td><a href="https://login.apps.cloud-platform-live-0.k8s.integration.dsd.io/">https://login.apps.cloud-platform-live-0.k8s.integration.dsd.io/</a></td>
</tr>
<tr>
<td><code>live-1.cloud-platform</code></td>
<td><a href="https://login.cloud-platform.service.justice.gov.uk/">https://login.cloud-platform.service.justice.gov.uk/</a></td>
</tr>

        </table>
      </div>
<div style="height:1px;font-size:1px;">&nbsp;</div>
<h4 id="how-do-i-connect-to-a-cluster">How do I connect to a cluster?</h4><p>We employ <a href="https://github.com/negz/kuberos">Kuberos</a> to help with the setup, a service that can generate the client configuration for users.</p>
<p>To authenticate with a cluster, please follow the steps below;</p>

<ul>
<li>Navigate to a login page from the table above</li>
<li>Click the login with GitHub option and authorise kuberos</li>
<li>Follow the instructions on the page presented, once finished you should have a <a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/"><code>kubeconfig</code></a> file in <code>~/.kube/config</code>.</li>
<li>You should now be able to run <code>kubectl</code> commands; try running such <code>kubectl get namespaces</code></li>
</ul>
<h5 id="troubleshooting-quot-current-quot-context">Troubleshooting: &ldquo;current&rdquo; context</h5><p>If you receive <code>The connection to the server localhost:8080 was refused</code> errors while executing <code>kubectl</code> commands,
check that your &ldquo;current&rdquo; context is set.</p>
<p>Run <code>kubectl config get-contexts</code>:</p>
<div class="highlight"><pre class="highlight plaintext"><code>CURRENT   NAME                                           CLUSTER                                        AUTHINFO                 NAMESPACE
          live-1.cloud-platform.service.justice.gov.uk   live-1.cloud-platform.service.justice.gov.uk   &lt;your github e-mail&gt;
</code></pre></div><p>Set the context you want to use as &ldquo;current&rdquo; with: <code>kubectl config use-context &lt;NAME&gt;</code>.</p>
<p>E.g. <code>kubectl config use-context live-1.cloud-platform.service.justice.gov.uk</code></p>
<h4 id="multiple-clusters">Multiple clusters</h4><p>To setup additional clusters, follow the process above and save the generated <code>kubeconfig</code> with a different filename (eg.: <code>~/.kube/config_live1</code>).</p>
<p>You can then use the <code>KUBECONFIG</code> environment variable to have <code>kubectl</code> parse the additional configuration files, eg.: <code>KUBECONFIG=~/.kube/config:~/.kube/config_live1</code></p>
<p>For more information please read the <a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">official documentation</a></p>
<h5 id="usernames">Usernames</h5><p>By default, Kuberos will use your email address as the username in the generated <code>kubeconfig</code>.</p>
<p>When setting up multiple clusters, this will generate conflicts so you should rename the users by editing the files (and update the contexts appropriately).</p>
<h3 id="where-to-go-from-here">Where to go from here?</h3><p>Now that you&rsquo;ve setup <code>kubectl</code>, you might want to look at this handy <a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/">cheatsheet</a>.</p>
<p>Once you are ready to deploy applications you will need to create an environment first.</p>

<h2 id="creating-a-cloud-platform-environment">Creating a Cloud Platform Environment</h2><h3 id="introduction">Introduction</h3><p>This is a guide to creating a environment in one of our Kubernetes clusters.</p>
<p>We define an environment as a Kubernetes
<a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/">namespace</a>
with some key resources deployed in it. Each Kubernetes namespace creates a
logical separation within our cluster that provides isolation from any other
namespace.</p>
<p>Once you have created an environment you will be able to perform actions using
the <code>kubectl</code> tool in the namespace you have created.</p>
<h3 id="objective">Objective</h3><p>By the end of this guide you&rsquo;ll have created a Kubernetes namespace ready for
you to <a href="/tasks.html#deploying-a-39-hello-world-39-application-to-the-cloud-platform">deploy an application</a> into.</p>
<h3 id="create-an-environment">Create an environment</h3><p>You create an environment by adding the definition of the environment in YAML
to the following repository, hosted on GitHub:</p>
<p><a href="https://github.com/ministryofjustice/cloud-platform-environments">cloud-platform-environments</a></p>
<p>Adding your environment definition kicks off a pipeline which builds your
environment on the appropriate cluster.</p>
<h4 id="set-up">Set up</h4><p>First we need to clone the repository, change directory and create a new branch:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ git clone git@github.com:ministryofjustice/cloud-platform-environments.git
$ cd cloud-platform-environments
$ git checkout -b &lt;yourBranch&gt;
</code></pre></div><h4 id="the-directory-structure">The directory structure</h4><p>We build new environments by creating a new directory for our environment and
putting the YAML files that define the environment into that directory. To
understand where to create the directory it is useful to understand the overall
structure of the repo:</p>
<div class="highlight"><pre class="highlight plaintext"><code>cloud-platform-environments
├── namespace-resources
└── namespaces
    └── live-1.cloud-platform.service.justice.gov.uk
        ├── kube-system
        ├── user-roles.yaml

        └── ... NAMESPACE FOLDERS ...

</code></pre></div><p><strong>cloud-platform-environments</strong></p>
<p>This is the root of the repo, containing <code>namespaces</code> directory</p>
<p><strong>/namespaces</strong></p>
<p>The namespaces directory contains a directory for each of the clusters that you
can build environments on. Create your environment in the
<code>live-1.cloud-platform.service.justice.gov.uk</code> directory.</p>
<p><strong>/namespaces/live-1.cloud-platform.service.justice.gov.uk/</strong></p>
<p>Within the cluster directory you will generate a directory for your environment
in the format <code>&lt;servicename-env&gt;</code>, for example <code>myapp-dev</code>.</p>
<p><strong>/namespaces/live-1.cloud-platform.service.justice.gov.uk/servicename-env/</strong></p>
<p>The <code>&lt;servicename-env&gt;</code> directory for your environment defines the specific
resources we will create in your namespace. We describe these resources in more
detail in <a href="#how-we-set-up-an-environment">how we set up an environment</a>.</p>
<h4 id="how-we-set-up-an-environment">How we set up an environment</h4><p>To set up an environment we create 5 Kubernetes YAML files in the directory for your namespace:</p>

<ul>
<li><a href="#00-namespace-yaml"><code>00-namespace.yaml</code></a></li>
<li><a href="#01-rbac-yaml"><code>01-rbac.yaml</code></a></li>
<li><a href="#02-limitrange-yaml"><code>02-limitrange.yaml</code></a></li>
<li><a href="#03-resourcequota-yaml"><code>03-resourcequota.yaml</code></a></li>
<li><a href="#04-networkpolicy-yaml"><code>04-networkpolicy.yaml</code></a></li>
</ul>
<p>These files define key elements of the namespace and restrictions we want to
place on it so that we have security and resource allocation properties. We
will use terraform to create these files from templates. We also describe each
of these files <a href="#00-namespace-yaml">in more detail below</a> in case you want to
make future changes.</p>
<p>In addition to the Kubernetes configuration files, we create a terraform config file:</p>
<div class="highlight"><pre class="highlight plaintext"><code>resources/main.tf
</code></pre></div><p>This file defines the standard terraform backend and providers which you will
need when you add terraform modules to create the AWS resources your service
will use (e.g. an <a href="/tasks.html#creating-an-ecr-repository">ECR</a> for your docker images, <a href="/tasks.html#create-an-rds-instance">RDS
databases</a>, and S3 buckets).</p>
<h4 id="create-your-namespace-and-namespace-resources">Create your namespace and namespace resources</h4><p>We automate the creation of the namespace resource files using terraform. You will need to install terraform locally:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ brew install terraform@0.11
</code></pre></div>
<blockquote>
<p>Note: You must specify version 0.11 of terraform. 0.12 (the latest, at the time of writing) introduces breaking changes, and the cluster has not yet been upgraded to work with Terraform 0.12. If you try to use terraform 0.12, you will get errors about formatting and other problems. Please stick to version 0.11, for now.</p>
</blockquote>
<p>In each of the template files you need to replace some example values with
information about your namespace, team or app. We do this by running terraform
commands and filling in the values.</p>
<p>These are the inputs for the terraform module, that you will need to fill:</p>

<div style="height:1px;font-size:1px;">&nbsp;</div>
<div class="table-container">
        <table>
          <tr>
<th>Name</th>
<th>Description</th>
<th style="text-align: center">Type</th>
<th style="text-align: center">Default</th>
<th style="text-align: center">Required</th>
</tr>
<tr>
<td>application</td>
<td>The name of your application</td>
<td style="text-align: center">string</td>
<td style="text-align: center">-</td>
<td style="text-align: center">yes</td>
</tr>
<tr>
<td>business-unit</td>
<td>Area of the MOJ responsible for the service</td>
<td style="text-align: center">string</td>
<td style="text-align: center">-</td>
<td style="text-align: center">yes</td>
</tr>
<tr>
<td>cluster</td>
<td>What cluster are you deploying your namespace. i.e cloud-platform-test-1</td>
<td style="text-align: center">string</td>
<td style="text-align: center"><code>cloud-platform-live-1</code></td>
<td style="text-align: center">no</td>
</tr>
<tr>
<td>contact_email</td>
<td>Contact email address for owner of the application</td>
<td style="text-align: center">string</td>
<td style="text-align: center">-</td>
<td style="text-align: center">yes</td>
</tr>
<tr>
<td>environment</td>
<td>A label for your environment (e.g. dev/staging/&hellip;)</td>
<td style="text-align: center">string</td>
<td style="text-align: center">-</td>
<td style="text-align: center">yes</td>
</tr>
<tr>
<td>github_team</td>
<td>This is your team name as defined by the GITHUB api. This has to match the team name on the Github API</td>
<td style="text-align: center">string</td>
<td style="text-align: center">-</td>
<td style="text-align: center">yes</td>
</tr>
<tr>
<td>is-production</td>
<td></td>
<td style="text-align: center">string</td>
<td style="text-align: center"><code>false</code></td>
<td style="text-align: center">no</td>
</tr>
<tr>
<td>namespace</td>
<td>Namespace you would like to create on cluster <application>-<environment>. i.e myapp-dev</td>
<td style="text-align: center">string</td>
<td style="text-align: center">-</td>
<td style="text-align: center">yes</td>
</tr>
<tr>
<td>owner</td>
<td>Who is the owner/Who is responsible for this application</td>
<td style="text-align: center">string</td>
<td style="text-align: center">-</td>
<td style="text-align: center">yes</td>
</tr>
<tr>
<td>source_code_url</td>
<td>Url of the source code for your application</td>
<td style="text-align: center">string</td>
<td style="text-align: center">-</td>
<td style="text-align: center">yes</td>
</tr>

        </table>
      </div>
<div style="height:1px;font-size:1px;">&nbsp;</div>
<p>Run the following commands to create your namespace and these resources files:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ cd cloud-platform-environments/namespace-resources/
$ terraform init
$ terraform apply
</code></pre></div><p>Fill in your values in response to the prompts.</p>
<p>For <code>var.namespace</code>, this is the name of your (team&rsquo;s) &lsquo;private&rsquo; part of the
cluster. The name of your namespace must be unique across the whole of the
cluster. If you try to create a new namespace using the name of one which
already exists, you will get an error when you try to apply the generated
kubernetes config files (or when our build pipeline applies them on your
behalf).</p>
<p>For &lsquo;real&rsquo; services, this is very unlikely to be a problem - most services have
distinct names, so namespace name conflicts are unlikely. But, if you are
creating a test/dummy namespace in order to learn how the platform works, it&rsquo;s
better to avoid generic names like &#39;dummy&rsquo;, &#39;test&rsquo; or &#39;example&rsquo;. Add something
unique (e.g. your name) to minimise the risk of trying to re-use a name by
mistake.</p>
<p>For <code>var.source_code_url</code>, this should be the URL of an application which is
&#39;cluster-ready&rsquo; to be deployed. If you do not have such an application ready to
go, you can use the reference application which the Cloud Platform team have
prepared <code>git@github.com:ministryofjustice/cloud-platform-reference-app.git</code></p>
<p>Note: The <code>source_code_url</code> is a descriptive label, used by the Cloud Platform
team in supporting your namespace. It does not set up an explicit link between
your namespace and your application&rsquo;s code repository.</p>
<p>At the final prompt &ldquo;Do you want to perform these actions?&rdquo;, enter &ldquo;yes&rdquo;</p>
<p>You can then access your namespace files under
<code>cloud-platform-environments/namespaces/live-1.cloud-platform.service.justice.gov.uk/&lt;your-namespace&gt;</code>,
if satisfied you can then push the changes to your branch and create a pull
request against the
<a href="https://github.com/ministryofjustice/cloud-platform-environments"><code>cloud-platform-environments</code></a>
master repo.</p>
<p>The cloud platform team will review the pull request when it gets opened.  As
soon as the pull request has been approved by the cloud platform team you can
then merge it into the master branch which will kick off the pipeline that
builds the environment. You can check whether the build succeeded or failed in
the <a href="https://mojdt.slack.com/messages/CA5MDLM34/"><code>#cp-build-notification</code></a>
slack channel. This can take about 5 minutes.</p>
<p>Please create <strong>one PR per namespace</strong></p>
<p>i.e. if you need namespaces &#39;myapp-dev&rsquo;, &#39;myapp-staging&rsquo; and &#39;myapp-prod&rsquo;,
please raise a separate PR for each one. This makes it a lot easier for the
cloud platform team to review your PRs.</p>
<h3 id="accessing-your-environments">Accessing your environments</h3><p>Once the pipeline has completed you will be able to check that your environment
is available by running:</p>
<p><code>$ kubectl get namespaces</code></p>
<p>This will return a list of the namespaces within the cluster, and you should
see yours in the list.</p>
<p>You can now run commands in your namespace by appending the <code>-n</code> or
<code>--namespace</code> flag to <code>kubectl</code>. So for instance, to see the pods running in
our <code>myenv-dev</code> namespace, we would run:</p>
<p><code>$ kubectl get pods -n myenv-dev</code> or</p>
<p><code>$ kubectl get pods --namespace myenv-dev</code></p>
<h3 id="next-steps">Next steps</h3><p><a href="/tasks.html#creating-an-ecr-repository">Create an ECR repository</a> to push your application docker image to.</p>
<p>Then you can try <a href="/tasks.html#deploying-a-39-hello-world-39-application-to-the-cloud-platform">deploying an app to Kubernetes manually</a>
by writing some custom YAML files or <a href="/tasks.html#deploying-an-application-to-the-cloud-platform-with-helm">deploying an app with Helm</a>,
a Kubernetes <a href="https://helm.sh/">package manager</a>.</p>
<h3 id="more-information-on-environment-definition">More information on environment definition</h3><p>To set up an environment we create 5 files in that directory:</p>

<ul>
<li><a href="#00-namespace-yaml"><code>00-namespace.yaml</code></a></li>
<li><a href="#01-rbac-yaml"><code>01-rbac.yaml</code></a></li>
<li><a href="#02-limitrange-yaml"><code>02-limitrange.yaml</code></a></li>
<li><a href="#03-resourcequota-yaml"><code>03-resourcequota.yaml</code></a></li>
<li><a href="#04-networkpolicy-yaml"><code>04-networkpolicy.yaml</code></a></li>
</ul>
<p>These files define key elements of the namespace and restrictions we want to
place on it so that we have security and resource allocation properties. We
will use terraform to create these files from templates. We also describe each
of these files in more detail below in case you want to make changes.</p>
<h4 id="00-namespace-yaml"><code>00-namespace.yaml</code></h4><p>The <code>00-namespace.yaml</code> file defines the namespace so that the cluster
Kubernetes knows to create a namespace and what to call it.</p>
<div class="highlight"><pre class="highlight plaintext"><code>apiVersion: v1
kind: Namespace
metadata:
  name: myapp-dev ### This is where you will define your &lt;servicename-env&gt;
  labels:
    name: myapp-dev ### Also your &lt;servicename-env&gt;
</code></pre></div><h4 id="01-rbac-yaml"><code>01-rbac.yaml</code></h4><p>We will also create a <code>RoleBinding</code> resource by adding the <code>01-rbac.yaml</code> file.
This will provide us with <a href="https://kubernetes.io/docs/admin/authorization/rbac/">access
policies</a> on the
namespace we have created in the cluster.</p>
<p>A role binding resource grants the permissions defined in a role to a user or
set of users. A role can be another resource we can create but in this instance
we will reference a Kubernetes default role <code>ClusterRole - admin</code>.</p>
<p>This <code>RoleBinding</code> resource references the <code>ClusterRole - admin</code> to provide
admin permissions on the namespace to the set of users defined under
<code>subjects</code>. In this case, the <code>&lt;yourTeam&gt;</code> GitHub group will have admin access
to any resources within the namespace <code>myapp-dev</code>.</p>
<div class="highlight"><pre class="highlight plaintext"><code>kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: myapp-dev-admins  ### Your namespace with `-admin` e.g. `&lt;servicename-env&gt;-admin`
  namespace: myapp-dev ### Your namespace `&lt;servicename-env&gt;`
subjects:
  - kind: Group
    name: "github:&lt;yourTeam&gt;" ### Make this the name of the GitHub team you want to give access to
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: admin
  apiGroup: rbac.authorization.k8s.io
</code></pre></div><h4 id="02-limitrange-yaml"><code>02-limitrange.yaml</code></h4><p>As we are working on a shared Kubernetes cluster it is useful to put in place
limits on the resources that each namespace, pod and container can use. This
helps to stop us accidentally entering a situation where one service impacts
the performance of another through using more resources than are available.</p>
<p>The first Kubernetes limit we can use is a
<a href="https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">LimitRange</a>
which we define in <code>02-limitrange.yaml</code>.</p>
<p>The LimitRange object specifies two key resource limits on containers,
<code>defaultRequest</code> and <code>default</code>. <code>defaultRequest</code> is the memory and cpu a
container will request on startup. This is what the Kubernetes scheduler uses
to determine whether there is enough space on the cluster to run your
application and what your application will start up with when it is created.
<code>default</code> is the limit at which your application will be killed or throttled.</p>
<p>In <code>02-limitrange.yaml</code> you need to change the value of the <code>namespace</code> key to
match the name of your namespace in the form <code>&lt;service-env&gt;</code>. We have set
default values for the limits in the templates. As you learn more about the
behaviour of your applications on Kubernetes you can change them.</p>
<div class="highlight"><pre class="highlight plaintext"><code>apiVersion: v1
kind: LimitRange
metadata:
  name: limitrange
  namespace: myapp-dev ### Your namespace `&lt;servicename-env&gt;`
spec:
  limits:
  - default:
      cpu: 1000m
      memory: 2Gi
    defaultRequest:
      cpu: 100m
      memory: 128Mi
    type: Container
</code></pre></div><h4 id="03-resourcequota-yaml"><code>03-resourcequota.yaml</code></h4><p>The
<a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">ResourceQuota</a>
object allows us to set a total limit on the resources used in a namespace. As
with the LimitRange, the <code>requests.cpu</code> and <code>requests.memory</code> limits set how
much the namespace will request on creation. The <code>limits.cpu</code> and
<code>limits.memory</code> define the overall hard limits for the namespace.</p>
<p>In <code>03-resourcequota.yaml</code> you need to change the value of the <code>namespace</code> key
to match the name of your namespace in the form <code>&lt;service-env&gt;</code>. We have set
default values for the limits in the templates. As you learn more about the
behaviour of your applications on Kubernetes you can change them.</p>
<div class="highlight"><pre class="highlight plaintext"><code>apiVersion: v1
kind: ResourceQuota
metadata:
  name: namespace-quota
  namespace: myapp-dev ### Your namespace `&lt;servicename-env&gt;`
spec:
  hard:
    requests.cpu: 4000m
    requests.memory: 8Gi
    limits.cpu: 6000m
    limits.memory: 12Gi
</code></pre></div><h4 id="04-networkpolicy-yaml"><code>04-networkpolicy.yaml</code></h4><p>The
<a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">NetworkPolicy</a>
object defines how groups of pods are allowed to communicate with each other
and other network endpoints. By default pods are non-isolated, they accept
traffic from any source. We apply a network policy to restrict where traffic
can come from, allowing traffic only from the <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">ingress
controller</a>
and other pods in your namespace.</p>
<p>In <code>04-networkpolicy.yaml</code> you need to change the value of the <code>namespace</code> key
to match the name of your namespace in the form <code>&lt;service-env&gt;</code>.</p>
<div class="highlight"><pre class="highlight plaintext"><code>apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default
  namespace: myapp-dev ### Your namespace `&lt;servicename-env&gt;`
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector: {}
---
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-ingress-controllers
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          component: ingress-controllers
</code></pre></div>
<h2 id="creating-an-ecr-repository">Creating an ECR repository</h2><h3 id="creating-an-ecr-repository-introduction">Introduction</h3><p>This guide will guide you through the creation of an ECR (Elastic Container Registry) repository for your application&rsquo;s docker image.</p>
<p>AWS resources are provisioned through the <a href="https://github.com/ministryofjustice/cloud-platform-environments/">cloud-platform-environments</a> repository, per environment. Your application might be using multiple namespaces, however you typically only need one image repository and once created in any of them you can copy credentials for it to the others via <code>kubectl get/create secret</code>.</p>
<h4 id="creating-the-registry">Creating the registry</h4><p>1. In order to create the ECR Docker registry, git clone the repo and create a new branch.</p>
<div class="highlight"><pre class="highlight shell"><code>
  <span class="nv">$ </span>git clone git@github.com:ministryofjustice/cloud-platform-environments.git <span class="c">###git clone repo</span>

  <span class="nv">$ </span><span class="nb">cd </span>cloud-platform-environments <span class="c">### navigate into cloud-platform-environments directory.</span>

  <span class="nv">$ </span>git checkout <span class="nt">-b</span> add_ecr   <span class="c">### create and checkout new branch.</span>

</code></pre></div><p>2. You will need to navigate to your service&rsquo;s directory which is located in the namespaces directory. Create a directory named &ldquo;resources&rdquo;.</p>
<div class="highlight"><pre class="highlight shell"><code>
  <span class="nv">$ </span><span class="nb">cd </span>namespaces/live-1.cloud-platform.service.justice.gov.uk/<span class="nv">$your_service</span>  <span class="c">###navigate to your service's directory.</span>

  <span class="nv">$ </span>mkdir resources <span class="c">### make directory called resources</span>

  <span class="nv">$ </span><span class="nb">cd </span>resources

</code></pre></div><p>3. Create a terraform file within the resources directory.</p>
<div class="highlight"><pre class="highlight shell"><code>
  <span class="nv">$ </span>vi ecr.tf  <span class="c">###create a terraform file.</span>

</code></pre></div><p>4. Adapt the definition from the example described in the <a href="https://github.com/ministryofjustice/cloud-platform-terraform-ecr-credentials/tree/master/examples">cloud-platform-terraform-ecr-credentials module</a>; make sure you adjust the values of <code>team_name</code>, <code>repo_name</code> <code>name</code> and <code>namespace</code> to what is appropriate for your environment.</p>
<p>5. git add, commit and push to your branch.</p>
<div class="highlight"><pre class="highlight shell"><code>
  <span class="nv">$ </span>git add ecr.tf

  <span class="nv">$ </span>git commit

  <span class="nv">$ </span>git push

</code></pre></div><p>6. Once your request has been approved and the branches are merged, it will trigger our build pipeline which will apply the Terraform module and create the resources.</p>
<p>For more information about the terraform module being used, please read the documentation <a href="https://github.com/ministryofjustice/cloud-platform-terraform-ecr-credentials">here</a>.</p>
<h3 id="accessing-the-credentials">Accessing the credentials</h3><p>The end result will be a kubernetes <code>Secret</code> inside your environment, called <code>example-team-ecr-credentials-output</code> (or whatever you changed that to); the secret holds IAM access keys to authenticate with the registry and the actual repository URL.</p>
<p>Note: For <code>example-team-ecr-credentials-output</code> you should put whatever you used as the value of the <code>name</code> property of the <code>kubernetes_secret</code> resource in the <code>ecr.tf</code> file you created previously.</p>
<p>To retrieve the credentials:
<code>
kubectl -n &lt;namespace_name&gt; get secret example-team-ecr-credentials-output -o yaml
</code></p>
<p>The values in kubernetes <code>Secrets</code> are always <code>base64</code> encoded so you will have to decode them before you can use them outside kubernetes. Inside the cluster, the nodes already have access to the ECR so you don&rsquo;t need to make any changes.</p>
<p>This can be done at the command line using the following:
<code>
echo QWxhZGRpbjpvcGVuIHNlc2FtZQ== | base64 --decode; echo
</code></p>
<h4 id="setting-up-circleci">Setting up CircleCI</h4><p>In your CircleCI project, go to the settings (the cog icon) and select &lsquo;AWS Permissions&rsquo; from the left hand menu. Fill in the IAM credentials and CircleCI will be able to use ECR images. For more information please see <a href="https://circleci.com/docs/2.0/private-images/">the official docs</a>.</p>
<h3 id="creating-an-ecr-repository-next-steps">Next steps</h3><p>Try <a href="/tasks.html#deploying-an-application-to-the-cloud-platform-with-helm">deploying an app</a> with <a href="https://helm.sh/">Helm</a>, a Kubernetes package manager, or <a href="/tasks.html#deploying-a-39-hello-world-39-application-to-the-cloud-platform">deploying manually</a> by writing some custom YAML files.</p>

<h2 id="creating-a-route-53-hosted-zone">Creating a Route 53 Hosted Zone</h2><h3 id="overview">Overview</h3><p>This short guide will run through the process of creating a <a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/AboutHZWorkingWith.html">Route 53 Hosted Zone</a> through Terraform for your environment.</p>
<h3 id="pre-requisites">Pre-Requisites</h3><p>This guide assumes you have an environment already created in the <code>Live-1</code> cluster and defined in the <a href="https://github.com/ministryofjustice/cloud-platform-environments">cloud-platform-environments repository</a>.</p>
<h3 id="terraform-files">Terraform files</h3><p>Copy the Terraform resource code below and save it into the respective 2 files in the <code>[your namespace]/resources</code> directory in the <a href="https://github.com/ministryofjustice/cloud-platform-environments">cloud-platform-environments repository</a>:</p>

<ul>
<li><code>main.tf</code></li>
<li><code>variables.tf</code></li>
</ul>
<h4 id="main-tf">main.tf</h4><div class="highlight"><pre class="highlight plaintext"><code>resource "aws_route53_zone" "example_team_route53_zone" {
  name = "${var.domain}"

  tags {
    business-unit          = "${var.business-unit}"
    application            = "${var.application}"
    is-production          = "${var.is-production}"
    environment-name       = "${var.environment-name}"
    owner                  = "${var.team_name}"
    infrastructure-support = "${var.infrastructure-support}"
  }
}

resource "kubernetes_secret" "example_route53_zone_sec" {
  metadata {
    name      = "example-route53-zone-output"
    namespace = "${var.namespace}"
  }

  data {
    zone_id   = "${aws_route53_zone.example_team_route53_zone.zone_id}"
  }
}

</code></pre></div><h4 id="variables-tf">variables.tf</h4><div class="highlight"><pre class="highlight plaintext"><code>variable "domain" {
  description = "The domain you intend to create. This should be a part of either parent zone of *.service.gov.uk or *.service.justice.gov.uk"
}

variable "application" {}

variable "business-unit" {
  description = "Area of the MOJ responsible for the service."
  default     = "mojdigital"
}

variable "team_name" {
  description = "The name of your development team"
}

variable "environment-name" {
  description = "The type of environment you're deploying to."
}

variable "infrastructure-support" {
  description = "The team responsible for managing the infrastructure. Should be of the form team-email."
}

variable "is-production" {
  default = "false"
}
</code></pre></div><h3 id="creating-the-resource">Creating the resource</h3><p>Make sure to change placeholder values to what is appropriate, with the 2 files saved, commit your changes to a branch and raise a pull request. Once approved, you can merge and the changes will be applied. Shortly after, you should be able to access the <code>Zone_ID</code> as Secret on kubernetes in your namespace.</p>
<p>Please contact the Cloud Platform team via the <code>#ask-cloud-platform</code> Slack channel. Provide them with the zone_ID, and they will verify that your hosted zone has been created successfully.</p>

<h2 id="adding-aws-resources-to-your-environment">Adding AWS resources to your environment</h2><p>Through the <a href="https://github.com/ministryofjustice/cloud-platform-environments/">cloud-platform-environments</a> repository, you can provision AWS resources for your environments. This is done using terraform and more specifically, terraform modules we provide for use on the Cloud Platform.</p>
<p>The documentation for the modules lives in each module&rsquo;s repository and you can find a list of the available ones below.</p>
<h3 id="available-modules">Available modules</h3><p>The updated list of terraform modules provided by the MoJ are available here: <a href="https://github.com/ministryofjustice/cloud-platform#terraform-modules">Terraform Modules</a></p>
<div class="highlight"><pre class="highlight plaintext"><code>!!!  WARNING  !!!

Be aware that the latest versions of these modules will only be compatible
with Live-1.

If you are planing on deploying them against Live-0, please read their
respective READMEs carefully for instruction.

</code></pre></div><h3 id="usage">Usage</h3><p>In each terraform module repository, you will find a directory named <code>example</code> which includes sample configuration for use in Cloud Platform.</p>
<p>In your namespace&rsquo;s path in the <a href="https://github.com/ministryofjustice/cloud-platform-environments/">cloud-platform-environments</a> repository, create a directory called <code>resources</code> (if you have not created one already) and refer to the module&rsquo;s example to define your resources.</p>
<p>Each example will have some global configuration defined, however, this should only be declared once, regardless of the number of modules used:</p>
<div class="highlight"><pre class="highlight plaintext"><code>terraform {
  backend "s3" {}
}

provider "aws" {
  region = "eu-west-1"
}
</code></pre></div><p>Additionally, some example might define variables; again, these should only be declared once per namespace:</p>
<div class="highlight"><pre class="highlight plaintext"><code>variable "cluster_name" {}

variable "cluster_state_bucket" {}
</code></pre></div><p>The main README file of each module repository will list all the available configuration options that can be passed to the module.</p>
<h4 id="outputs">Outputs</h4><p>Each module will have its own outputs. These expose useful information, such as endpoints, credentials etc. The module examples all use a common approach: they employ the <code>kubernetes_secret</code> terraform resource to push the outputs straight into your environment in the form of a <code>Secret</code> which you could then extract information from or directly reference in <code>Pods</code>.</p>
<p>This is currently the only supported way of accessing terraform outputs.</p>
<h4 id="versioning">Versioning</h4><p>All modules are versioned. This allows us to implement changes without breaking existing resources. To use a specific version of a module you need to define it in the <code>source</code> attribute by specifying the <code>ref</code> attribute in the query string of the source URL:</p>
<div class="highlight"><pre class="highlight plaintext"><code>module "my_module" {
  source = "https://github.com/ministryofjustice/cloud-platform-terraform-ecr-credentials?ref=1.0"
}
</code></pre></div><p>Please check the version badge for the module you are using (visit the web page of the module&rsquo;s github repository - the version badge will be just below the heading), and make sure you are using the latest version of the module in your configuration.</p>
<p>Upgrading to a new major version usually means that the configured resource will have to be re-created by terraform.</p>
<p>Refer to the <a href="http://terraform.io/docs/modules">terraform documentation on modules</a> for more information on usage.</p>


<h2 id="deploying-a-39-hello-world-39-application-to-the-cloud-platform">Deploying a &lsquo;Hello World&rsquo; application to the Cloud Platform</h2><p><a href="../images/k8s-cluster-application-deployment-process.png" target="_blank" rel="noopener noreferrer"><img src="/images/k8s-cluster-application-deployment-process.png" alt="Deployment Process Diagram" /></a></p>
<h3 id="deploying-a-39-hello-world-39-application-to-the-cloud-platform-overview">Overview</h3><p>The aim of this guide is to walkthrough the process of deploying an application into the Cloud Platform.</p>
<p>This guide uses a pre-configured <a href="https://github.com/ministryofjustice/cloud-platform-helloworld-ruby-app">&ldquo;Hello World&rdquo; application</a> as an example of how to deploy your own. This application merely returns a static HTML response, and has no dependencies. Later examples will use more representative applications.</p>
<p>The process we will follow consists of the following stages:</p>

<ul>
<li>Build a docker image from the demo application</li>
<li>Tag the image and push it to your ECR</li>
<li>Edit kubernetes config files</li>
<li>Apply the config files to make the cluster run our application</li>
</ul>
<p>The process of building an image and pushing it to an ECR will normally be carried out by a build pipeline. For this initial walkthrough, we will go through these steps manually. Later we will go through an example of setting up a <a href="https://circleci.com">CircleCI</a> job to do this automatically. The steps are similar if you&rsquo;re using other CI/CD tools such as <a href="https://travis-ci.org">TravisCI</a>.</p>
<h3 id="prerequisites">Prerequisites</h3><p>This guide assumes the following:</p>

<ul>
<li><a href="https://www.docker.com">Docker</a> is installed and configured.</li>
<li>Kubectl is installed and configured (<code>brew install kubectl</code> on a Mac with <a href="https://brew.sh">Homebrew</a> installed)</li>
<li>AWS CLI is installed (<code>brew install awscli</code> on a Mac with <a href="https://brew.sh">Homebrew</a> installed)</li>
<li>You have <a href="/tasks.html#creating-a-cloud-platform-environment">created an environment for your application</a></li>
<li>You have <a href="/tasks.html#creating-an-ecr-repository">created an Amazon ECR</a> to host your docker image</li>
</ul>
<h3 id="step-1-build-your-docker-image">Step 1 - Build your docker image</h3>
<ul>
<li>Clone the <a href="https://github.com/ministryofjustice/cloud-platform-helloworld-ruby-app">demo application</a></li>
</ul>
<div class="highlight"><pre class="highlight shell"><code>git clone https://github.com/ministryofjustice/cloud-platform-helloworld-ruby-app  
<span class="nb">cd </span>cloud-platform-helloworld-ruby-app
</code></pre></div>
<ul>
<li>Build the docker image</li>
</ul>
<div class="highlight"><pre class="highlight shell"><code>docker build <span class="nt">-t</span> <span class="o">[</span>ECR Team Name]/[ECR Repository Name] <span class="nb">.</span>
</code></pre></div><p>The <code>ECR Team Name</code> and <code>ECR Repository Name</code> must match the <code>team_name</code> and <code>repo_name</code> values you entered when you created the ECR via <a href="https://github.com/ministryofjustice/cloud-platform-environments">cloud-platform-environments</a> Github repository.</p>
<p>You can find them in the file <code>namespaces/live-1.cloud-platform.service.justice.gov.uk/[YOUR ENVIRONMENT]/resources/ecr.tf</code>.</p>
<h4 id="amazon-ecr-terminology">Amazon ECR Terminology</h4><p>Amazon ECR uses the terms <code>repository</code> and <code>image</code> in a rather confusing way. Normally, you would think of a docker image repository as holding multiple images, each with a different name, where each image can have multiple tags. Amazon ECR conflates the repository and image - i.e. you can only push images with the same name to a given ECR.</p>
<p>So, if you created your ECR using the team_name <code>davids-dummy-team</code> and repo_name <code>davids-dummy-app</code>, then you can only push images to the ECR if they are named <code>davids-dummy-team/davids-dummy-app:[something]</code>. You are free to change the tag of the image (shown as [something], here), and some teams overload the tag value as a way to store multiple completely different docker images in a single ECR.</p>
<h3 id="step-2-push-the-image-to-your-ecr">Step 2 - Push the image to your ECR</h3><h4 id="authenticating-to-your-docker-image-repository">Authenticating to your docker image repository</h4><p>You must authenticate to the docker image repository before you can push an image to it.</p>
<p>To authenticate to your ECR, you will need the <code>access_key_id</code> and <code>secret_access_key</code> which were created for you when you created your ECR. To retrieve these, see the <a href="/tasks.html#accessing-the-credentials">this section</a> of this guide.</p>
<p><em>tl;dr</em> use this command:</p>
<div class="highlight"><pre class="highlight plaintext"><code>  kubectl -n [namespace_name] get secret [name of your secret] -o yaml
</code></pre></div><p>Don&rsquo;t forget to base64 decode the <code>access_key_id</code> and <code>secret_access_key</code> values before using them.</p>
<div class="highlight"><pre class="highlight plaintext"><code>  echo 'your_access_key_id_value' | base64 --decode
</code></pre></div><p>Once you have your <code>access_key_id</code> and <code>secret_access_key</code>, set up an AWS profile using the AWS cli tool.</p>
<div class="highlight"><pre class="highlight plaintext"><code>  aws configure
</code></pre></div><p>Supply your credentials when prompted.</p>
<p>This guide assumes you are using these credentials in your <code>default</code> AWS profile. If you have used a different name for this AWS profile, please add <code>--profile [YOUR PROFILE]</code> to all of the following AWS commands.</p>
<h4 id="authenticating-with-the-repository">Authenticating with the repository</h4><p>Use the following command to login to Amazon ECR</p>
<div class="highlight"><pre class="highlight plaintext"><code>  $(aws ecr get-login --no-include-email --region eu-west-2)
</code></pre></div><p>Note: The output of the <code>aws ecr...</code> command is a long <code>docker login...</code> command. Including the <code>$(...)</code> around the command executes this output in the context of the current shell</p>
<p>The output of the above should include <code>Login Succeeded</code> to confirm you have authenticated to the docker image repository.</p>
<p>These credential are valid for 12 hours. So, if you are working through this example over a longer period, you will have to login again, e.g. the following day.</p>
<h4 id="pushing-your-docker-image-to-the-ecr">Pushing your docker image to the ECR</h4><p>All of the MoJ Digital docker images are stored within the same Cloud Platform AWS account (cloud-platform-aws).</p>
<p>Your specific ECR will be:</p>
<div class="highlight"><pre class="highlight plaintext"><code>  754256621582.dkr.ecr.eu-west-2.amazonaws.com/[team_name]/[repo_name]
</code></pre></div><p>Where <code>team_name</code> and <code>repo_name</code> are the values from your <code>ecr.tf</code> file.</p>
<p>Ensure the Docker image for your application has been built and is stored locally on your machine.</p>
<div class="highlight"><pre class="highlight plaintext"><code>  docker build -t [team_name]/[repo_name] .
</code></pre></div><p>Now we need to tag the image so it can be pushed into the correct repository.</p>
<div class="highlight"><pre class="highlight plaintext"><code>  docker tag [team_name]/[repo_name]:latest 754256621582.dkr.ecr.eu-west-2.amazonaws.com/[team_name]/[repo_name]:latest
</code></pre></div><p>Finish by running the last command to push the image to your repository.</p>
<div class="highlight"><pre class="highlight plaintext"><code>docker push 754256621582.dkr.ecr.eu-west-2.amazonaws.com/[team_name]/[repo_name]:latest
</code></pre></div><h3 id="step-3-configure-your-namespace-in-the-kubernetes-cluster">Step 3 - Configure your namespace in the Kubernetes Cluster</h3><p>To deploy an application to the Cloud Platform, a number of deployment files must first be configured. You can find examples of these in the <code>kubectl_deploy</code> directory of the <a href="https://github.com/ministryofjustice/cloud-platform-helloworld-ruby-app">demo application</a>, but you will need to edit your copy to replace some of the values to use your kubernetes cluster environment and docker image.</p>
<p><em>Tip:</em> You can find more deployment config info <a href="https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/">in the kubernetes developer documentation</a>.</p>
<h4 id="deployment-yaml">deployment.yaml</h4><div class="highlight"><pre class="highlight plaintext"><code>apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: helloworld-rubyapp
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: helloworld-rubyapp
    spec:
      containers:
      - name: rubyapp
        image: 754256621582.dkr.ecr.eu-west-2.amazonaws.com/davids-dummy-team/davids-dummy-app:latest
        ports:
        - containerPort: 4567
</code></pre></div><p>This file tells Kubernetes to run a single pod (<code>replicas: 1</code>) containing a single container based on a specific docker image from your ECR.</p>
<p>Change the image value to refer to the image you pushed to your ECR in the earlier step.</p>
<p>The <code>service.yaml</code> and <code>ingress.yaml</code> files make it possible to access your application from the outside world.</p>
<h4 id="service-yaml">service.yaml</h4><p>Service files are used to specify port and protocol information for your application and are also used to bundle together the set of pods created by the deployment.</p>
<p>This exposes port 4567 internally to your namespace. i.e. it enables pods and other objects within your namespace to connect to port 4567 of your container.</p>
<div class="highlight"><pre class="highlight plaintext"><code>apiVersion: v1
kind: Service
metadata:
  name: rubyapp-service
  labels:
    app: rubyapp-service
spec:
  ports:
  - port: 4567
    name: http
    targetPort: 4567
  selector:
    app: helloworld-rubyapp
</code></pre></div><p>The value of <code>spec/selector/app</code> must be the same as <code>spec/template/metadata/labels/app</code> in the <code>deployment.yml</code> file.</p>
<p><em>Tip:</em> You can find more info on service definition in the <a href="https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/">kubernetes docs</a>.</p>
<h4 id="ingress-yaml">ingress.yaml</h4><p>Ingress files are to use to define external access to the application.</p>
<p>This creates an <a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/">ingress controller</a> to enable network connections from outside of the cluster.</p>
<p>Note: Because we are specifying <code>http</code>, this ingress controller will expose port 80, and will redirect connections to port 4567 of the named service.</p>
<div class="highlight"><pre class="highlight plaintext"><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: helloworld-rubyapp-ingress
spec:
  tls:
  - hosts:
    - helloworld-rubyapp.apps.live-1.cloud-platform.service.justice.gov.uk
  rules:
  - host: helloworld-rubyapp.apps.live-1.cloud-platform.service.justice.gov.uk
    http:
      paths:
      - path: /
        backend:
          serviceName: rubyapp-service
          servicePort: 4567
</code></pre></div><p>The value of <code>serviceName</code> and <code>servicePort</code> must be the same as those specified in the <code>service.yml</code> file.</p>
<p>Change the <code>helloworld-rubyapp</code> prefix of the <code>host</code> string to the value you want to use as the hostname part of the URL on which your application will be available to the world (do not change the <code>.apps.live-1.cloud-platform...</code> part).</p>
<p><em>Tip:</em> You can find more info on ingress in the <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">kubernetes docs</a></p>
<h3 id="step-4-deploy-the-application">Step 4 - Deploy the application</h3><p>With all of the deployment files configured, you can now deploy your application to the Cloud Platform.</p>
<p>Start by listing the namespaces on the cluster you are connected to:</p>
<div class="highlight"><pre class="highlight plaintext"><code>  kubectl get namespaces
</code></pre></div><p>The list that gets returned should include the one you <a href="/tasks.html#creating-a-cloud-platform-environment">created earlier</a>, here we assume it is called <code>davids-dummy-dev</code>. Please change that to whatever your namespace (environment) is called, in all of the following commands.</p>
<p>To deploy your application run the following command. This command assumes that the current directory is the root directory of your working copy of the <a href="https://github.com/ministryofjustice/cloud-platform-helloworld-ruby-app">demo application</a>. i.e. <code>kubectl_deploy</code> points to the directory where the deployment files are stored.</p>
<div class="highlight"><pre class="highlight plaintext"><code>  kubectl create --filename kubectl_deploy --namespace davids-dummy-dev
</code></pre></div><p>You have to specify the namespace you want to deploy to, this should be the namespace of the environment you created.</p>
<p>Confirm the deployment with:</p>
<div class="highlight"><pre class="highlight plaintext"><code>  kubectl get pods --namespace davids-dummy-dev
</code></pre></div><h3 id="interacting-with-the-application">Interacting with the application</h3><p>With the application deployed into the Cloud Platform, there are a few ways of managing it:</p>

<ul>
<li><strong>View pods</strong> - <code>kubectl get pods --namespace davids-dummy-dev</code></li>
<li><strong>Check host</strong> - <code>kubectl get ingress --namespace davids-dummy-dev</code></li>
<li><strong>Delete application</strong> - <code>kubectl delete --filename kubectl_deploy --namespace davids-dummy-dev</code></li>
<li><strong>Shell into container</strong> - <code>kubectl exec --stdin --tty --namespace davids-dummy-dev [POD-NAME] -- /bin/sh</code></li>
</ul>
<p>For <code>[POD-NAME]</code> use the value returned by the <code>kubectl get pods...</code> command</p>
<p><em>Tip:</em> You can find more about the <code>kubectl</code> command <a href="https://kubernetes.io/docs/reference/kubectl/overview/">here</a></p>
<p>You should be able to view the app. at the following URL:</p>
<div class="highlight"><pre class="highlight plaintext"><code>  curl -L https://helloworld-rubyapp.apps.live-1.cloud-platform.service.justice.gov.uk
</code></pre></div><p>Don&rsquo;t forget to change <code>helloworld-rubyapp</code> to whatever hostname you chose earlier.</p>
<p>You need the <code>-L</code> flag to make curl follow the 308 redirect response that it will receive from the ingress controller. If you view the URL in a web browser, it should just work.</p>
<p>If you are wondering why https &#39;just works&rsquo;, there is some magic behind the scenes whereby a LetsEncrypt SSL certificate is created for you, and applied to your ingress. A future user guide article will describe this in more detail.</p>
<h3 id="add-http-basic-authentication">Add HTTP Basic Authentication</h3><p>The application can be accessed from the internet at:</p>
<div class="highlight"><pre class="highlight plaintext"><code>https://helloworld-rubyapp.apps.live-1.cloud-platform.service.justice.gov.uk
</code></pre></div><p>As per the <a href="https://ministryofjustice.github.io/technical-guidance/standards/naming-domains/#justicegovuk">guidance for domain names</a>, our application should have some authentication to prevent citizens accidentally mistaking development websites for live government services. Whilst this isn&rsquo;t much of a problem with a &#39;hello world&rsquo; site, it could be an issue for sites using the GDS prototype kit, which look exactly like live services. So, let&rsquo;s add <a href="https://en.wikipedia.org/wiki/Basic_access_authentication">http basic authentication</a> to our application.</p>
<p>We can do this by amending our <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress</a>, which is the kubernetes object that routes traffic to our application from the internet. We&rsquo;ll create an encrypted username and password, and then store that in a <a href="https://kubernetes.io/docs/concepts/configuration/secret/">kubernetes secret</a>. Then, we will update our Ingress to use basic authentication, and tell it where to find the credentials.</p>
<h4 id="create-the-username-and-password">Create the username and password</h4><p>First we&rsquo;ll use the <a href="https://httpd.apache.org/docs/2.4/programs/htpasswd.html">htpasswd</a> program to create a one-way hashed username and password. htpasswd is a system program which should be pre-installed on your computer.</p>
<p>To create a username &#39;bob&rsquo; with password &#39;password123&rsquo; in a file called &#39;auth&rsquo;, run the following command:</p>
<div class="highlight"><pre class="highlight shell"><code><span class="nv">$ </span>htpasswd <span class="nt">-cb</span> auth bob password123
</code></pre></div><p>Whatever value you use for your password (which should <em>not</em> be &#39;password123&rsquo; be sure to make a note of it now - it will not be visible again.</p>
<p>Kubernetes secrets are stored as base64-encoded text strings, so we need to run the &#39;auth&rsquo; file through base64 (which should also be pre-installed):</p>
<div class="highlight"><pre class="highlight shell"><code><span class="nv">$ </span><span class="nb">cat </span>auth | base64
</code></pre></div><p>This will output the string we need to store in our secret. For the purpose of this tutorial, I&rsquo;m going to use <code>Ym9iOiRhcHIxJFVXQ1cxWDlvJGt3WDdoMTFZemNYdmVseHE2UFV2VzAK</code> Please substitute the value you got from the step above, using your own choice of username and password.</p>
<p>Note: only the hash of the password is stored in this string, not the password itself, so it is safe to store this string in a public github repository.</p>
<p>You can now delete the &#39;auth&rsquo; file - we don&rsquo;t need it anymore.</p>
<h4 id="create-the-kubernetes-secret">Create the kubernetes secret</h4><p>Create a file called <code>kubectl_deploy/secret.yaml</code> containing the following:</p>
<div class="highlight"><pre class="highlight yaml"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Secret</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">basic-auth</span>
<span class="na">data</span><span class="pi">:</span>
  <span class="na">auth</span><span class="pi">:</span> <span class="s">Ym9iOiRhcHIxJFVXQ1cxWDlvJGt3WDdoMTFZemNYdmVseHE2UFV2VzAK</span>
</code></pre></div><p>Remember to substitute your base64-encoded credentials string.</p>
<p>The next time we apply our yaml files to our namespace, this file will create a kubernetes secret called &#39;basic-auth&rsquo;. Now we need to configure our Ingress to use it.</p>
<h4 id="configure-the-ingress">Configure the Ingress</h4><p>To configure our ingress to use basic authentication, we just need to add a couple of lines to the metadata section of <code>kubectl_deploy/ingress.yaml</code></p>
<p>Replace this:</p>
<div class="highlight"><pre class="highlight yaml"><code><span class="nn">...</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">helloworld-rubyapp-ingress</span>
<span class="na">spec</span><span class="pi">:</span>
<span class="nn">...</span>
</code></pre></div><p>&hellip;with this:</p>
<div class="highlight"><pre class="highlight yaml"><code><span class="nn">...</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">helloworld-rubyapp-ingress</span>
  <span class="na">annotations</span><span class="pi">:</span>
    <span class="s">nginx.ingress.kubernetes.io/auth-type</span><span class="pi">:</span> <span class="s">basic</span>
    <span class="s">nginx.ingress.kubernetes.io/auth-secret</span><span class="pi">:</span> <span class="s">basic-auth</span>
<span class="na">spec</span><span class="pi">:</span>
<span class="nn">...</span>
</code></pre></div><p>This tells the Ingress what kind of authentication to use, and which kubernetes secret contains the credentials.</p>
<h4 id="apply-the-changes">Apply the changes</h4><p>All that remains is to apply our updated yaml files in almost exactly the same way as we did before, when we deployed the application:</p>
<div class="highlight"><pre class="highlight shell"><code>kubectl apply <span class="nt">--filename</span> kubectl_deploy <span class="nt">--namespace</span> <span class="o">[</span>your namespace]
</code></pre></div><p>The only difference is that we are running <code>kubectl apply</code> instead of <code>kubectl create</code>.</p>
<p>You should see output like this:</p>
<div class="highlight"><pre class="highlight shell"><code>deployment.extensions <span class="s2">"helloworld-rubyapp"</span> unchanged
ingress.extensions <span class="s2">"helloworld-rubyapp-ingress"</span> configured
secret <span class="s2">"basic-auth"</span> created
service <span class="s2">"rubyapp-service"</span> unchanged
</code></pre></div><p>Now, if you reload the browser page showing the &#39;Hello world&rsquo; message from the application, you will be prompted to enter the username and password.</p>

<h2 id="deploying-a-multi-container-application-to-the-cloud-platform">Deploying a multi-container application to the Cloud Platform</h2><h3 id="deploying-a-multi-container-application-to-the-cloud-platform-overview">Overview</h3><p>This section goes through the process of deploying a <a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app">demo application</a> consisting of several components, each running in its own container.</p>
<p>Please see the <a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app#multi-container-demo-application">application README</a> for a description of the different components, and how they connect. You can also run the application locally via docker-compose to confirm that it works as it should.</p>
<h3 id="running-in-the-kubernetes-cluster">Running in the Kubernetes Cluster</h3><p>In the <a href="https://github.com/ministryofjustice/cloud-platform">Cloud Platform</a> kubernetes cluster, the application will be set up like this:</p>
<p><a href="../images/multi-container-k8s.png" target="_blank" rel="noopener noreferrer"><img src="/images/multi-container-k8s.png" alt="Multi-container architecture diagram" /></a></p>
<p>Each container needs a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">Deployment</a> which will contain a <a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/">Pod</a>. <a href="https://kubernetes.io/docs/concepts/services-networking/service/">Services</a> make pods available on the cluster&rsquo;s internal network, and an <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress</a> exposes one or more services to the outside world.</p>
<h3 id="create-an-rds-instance">Create an RDS instance</h3><p>The application database will be an Amazon RDS instance. To create this, refer to the <a href="https://github.com/ministryofjustice/cloud-platform-terraform-rds-instance">cloud platform RDS</a> repository, and create a terraform file in your sub-directory of the <a href="https://github.com/ministryofjustice/cloud-platform-environments">cloud platform environments</a> repository (you will need to raise a PR for this, and get the cloud platform team to approve it).</p>
<p>For more information see <a href="/tasks.html#adding-aws-resources-to-your-environment">Adding AWS resources to your environment</a>.</p>
<p>The <a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app">demo application</a>, and this guide, assumes a DATABASE_URL environment variable, exported by the terraform RDS module as follows:</p>
<div class="highlight"><pre class="highlight plaintext"><code># rds.tf
...
data {
  url = postgres://${module.module_name.database_username}:${module.module_name.database_password}@${module.module_name.rds_instance_endpoint}/${module.module_name.database_name}
}
...
</code></pre></div><p>Please ensure that your <code>rds.tf</code> file exports a database <code>url</code> value in this way (changing <code>module_name</code> to match the name you use in your <code>rds.tf</code> file).</p>
<h4 id="connecting-to-your-rds-instance-from-your-local-machine">Connecting to your RDS instance from your local machine</h4><p>This is not required for this tutorial.</p>
<p>If you need to access an RDS instance from your local machine, you can find instructions for doing so <a href="https://github.com/ministryofjustice/cloud-platform-terraform-rds-instance#access-outside-the-cluster">here</a>.</p>
<h3 id="build-docker-images-and-pushing-to-ecr">Build docker images and pushing to ECR</h3><p>As before, we need to build docker images which we will push to our <a href="https://aws.amazon.com/ecr/">Amazon ECR</a>.</p>
<p>Please carry out the following steps on your own working copy of the <a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app">demo application</a>.</p>
<p>For <code>team_name</code> and <code>repo_name</code> please use the values from your <code>ecr.tf</code> file, when you <a href="/tasks.html#creating-an-ecr-repository">created your ECR</a>.</p>
<div class="highlight"><pre class="highlight plaintext"><code>cd rails-app
docker build -t [team_name]/[repo_name]:rails-app .
docker tag [team_name]/[repo_name]:rails-app 754256621582.dkr.ecr.eu-west-2.amazonaws.com/[team_name]/[repo_name]:rails-app-1.0
docker push 754256621582.dkr.ecr.eu-west-2.amazonaws.com/[team_name]/[repo_name]:rails-app-1.0
</code></pre></div><p>Note that we are overloading the tag value to push multiple different containers to a single Amazon ECR. This is because of a quirk in the way Amazon ECR refers to <code>image repositories</code> and <code>images</code>.</p>
<p>Repeat the steps above for the <code>content-api</code> and <code>worker</code> sub-directories (changing <code>rails-app</code> as appropriate, in the commands).</p>
<p>The <code>makefile</code> in the <a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app">demo application</a> contains commands to make this process easier. Don&rsquo;t forget to edit the values for <code>TEAM_NAME</code>, <code>REPO_NAME</code> and <code>VERSION</code> appropriately.</p>
<h3 id="kubernetes-configuration">Kubernetes configuration</h3><p>As per the diagram, we need to configure six objects in kubernetes - 3 deployments, 2 services and 1 ingress.</p>
<p>You can see these YAML config files in the <code>kubernetes_deploy</code> directory of the <a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app">demo application</a>.</p>
<p>Note: The yaml files in the github repository have the namespace name <code>davids-dummy-dev</code>, team name <code>davids-dummy-team</code> and application name <code>davids-dummy-app</code>. You will need to change these to the corresponding values for your situation, and also the full names of your docker images.</p>
<p>You may also need to change the <code>host</code> entry in the <code>ingress.yaml</code> file, if someone else has deployed an instance of the demo application using the same hostname.</p>
<p>In <code>rails-app-deployment.yaml</code> and <code>worker-deployment.yaml</code> you can see the configuration for two environment variables:</p>

<ul>
<li><code>DATABASE_URL</code> is retrieved from the kubernetes secret which was created when the RDS instance was set up</li>
<li><code>CONTENT_API_URL</code> uses the name and port defined in <code>content-api-service.yaml</code></li>
</ul>
<h3 id="deploying-to-the-cluster">Deploying to the cluster</h3><p>After you have built and pushed your docker images, and made the corresponding changes to the <code>kubernetes_deploy/*.yaml</code> files, you can apply the configuration to your namespace in the kubernetes cluster:</p>
<div class="highlight"><pre class="highlight plaintext"><code>  kubectl apply --filename kubernetes_deploy --namespace [your namespace]
</code></pre></div><h3 id="deploying-a-multi-container-application-to-the-cloud-platform-interacting-with-the-application">Interacting with the application</h3><p>You should be able to view the application in your browser at:</p>
<div class="highlight"><pre class="highlight plaintext"><code>  https://multi-container-demo.apps.live-1.cloud-platform.service.justice.gov.uk/
</code></pre></div><p>It should behave in the same way as when you were running it locally via docker-compose.</p>
<h3 id="further-development">Further Development</h3><p>After you deployed and Interacting with multi-container application and would like to create Monitoring dashboard , Custom Alerts and Cronjobs for your application use this working examples:</p>

<ul>
<li><p><strong>Grafana dashboard</strong> - Follow the <a href="/tasks.html#creating-dashboards">guide</a> on how to create a Graphana dashboard, make corresponding changes to <a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app/tree/grafana-dashboard-v1.0/k8s_additional_resources">monitoring-grafana-dashboard.yaml</a> file and apply the configmap to your namespace in the kubernetes cluster, which will show your Grafana-Dashboard <a href="https://grafana.cloud-platform.service.justice.gov.uk">here</a>.</p>
<div class="highlight"><pre class="highlight plaintext"><code>  kubectl apply --filename monitoring-grafana-dashboard.yaml --namespace [your namespace]
</code></pre></div></li>
<li><p><strong>Custom Alerts</strong> - Follow the <a href="https://user-guide.cloud-platform.service.justice.gov.uk/tasks.html#creating-your-own-custom-alerts">guide</a> on how to create Custom Alerts, make corresponding changes to <a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app/tree/custom-alerts-v1.0/k8s_additional_resources">prometheus-app-alert.yaml</a> file and apply the PrometheusRule to your namespace in the kubernetes cluster, this will create custom alerts for your application.</p>
<div class="highlight"><pre class="highlight plaintext"><code>  kubectl apply --filename prometheus-app-alert.yaml --namespace [your namespace]
</code></pre></div></li>
<li><p><strong>Cronjobs</strong> - Follow the <a href="https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#creating-a-cron-job">guide</a> on creating a Cronjob, make corresponding changes to <a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app/tree/cronjob-example-v1.0/k8s_additional_resources">cronjob-ecr.yaml</a> file and apply the Cronjob to your namespace in the kubernetes cluster, this runs a job periodically on a given schedule to delete untagged images in the ecr-repo, this will help to limit the count of Images in the ecr-repo.</p>
<div class="highlight"><pre class="highlight plaintext"><code>  kubectl apply --filename cronjob-ecr.yaml --namespace [your namespace]
</code></pre></div></li>
</ul>


<h2 id="deploying-with-helm-and-circleci">Deploying with Helm and CircleCI</h2>
<p><h3 id="prerequisite-for-live-1-deployment">Prerequisite for Live-1 deployment</h3><p><strong>This section only applies to applications aiming to be deployed to Live-1.</strong></p>
<p>In Live-1, the Cloud Platform team introduced <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">Pod Security Policies</a>, to
tighten the security on this production cluster.  Two policies have been
applied to the cluster, <em>restricted</em> and <em>privileged</em>.</p>
<p>By default, any new environment/namespace on Live-1 will be assigned the
<em>restricted</em> policy</p>
<h4 id="impact">Impact</h4><p>The main impact of this <em>restricted</em> policy is that it prevents pods/containers
from running as the root user. A container&rsquo;s user is usually defined in its
Dockerfile. If no user is explicitly specified in the Dockerfile, the chances
are that it will run as root.</p>
<p>It is important to understand that not being able to use root also implies that
it is impossible to bind to a privileged port (e.g. 80, 443).</p>
<p>The policies only take effect after the container has started. Anything in the
Dockerfile can be run as root at image build time e.g. to install required
software.</p>
<h4 id="how-to-adapt-to-the-pod-security-policies">How to adapt to the pod security policies</h4><p>Most of the time, your application&rsquo;s Dockerfile can be easily adapted by:</p></p>

<ul>
<li><p>Creating a user with a UID which is greater than 1 (which is the UID
reserved for root)</p></li>
<li><p>Giving this user any required permissions to access the files/directories
the application requires.</p></li>
<li><p>Adding a <code>USER</code> clause in your Dockerfile to switch to your non-root user</p></li>
</ul>

<p>Example:</p>

<div class="highlight"><pre class="highlight yaml"><code><span class="s">FROM busybox</span>

<span class="s">RUN mkdir -p /opt/myFolder &amp;&amp; \</span>
    <span class="s">adduser --disabled-password myNewUser -u 1001 &amp;&amp; \</span>
    <span class="s">chown -R myNewUser:myNewUser /opt/myFolder</span>

<span class="s">USER 1001</span>

<span class="s">CMD myApplication</span>
</code></pre></div><p>Depending on the base image, you might also need to explicitly create a group
for the user. In the busybox example above, a &lsquo;myNewUser&rsquo; group is implicitly
created by the adduser command.</p>
<p><strong>You must specify the user by its numeric UID</strong>, as above, not by its
username.  If you use the username (<code>USER myNewUser</code>) then the pod security
policy will not be able to tell that that is a non-root user, and your
container will not be scheduled.</p>
<p>A more complete example can be found here :
<a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app/blob/master/rails-app/Dockerfile">Dockerfile</a></p>
<h5 id="adapting-the-nginx-image">Adapting the NGINX image</h5><p>Since NGINX binds itself to a privileged port by default, it will not be able
to run as-is with the <em>restricted</em> policy.</p>
<p>The cloud-platform team will update this page with relevant documentation
regarding nginx, as soon as it is ready.  In the meantime, feel free to reach
out to the cloud-platform team for help : <a href="/getting-help.html">Getting Help</a></p>

<h3 id="deploying-an-application-to-the-cloud-platform-with-helm">Deploying an application to the Cloud Platform with Helm</h3><h4 id="deploying-an-application-to-the-cloud-platform-with-helm-introduction">Introduction</h4><p>This document will act as a guide to your first application deployment into the Cloud Platform. If you have any issues completing the objective or have any suggestions please feel free to drop use a line in the <code>#ask-cloud-platform</code> slack channel.</p>
<h5 id="deploying-an-application-to-the-cloud-platform-with-helm-introduction-objective">Objective</h5><p>By the end of this guide you&rsquo;ll have deployed a reference <a href="https://github.com/ministryofjustice/cloud-platform-reference-app">Django application</a> to a cluster using the Kubernetes package manager <a href="https://helm.sh/">Helm</a>.</p>
<p><em>Disclaimer: You&rsquo;ll see fairly quickly that the application is not fit for production. A perfect example of this is the <a href="https://github.com/ministryofjustice/cloud-platform-reference-app/blob/master/helm_deploy/django-app/templates/secret.yaml">plaintext secrets file</a>. For the reference application we&rsquo;ve left this file in plaintext but it <strong>must</strong> be encrypted when writing your own manifests for production/non-production work in the MoJ.</em></p>
<h5 id="requirements">Requirements</h5><p>It is assumed you have the following:</p>

<ul>
<li>You have a basic understanding of what <a href="https://kubernetes.io/">Kubernetes</a> is.</li>
<li>You have <a href="/tasks.html#creating-a-cloud-platform-environment">created an environment for your application</a></li>
<li>You have installed <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">Kubectl</a> on your local machine.</li>
<li>You have <a href="/tasks.html#authentication">Authenticated</a> to the cloud-platform-live-1 cluster.</li>
</ul>
<h4 id="deploy-the-app">Deploy the app</h4><p>The reference application we&rsquo;re going to use is a very simple Django application with an on-cluster Postgresql database.</p>

<blockquote>
<p>Note: Even though we are going to install a database within the Kubernetes cluster, it is recommended to use a database as a service offering such as <a href="https://aws.amazon.com/rds/">AWS RDS</a> if running in production.</p>
</blockquote>
<p>The Helm deployment manifests have been pre-written for this exercise. But if you wish to know more about these files and what they do have a quick browse of the <a href="https://github.com/ministryofjustice/cloud-platform-reference-app/tree/master/helm_deploy/django-app/README.md">README</a>.</p>
<h5 id="deploy-the-app-set-up">Set up</h5><p>First we need to clone our reference application and change directory:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ git clone https://github.com/ministryofjustice/cloud-platform-reference-app.git
$ cd cloud-platform-reference-app
</code></pre></div><p>You now have a functioning git repository that contains a simple Django application. Have a browse around and get familiar with the directory structure.</p>
<h5 id="browse-the-cluster">Browse the cluster</h5><p>Let&rsquo;s make use of the command line tool <code>kubectl</code> to browse around the cluster to see what it looks like before we deploy our application:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ kubectl get pods --namespace &lt;env-name&gt;
</code></pre></div><p><em>The <code>&lt;env-name&gt;</code> here is the environment you created, listed in the requirements section at the beginning of the document.</em></p>
<p>If you receive the below error message then you&rsquo;ve either not typed in your namespace correctly or you don&rsquo;t have permission to perform a <code>get pods</code> command. Either way, you&rsquo;ll need to go back and review the <a href="/tasks.html#creating-a-cloud-platform-environment">Creating an Environment</a> document previously mentioned.</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ Error from server (Forbidden): pods is forbidden: User "test-user" cannot list pods in the namespace "demo"
</code></pre></div><h5 id="using-helm">Using Helm</h5><p>Helm allows you to manage application deployment to Kubernetes using Charts. You can read about of some of the many features of <a href="https://docs.helm.sh/developing_charts/">Helm Charts</a>. We&rsquo;ve chosen to use Helm as the default way to deploy applications to the Cloud Platform as it provides useful tooling as an interface to the YAML files that Kubernetes uses to run.</p>
<h6 id="tiller-rbac-configuration">Tiller RBAC Configuration</h6><p>There are two parts to Helm: The client and the Helm server (Tiller).</p>
<p>We will create a <code>Service Account</code> resource by adding to your <code>01-rbac.yaml</code> file. This gives Helm the permissions it needs to deploy within your namespace.</p>
<p>Add the following to the bottom of the <code>01-rbac.yaml</code> file you defined when you <a href="/tasks.html#creating-a-cloud-platform-environment">created your environment</a>:</p>
<div class="highlight"><pre class="highlight plaintext"><code>---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: myapp-dev ### Your namespace `&lt;servicename-env&gt;`
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: tiller
  namespace: myapp-dev ### Your namespace `&lt;servicename-env&gt;`
subjects:
- kind: ServiceAccount
  name: tiller
  namespace: myapp-dev ### Your namespace `&lt;servicename-env&gt;`
roleRef:
  kind: ClusterRole
  name: admin
  apiGroup: rbac.authorization.k8s.io
</code></pre></div><p>After you have added this to the file, commit it and create a pull request against the <a href="https://github.com/ministryofjustice/cloud-platform-environments">cloud-platform-environments</a> master repo.</p>
<p>Once it is merged and applied, you will have a service account for Tiller that allows it act on your namespace. Now you have to install Helm and Tiller into your namespace.</p>
<h6 id="installing-and-configuring-helm-and-tiller">Installing and configuring Helm and Tiller</h6><p>Install the client via Homebrew or by other <a href="https://docs.helm.sh/using_helm/#installing-helm">means</a>:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ brew install kubernetes-helm
</code></pre></div><p>Note: The Helm version <strong>must</strong> be at least <strong>2.14</strong>. Earlier versions try to run <code>tiller</code> as root, which is incompatible with our cluster security policies.</p>
<p>Now configure the installation with Tiller:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ helm init --tiller-namespace &lt;env-name&gt; --service-account tiller
</code></pre></div><p>When succesful, you&rsquo;ll be greeted with the message:</p>
<div class="highlight"><pre class="highlight plaintext"><code>Happy Helming
</code></pre></div><p>This is an indication we&rsquo;re ready to deploy our applicaton.</p>
<h6 id="application-install">Application install</h6><p>To deploy the application with Helm first change directory so we can focus on the app we need:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ cd helm_deploy/django-app/
</code></pre></div><p>Values for our application are stored in the <code>values.yaml</code> at the root of our directory. Configurations such as &lsquo;number of pods running&rsquo; and which image repository to use is stored here in this file. Open this file and get familiar with our application layout.</p>
<p>We need to set a <code>host</code> value, to set the URL for your application. We have to provide this value as an argument to our installation command.</p>
<p>Run the following (replacing the <code>YourName</code> with your own name and <code>env-name</code> with your environment name:</p>
<div class="highlight"><pre class="highlight plaintext"><code>    $ helm install . \
      --name django-app-&lt;YourName&gt; \
      --namespace &lt;env-name&gt; \
      --set deploy.host=django-&lt;YourName&gt;.apps.live-1.cloud-platform.service.justice.gov.uk \
      --tiller-namespace &lt;env-name&gt;
</code></pre></div>

<blockquote>
<p>Note: We&rsquo;re naming it like this as app names and host names have to be unique on the cluster.</p>
</blockquote>

<p>You&rsquo;ll see quite a lot of output as the various components are created.</p>

<h5 id="viewing-your-application">Viewing your application</h5><p>Congratulations on getting this far. If all went well your pods are now deployed and is now being served on your specified URL.</p>
<p>Let&rsquo;s check:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ kubectl get pods --namespace &lt;env-name&gt;
</code></pre></div><p>If the deploy was successful you should be greeted with something similar to the below:</p>
<div class="highlight"><pre class="highlight plaintext"><code>NAME                                           READY     STATUS    RESTARTS   AGE
django-app-&lt;Name&gt;-fcc657679-w69cr               1/1       Running   1          39m
django-app-&lt;Name&gt;-fcc657679-c5wdm               1/1       Running   1          39m
django-app-&lt;Name&gt;-db-migration-dgnse-qgs7r      0/1       Completed 0          39m
django-app-&lt;Name&gt;-postgresql-7b4bdff4b8-xdlw2   1/1       Running   0          39m
</code></pre></div><p>You should have a postgres pod and 2 app pods <strong>ready</strong> with the status <strong>running</strong>.</p>
<p>(There&rsquo;s also a line for a pod which ran the initial database migrations, but that&rsquo;s completed and no longer running so we&rsquo;ll ignore it.)</p>
<p>Let&rsquo;s check your host has a URL by running:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ kubectl get ingress --namespace &lt;env-name&gt;
</code></pre></div><p>This will return the URL of your given app. Open it using your favourite browser.</p>
<p>The application is secured with http basic authentication. The default credentials are user: <code>myuser</code>, password: <code>password123</code>. For more information, see <a href="/tasks.html#add-http-basic-authentication">this topic</a>.</p>
<p>You should be met with an MoJ reference app with the title, <em>&#39;Cloud Platforms Deployment&rsquo;</em>. As we mentioned before, there is nothing complicated about this application. You can enter your name and job role, calling the on-cluster postgresql database.</p>
<h5 id="view-the-logs">View the logs</h5><p>Each pod will generate logs that can be viewed via the API. Let&rsquo;s have a browse of our application logs.</p>
<p>First grab the pod name:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ kubectl get pods --namespace &lt;env-name&gt;
</code></pre></div><p>Then copy the name of a pod that isn&rsquo;t postgresql.</p>
<div class="highlight"><pre class="highlight plaintext"><code>   NAME                                           READY     STATUS    RESTARTS   AGE
 * django-app-&lt;name&gt;-fcc657679-w69cr               1/1       Running   1          54m
   django-app-&lt;name&gt;-fcc657679-c5wdm               1/1       Running   1          39m
   django-app-&lt;name&gt;-postgresql-7b4bdff4b8-xdlw2   1/1       Running   0          54m
</code></pre></div><p>We&rsquo;re going to follow the log, so we&rsquo;ll run:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ kubectl logs django-app-&lt;name&gt;-fcc657679-w69cr --namespace &lt;env-name&gt; -f
</code></pre></div><p>As you can see, this tails the log and you should see our health checks giving a HTTP 200.</p>
<p>Read more about Kubernetes logging <a href="https://kubernetes.io/docs/concepts/cluster-administration/logging/">here</a>.</p>
<h5 id="scale-the-application">Scale the application</h5><p>You now have our application up and running but you decide two pods aren&rsquo;t enough. Say you want to run three. This is simply a case of changing the replicaCount value in the values.yaml and then running the <code>helm upgrade</code> command.</p>
<p>Edit <code>values.yaml</code> and change <code>replicaCount</code> from 1 to 3. Save the file, then run:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ helm upgrade django-app-&lt;YourName&gt; . --tiller-namespace &lt;env-name&gt; --set deploy.host=&lt;DeploymentURL&gt;
</code></pre></div><p>This command spins up another pod to bring the total number to 3.</p>
<p>If we run the familiar command we&rsquo;ve been using:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ kubectl get pods --namespace &lt;env-var&gt;
</code></pre></div><p>You&rsquo;ll see the pod replication in progress.</p>
<h5 id="tear-it-all-down">Tear it all down</h5><p>Finally, we have built are app and deployed to the cluster. There is only one thing left to do. Destroy it.</p>
<p>To delete the deployment you simply run:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ helm del --purge django-app-&lt;YourName&gt; --tiller-namespace &lt;env-name&gt;
</code></pre></div><p>And then confirm the pods are terminating as expected:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ kubectl get pods --namespace &lt;env-var&gt;
</code></pre></div><h4 id="deploying-an-application-to-the-cloud-platform-with-helm-next-steps">Next steps</h4><p>The next step will be to create your own Helm Chart. You can try this with an application of your own or run through <a href="https://docs.bitnami.com/kubernetes/how-to/create-your-first-helm-chart/">Bitnami&rsquo;s excellent guide</a> on how to build using a simple quickstart.</p>

<h3 id="continuous-deployment-of-an-application-using-circleci-and-helm">Continuous Deployment of an application using CircleCI and Helm</h3><h4 id="continuous-deployment-of-an-application-using-circleci-and-helm-introduction">Introduction</h4><p>This document covers how to continuously deploy your application in the Cloud Platform. It is essentially a continuation of <a href="/tasks.html#deploying-an-application-to-the-cloud-platform-with-helm">‘Deploying an application to the Cloud Platform with Helm’</a>.</p>
<p><em>Note: This document is specific to using <a href="https://circleci.com/">CircleCI</a> as the Continuous Integration method.</em></p>
<h5 id="continuous-deployment-of-an-application-using-circleci-and-helm-introduction-objective">Objective</h5><p>By the end of the tutorial, you will have done the following:</p>

<ul>
<li>Created a Service Account for CircleCI in your namespace</li>
<li>Generated a CircleCI configuration file in your application repository. The configuration file will authenticate with your chosen cluster, build a docker image and push it to ECR and upgrade your helm deployment with the new docker image.</li>
<li>Have an automated CircleCI pipeline that upgrades your helm deployment when a new change is pushed to your master branch</li>
</ul>
<h5 id="continuous-deployment-of-an-application-using-circleci-and-helm-introduction-requirements">Requirements</h5><p>It is assumed you have the following:</p>

<ul>
<li>You have <a href="/tasks.html#creating-a-cloud-platform-environment">created an environment for your application</a></li>
<li>You have <a href="/tasks.html#deploying-an-application-to-the-cloud-platform-with-helm">deployed an application</a> to the &lsquo;cloud-platform-live-1&rsquo; cluster using Helm.</li>
<li>You have created an <a href="/tasks.html#creating-an-ecr-repository">ECR repository</a></li>
</ul>
<h5 id="creating-a-service-account-for-circleci">Creating a Service Account for CircleCI</h5><p>As part of the CircleCI deployment pipeline, CircleCI will need to authenticate with the Kubernetes cluster. In order to do so, Kubernetes uses <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/">Service Accounts</a>. Service Accounts provide an identity for processes that run in a cluster allowing the process to access the API server.</p>
<p>A Service Account is created in the <a href="https://github.com/ministryofjustice/cloud-platform-environments/tree/master/namespaces">namespace creation github repository</a>.</p>
<div class="highlight"><pre class="highlight shell"><code>  <span class="nv">$ </span>kubectl get serviceaccounts <span class="nt">--namespace</span> <span class="nv">$ns</span>
  NAME       SECRETS   AGE
  circleci   1         3h

  <span class="nv">$ </span>kubectl get serviceaccounts/circleci <span class="nt">--namespace</span> reference-app <span class="nt">-o</span> yaml
  apiVersion: v1
  kind: ServiceAccount
  ...
  secrets:
  - name: circleci-token-prlkp

  <span class="nv">$ </span>kubectl get secrets <span class="nt">--namespace</span> <span class="nv">$ns</span>
  NAME                   TYPE                                  DATA      AGE
  circleci-token-prlkp   kubernetes.io/service-account-token   3         3h

  <span class="nv">$ </span>kubectl get secrets/circleci-token-prlkp <span class="nt">--namespace</span> <span class="nv">$ns</span> <span class="nt">-o</span> yaml
  ...
  namespace: cm..cA<span class="o">==</span>
  token: ZX...EE<span class="o">=</span>
</code></pre></div><h5 id="linking-repository-to-circleci">Linking Repository to CircleCI</h5><p>MoJ has as an account with CircleCI, please login to <a href="https://circleci.com/dashboard">CircleCI</a> using GitHub credentials. Select project, and if config.yml is in the repo CircleCI will build and run tests.</p>
<h5 id="add-variables-to-circleci">Add variables to CircleCI</h5><p>There is a number of environment variables that you will need to set on your CircleCI project in order to build a docker image, push it to the ECR and trigger a deployment in your environment. On the project page, click the cog icon in the top right corner and select <code>Enviroment Variables</code> under <code>Build Settings</code>. The variables you will need to set are listed below.</p>
<h6 id="aws-credentials">AWS credentials</h6><p>To authenticate with ECR, you will need to set:</p>

<ul>
<li><code>AWS_DEFAULT_REGION</code> - would be <code>eu-west-2</code> for Cloud Platform clusters unless specified otherwise</li>
<li><code>AWS_ACCESS_KEY_ID</code></li>
<li><code>AWS_SECRET_ACCESS_KEY</code></li>
<li><code>ECR_ENDPOINT</code> is optional but useful if you want to avoid having to hardcode the full hostname of the registry</li>
</ul>
<h6 id="kubernetes-serviceaccount-credentials">Kubernetes <code>ServiceAccount</code> credentials</h6><p>Since a single CircleCI project will need to access multiple namespaces in kubernetes (the environments), it will also need to handle multiple credentials. To simplify authentication, we provide a helper script in our supported <a href="https://github.com/ministryofjustice/cloud-platform-tools-image">build image</a>. For a usage example, see <a href="###upload-to-ecr">Deploy To Kubernetes</a> below.</p>
<p>There are four different variables that CircleCI will need to access <em>per environment</em>. Our helper script expects environment variables to be named according to the list below where <code>&lt;ENVIRONMENT&gt;</code> should be replaced by some identifier of your choosing (eg.: <code>STAGING</code>, <code>PRODUCTION</code>).</p>

<ul>
<li><code>KUBE_ENV_&lt;ENVIRONMENT&gt;_NAME</code> - the full name of the cluster (eg.: <code>live-1.cloud-platform.service.justice.gov.uk</code>)</li>
<li><code>KUBE_ENV_&lt;ENVIRONMENT&gt;_NAMESPACE</code> - the name of the <code>Namespace</code> (see <a href="/tasks.html#creating-a-cloud-platform-environment">Create a namespace</a>)</li>
<li><code>KUBE_ENV_&lt;ENVIRONMENT&gt;_CACERT</code> - the CA Certificate for the cluster, can be acquired from the <code>Secret</code> that is generated for the <code>ServiceAccount</code></li>
<li><code>KUBE_ENV_&lt;ENVIRONMENT&gt;_TOKEN</code> - the access token generated for the <code>ServiceAccount</code>. Please note, you should first base64 decode the token value you retrieve from the secret <a href="###creating-a-service-account-for-circleci">in the previous section</a>, e.g. <code>echo &lt;thereallylongstringthatyougetback&gt; | base64 --decode</code>.</li>
</ul>
<h5 id="creating-the-config-yml">Creating the config.yml</h5><p>CircleCI uses a YAML file to identify how you want your testing environment set up and what tests you want to run. On CircleCI 2.0, this file must be called <code>config.yml</code> and must be in a hidden folder called <code>.circleci</code> .</p>
<p><a href="https://circleci.com/docs/2.0/tutorials/">Tutorial</a> on creating a config.yml file. As long as you are building a Docker image you can configure Circle however you wish. The only additional configuration you will need to add is to upload an image to ECR and deploy to Kubernetes:</p>
<h6 id="upload-to-ecr">Upload to ECR</h6><p>Example of how you can push a built docker image to an ECR repository:</p>
<div class="highlight"><pre class="highlight yaml"><code><span class="pi">-</span> <span class="na">deploy</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">Push application Docker image</span>
    <span class="na">command</span><span class="pi">:</span> <span class="pi">|</span>
      <span class="no">$(aws ecr get-login --no-include-email)</span>
      <span class="no">docker tag app "${ECR_ENDPOINT}/${GITHUB_TEAM_NAME_SLUG}/${CIRCLE_PROJECT_REPONAME}:${CIRCLE_SHA1}"</span>
      <span class="no">docker push "${ECR_ENDPOINT}/${GITHUB_TEAM_NAME_SLUG}/${CIRCLE_PROJECT_REPONAME}:${CIRCLE_SHA1}"</span>
      <span class="no">if [ "${CIRCLE_BRANCH}" == "master" ]; then</span>
        <span class="no">docker tag app "${ECR_ENDPOINT}/${GITHUB_TEAM_NAME_SLUG}/${CIRCLE_PROJECT_REPONAME}:latest"</span>
        <span class="no">docker push "${ECR_ENDPOINT}/${GITHUB_TEAM_NAME_SLUG}/${CIRCLE_PROJECT_REPONAME}:latest"</span>
      <span class="no">fi</span>
</code></pre></div><h6 id="deploy-to-kubernetes">Deploy to Kubernetes</h6><p>We provide a docker image that simplifies the CircleCI configuration by encapsulating the authentication process in a script. For example, given a configured <code>DEVELOPMENT</code> environment (see the section on environment variables above):</p>
<div class="highlight"><pre class="highlight yaml"><code><span class="na">deploy_development</span><span class="pi">:</span>
  <span class="na">docker</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">image</span><span class="pi">:</span> <span class="s">${ECR_ENDPOINT}/cloud-platform/tools:circleci</span>
  <span class="na">steps</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">checkout</span>
    <span class="pi">-</span> <span class="na">deploy</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">Helm deployment</span>
        <span class="na">command</span><span class="pi">:</span> <span class="pi">|</span>
          <span class="no">setup-kube-auth</span>
          <span class="no">kubectl config use-context development</span>
          <span class="no">if [ "${CIRCLE_BRANCH}" == "master" ]; then</span>
            <span class="no">helm upgrade ${APPLICATON_DEPLOY_NAME} ./helm_deploy/django-app/. \</span>
                          <span class="no">--install \</span>
                          <span class="no">--tiller-namespace=${NON_PROD_NS} \</span>
                          <span class="no">--namespace=${NON_PROD_NS} \</span>
                          <span class="no">--set image.repository="${ECR_ENDPOINT}/${GITHUB_TEAM_NAME_SLUG}/${CIRCLE_PROJECT_REPONAME}" \</span>
                          <span class="no">--set image.tag="${CIRCLE_SHA1}" \</span>
                          <span class="no">--set deploy.host="${APPLICATION_HOST_URL}" \</span>
                          <span class="no">--set replicaCount="4" \</span>
                          <span class="no">--debug</span>
          <span class="no">fi</span>
</code></pre></div>

<h2 id="how-do-i-run-rails-database-migrations">How do I run Rails database migrations?</h2><p>The short and naive answer is that you create a Kubernetes &lsquo;Job&rsquo; yaml file, using your rails application Docker image, and run <code>rails db:migrate</code></p>
<p>Here is an <a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app/blob/master/kubernetes_deploy/rails-migrations-job.yaml">example</a></p>
<p>This approach may work for your service, but there are potential issues that
you should be aware of. In particular, kubernetes makes no guarantees about
when your job will run, in relation to when the rest of your service starts.</p>
<p>So, it is possible for your application containers to be replaced with the
latest version <strong>before</strong> your migration job has run. In this case, there will
be a period of time when your updated application code may try to interact with
the database while it is still in the &#39;old&rsquo; state (e.g. before your migration
added a new column). This might cause users to see errors.</p>
<p>Alternatively, it is possible that your migration job will complete before all
of your application pods have been replaced with pods running the latest code.
So, you might have application code which expects to see the old database
structure, but which are running against an updated database. If your migration
removed a column, or otherwise updated the database in ways which are
incompatible with the old version of the application code, then this could
cause users to see errors.</p>
<p>These problems are not unique to Kubernetes. They occur in any scenario where a
mismatch between the state of your database and the state of your application
code can cause errors.</p>
<p>In the majority of cases, the window during which errors might occur is likely
to be so brief that no users will be affected, but it is worth being aware of,
even if you decide to just accept that it <strong>might</strong> happen.</p>
<p>Some strategies to protect against this include:</p>

<ul>
<li>If planned downtime is a possibility, put the service into maintenance mode
before the migration, and bring it back into service when you are confident
that the application code and the database are both in the appropriate state.</li>
<li>Create a healthcheck endpoint in your application code which tests the
state of the database and fails if the database structure is not as
expected, so that kubernetes does not start any new application pods until
after the migration has completed (although this will not prevent problems if
the new database structure is incompatible with the old application code).</li>
<li>Break your migration into several stages such that, at every stage, your
application code works with both the current and next/previous state of the
database.</li>
</ul>
<h3 id="do-not-run-migrations-on-container-startup">Do not run migrations on container startup</h3><p>A pattern to avoid is having your application container start up using a
command like this:</p>
<div class="highlight"><pre class="highlight plaintext"><code>bundle exec rails db:migrate &amp;&amp; bundle exec rails server
</code></pre></div><p>In general, you should avoid overloading container startup in this way. If your
container takes a long time to start up (e.g. if the migration task takes a
long time to complete, in this example) then the cluster might assume your pod
has failed, and it will kill it and start a new one. In the worst cases, this
can cause your application to go into a crash loop such that it never starts at
all.</p>
<p>Keep your containers dedicated to a single purpose and, if you need to run
one-off jobs, use a dedicated job or other kubernetes object to do so.</p>
<h3 id="further-reading">Further reading</h3><p>The following StackOverflow threads discuss these issues:</p>

<ul>
<li><a href="https://stackoverflow.com/questions/50218376/managing-db-migrations-on-kubernetes-cluster">Managing DB migrations on Kubernetes cluster</a></li>
<li><a href="https://stackoverflow.com/questions/48877182/kubernetes-rolling-deployments-and-database-migrations">Kubernetes rolling deployments and database migrations</a></li>
<li><a href="https://stackoverflow.com/questions/37058812/how-best-to-run-one-off-migration-tasks-in-a-kubernetes-cluster">How best to run one-off migration tasks in a kubernetes cluster</a></li>
</ul>


<h2 id="adding-a-secret-to-an-application">Adding a secret to an application</h2><h3 id="adding-a-secret-to-an-application-overview">Overview</h3><p>The aim of this guide is to walkthrough the process of adding a secret (in this example for aws access-key credentials) to a previously deployed application in the Cloud Platform.</p>
<h3 id="adding-a-secret-to-an-application-prerequisites">Prerequisites</h3><p>This guide assumes the following:</p>

<ul>
<li>You have previously set up an env. See <a href="/tasks.html#creating-a-cloud-platform-environment">Creating a Cloud Platform Environment</a></li>
<li>You have previously deployed your application. See <a href="/tasks.html#deploying-a-39-hello-world-39-application-to-the-cloud-platform">Deploying an application to the Cloud-Platform</a></li>
<li>Check your deployment is running. See <a href="/tasks.html#interacting-with-the-application">Interacting with the application</a></li>
</ul>
<h3 id="configuring-secrets">Configuring secrets</h3><p>The following is an example of encoding (configuring) aws access-key credentials in your deployment.
See <a href="https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables">kuberenetes using secrets as environment variables</a></p>
<p>for detailed information regarding providing base64 values in secret objects to Kuberenetes pods)</p>
<p>Create your AWS Credentials access key (making a note of the aws_access_key_id and aws_secret_access_key)</p>
<h4 id="base64-encode-your-secret-as-follows">base64-encode your secret as follows:</h4><p>In this example  aws_access_key_id is &lsquo;AKIAFTKSAW15HJLOGD&rsquo;. Issue the following command to base64-encode:</p>
<div class="highlight"><pre class="highlight plaintext"><code>echo -n 'AKIAFTKSAW15HJLOGD' | base64 -b0
</code></pre></div><p>This will return the encoded id &#39;QUtJQUZUS1NBVzE1SEpMT0dE&rsquo;</p>
<p>In this example the is aws_secret_access_key &#39;g8hjpmhvgfhk4547gfdshhjj&rsquo;. Issue the following command to base64-encode:</p>
<div class="highlight"><pre class="highlight plaintext"><code>echo -n 'QUtJQUZUS1NBVzE1SEpMT0dE' | base64 -b0
</code></pre></div><p>This will return the encoded secret &#39;UVV0SlFVWlVTMU5CVnpFMVNFcE1UMGRF&rsquo;</p>
<h3 id="creating-the-secret">Creating the secret</h3><p>Create a secrets.yaml file similar to:</p>
<div class="highlight"><pre class="highlight plaintext"><code>apiVersion: v1
kind: Secret
metadata:
  name: demosecret
type: Opaque
data:
  aws_access_key_id: QUtJQUZUS1NBVzE1SEpMT0dE
  aws_secret_access_key: UVV0SlFVWlVTMU5CVnpFMVNFcE1UMGRF
</code></pre></div><p>issue the following command:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ kubectl apply -f secrets.yaml
secret "demosecret" created
</code></pre></div><p>To see the secrets:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ kubectl get secrets
NAME                                          TYPE                                  DATA      AGE
calico-zebu-external-dns-token-pldjb          kubernetes.io/service-account-token   3         16d
dandy-bumblebee-nginx-ingress-token-bspl6     kubernetes.io/service-account-token   3         14d
default-token-hz7z7                           kubernetes.io/service-account-token   3         26d
demosecret                                    Opaque                                2         5d
</code></pre></div><p>Decoding the Secret
Secrets can be retrieved via the kubectl get secret command. For example, to retrieve the secret you created:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ kubectl get secret demosecret  -o yaml
apiVersion: v1
data:
  aws_access_key_id: dGVzdCBrZXk=
  aws_secret_access_key: dGVzdCBrZXk=
kind: Secret
metadata:
  creationTimestamp: 2018-05-15T12:24:33Z
  name: demosecret
</code></pre></div><p>Add the AWS_ACCESS_KEY_ID referencing &#39;aws_access_key_id&rsquo; and AWS_SECRET_ACCESS_KEY referencing &#39;aws_secret_access_key&rsquo; (as previously set) to the containers env in <code>deployment-files/deployment.yaml</code></p>
<div class="highlight"><pre class="highlight plaintext"><code>    spec:
      containers:
        - name: django-demo-container
          image: 754256621582.dkr.ecr.eu-west-2.amazonaws.com/cloud-platform-reference-app:django
          ports:
            - containerPort: 8000
          env:
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: demosecret
                  key: aws_access_key_id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: demosecret
                  key: aws_secret_access_key
</code></pre></div>
<h2 id="creating-pingdom-checks">Creating Pingdom checks</h2><h3 id="creating-pingdom-checks-overview">Overview</h3><p><a href="https://my.pingdom.com">Pingdom</a> is a global performance and availability monitor for your web application. The aim of this document is to provide you with the necessary information to create Pingom checks via the <a href="https://github.com/ministryofjustice/cloud-platform-environments">cloud-platform-environments</a> pipeline, and then send failing checks to a Slack channel of your choosing.</p>
<h3 id="creating-pingdom-checks-prerequisites">Prerequisites</h3><p>This guide assumes the following:</p>

<ul>
<li>You have <a href="/tasks.html#creating-a-cloud-platform-environment">created a namespace for your application</a></li>
<li>You have a slack channel to send alerts to</li>
</ul>
<h3 id="create-a-pingdom-check">Create a Pingdom check</h3><p>To create a Pingdom check simply add a <code>pingdom.tf</code> file in the resources directory of your namespace in your <a href="https://github.com/ministryofjustice/cloud-platform-environments/tree/master/namespaces/live-1.cloud-platform.service.justice.gov.uk">cloud-platform-environments</a> repository. You can define the conditions of your check using the resources outlined in the <a href="https://github.com/russellcardullo/terraform-provider-pingdom">Terraform community provider</a>. Here&rsquo;s a working example of a <a href="https://github.com/ministryofjustice/cloud-platform-environments/tree/master/namespaces/cloud-platform-live-0.k8s.integration.dsd.io/monitoring/resources">basic check</a>.</p>
<div class="highlight"><pre class="highlight yaml"><code><span class="s">terraform {</span>
  <span class="s">backend "s3" {}</span>
<span class="err">}</span>

<span class="s">provider "pingdom" {}</span>

<span class="s">resource "pingdom_check" "cloud-platform-prometheus-live-0-healthcheck" {</span>
   <span class="s">type                     = "http"</span>
   <span class="s">name                     = "Prometheus - live-0 - cloud-platform - Healthcheck"</span>
   <span class="s">host                     = "prometheus.apps.cloud-platform-live-0.k8s.integration.dsd.io"</span>
   <span class="s">resolution               = 1</span>
   <span class="s">notifywhenbackup         = </span><span class="no">true</span>
   <span class="s">sendnotificationwhendown = 6</span>
   <span class="s">notifyagainevery         = 0</span>
   <span class="s">url                      = "/-/healthy"</span>
   <span class="s">encryption               = </span><span class="no">true</span>
   <span class="s">port                     = 443</span>
   <span class="s">tags                     = "businessunit_platforms,application_prometheus,component_healthcheck,isproduction_true,environment_prod,infrastructuresupport_platforms"</span>
   <span class="s">probefilters             = "region:EU"</span>
   <span class="s">publicreport             = "true"</span>
 <span class="s">}</span>
</code></pre></div><p><strong>Note</strong>: You&rsquo;ll need to include the <code>provider &quot;pingdom&quot;</code> and <code>terraform</code> blocks either in this file or in a <code>main.tf</code> file.</p>
<p>This basic check simply checks that the host/url (in this case; https://prometheus.apps.cloud-platform-live-0.k8s.integration.dsd.io/-/healthy) returns a 200 every minute (resolution = 1 minute). When six (sendnotificationwhendown = 6) consecutive checks fail it triggers an alarm. As publicreport is set to true, you can view the status of your check by visiting the <a href="http://pingdom.service.dsd.io">public status page</a>, where this check would appear with the name &ldquo;Prometheus - live-0 - cloud-platform - Healthcheck&rdquo;.</p>
<p><a href="https://github.com/russellcardullo/terraform-provider-pingdom#pingdom-check">This</a> page explains all the attributes used in the check.</p>
<p>All resources, including Pingdom checks <strong>must</strong> be tagged and adhere to the technical guidance outlined <a href="https://github.com/ministryofjustice/technical-guidance/blob/master/standards/documenting-infrastructure-owners.md">here</a>. Ensure your check has appropriate tags before submitting a pull request.</p>
<p>Once reviewed and merged to master, the pipeline will create your check in the MoJ Pingdom account.</p>
<h4 id="adding-slack-notification">Adding Slack notification</h4><p>You can enable the option to send a failing alert to Slack via a webhook by simply adding Pingdom integration id. You need administrator permissions to manage the mojdt <a href="https://mojdt.slack.com/apps/A0F814AV7-pingdom?next_id=0">Pingdom Slack</a> webhook and then <a href="https://my.pingdom.com">Pingdom</a> to create the integration id.</p>
<p>The Cloud Platform team can do this on your behalf. Create a ticket requesting a Pingdom integration id with the following information:</p>

<ul>
<li>team name</li>
<li>application name</li>
<li>slack channel</li>
</ul>
<p>The team will provide you with an integration id, following the steps outlined <a href="https://github.com/ministryofjustice/cloud-platform-environments/blob/master/docs/creating-pingdom-webhook.md">here</a>.</p>
<p>You can now add <code>integrationids</code> to your <code>pingdom.tf</code>. Appending the example above, your check will now appear as follows (assuming you were given 1000 as the integration id):</p>
<div class="highlight"><pre class="highlight yaml"><code><span class="s">terraform {</span>
   <span class="s">backend "s3" {}</span>
 <span class="s">}</span>

 <span class="s">provider "pingdom" {}</span>

 <span class="s">resource "pingdom_check" "cloud-platform-prometheus-live-0-healthcheck" {</span>
    <span class="s">type                     = "http"</span>
    <span class="s">name                     = "Prometheus - live-0 - cloud-platform - Healthcheck"</span>
    <span class="s">host                     = "prometheus.apps.cloud-platform-live-0.k8s.integration.dsd.io"</span>
    <span class="s">resolution               = 1</span>
    <span class="s">notifywhenbackup         = </span><span class="no">true</span>
    <span class="s">sendnotificationwhendown = 6</span>
    <span class="s">notifyagainevery         = 0</span>
    <span class="s">url                      = "/-/healthy"</span>
    <span class="s">encryption               = </span><span class="no">true</span>
    <span class="s">port                     = 443</span>
    <span class="s">tags                     = "businessunit_platforms,application_prometheus,component_healthcheck,isproduction_true,environment_prod,infrastructuresupport_platforms"</span>
    <span class="s">probefilters             = "region:EU"</span>
    <span class="s">publicreport             = "true"</span>
    <span class="s">integrationids           = [1000]</span>
  <span class="s">}</span>

</code></pre></div>
<h2 id="cleaning-up">Cleaning up</h2><p>When you have finished with a namespace, please clean it up, along with any
additional AWS resources you created. This helps to keep the cloud platform
repositories well-organised, and speeds up deployments and changes to the
cluster (because the build process doesn&rsquo;t have to spend time managing
unnecessary resources). It also helps to keep our hosting costs down.</p>
<p>The resources to be removed are:</p>

<ul>
<li>The AWS ECR which stores your docker images</li>
<li>Your namespace in the cluster. This contains all of the pods, containers and
other cluster resources for your application.</li>
<li>Any other AWS resources (e.g. RDS/Elasticache instances, S3 buckets, etc.)</li>
</ul>
<p>Cleaning up a namespace is a multi-stage process, as follows:</p>

<ol>
<li>Delete any CI/CD pipeline you have created, which deploys into your
namespace. This should be done first, so that anything you delete is not
immediately recreated by your build pipeline.</li>
<li>Tell terraform to delete the AWS resources it created for you.</li>
<li>Remove your namespace code from the <a href="https://github.com/ministryofjustice/cloud-platform-environments">cloud-platform-environments</a> repository.</li>
<li>Delete all of the kubernetes resources inside your namespace.</li>
<li>Delete your namespace from the cluster.</li>
</ol>
<p>The first step depends on how you have set up your CI/CD pipeline, and is not
covered here.</p>
<h3 id="2-tell-terraform-to-delete-your-aws-resources">2. Tell terraform to delete your AWS resources</h3><p>AWS resources are created by adding terraform code to the <code>resources</code> directory
in your namespace&rsquo;s folder in the <a href="https://github.com/ministryofjustice/cloud-platform-environments">cloud-platform-environments</a> repository:</p>
<div class="highlight"><pre class="highlight plaintext"><code> cloud-platform-environments/namespaces/live-1.cloud-platform.service.justice.gov.uk/[your namespace]/resources/
</code></pre></div><p>To get terraform to delete the resources it created, you need to delete all the
<code>\*.tf</code> files in this directory <strong>except <code>main.tf</code></strong></p>
<p>If you delete <code>main.tf</code> at this point, terraform has no way of knowing it is
responsible for managing any resources in the namespace, so it will not delete
anything. By leaving <code>main.tf</code> but nothing else, you are telling terraform that
it should manage resources for this namespace, but that there should be no
resources, so terraform will delete any resources that do exist.</p>
<p>Once you have deleted the other <code>*.tf</code> files from your namespace&rsquo;s resources
folder, raise a <a href="https://help.github.com/en/articles/about-pull-requests">PR</a> to get your changes merged. As soon as this happens, the
cloud platform build pipeline will run, and your AWS resources will be deleted.</p>
<h3 id="3-remove-your-namespace-code-from-the-cloud-platform-environments-repository">3. Remove your namespace code from the cloud-platform-environments repository</h3><p>After your change to delete all the <code>\*.tf</code> files except <code>main.tf</code> has been
merged, please raise an additional <a href="https://help.github.com/en/articles/about-pull-requests">PR</a> removing the whole of your namespace code
from the <a href="https://github.com/ministryofjustice/cloud-platform-environments">cloud-platform-environments</a> repository.</p>
<p>i.e. deleting the whole of the directory:</p>
<div class="highlight"><pre class="highlight plaintext"><code> cloud-platform-environments/namespaces/live-1.cloud-platform.service.justice.gov.uk/[your namespace]
</code></pre></div><p>Merging this <a href="https://help.github.com/en/articles/about-pull-requests">PR</a> will prevent the cloud platform build pipeline from recreating
your namespace, after it is deleted.</p>
<h3 id="4-delete-all-of-the-kubernetes-resources-inside-your-namespace">4. Delete all of the kubernetes resources inside your namespace.</h3><p>In your working copy of your application code, you can use the kubernetes
deployment yaml files to delete your namespace and all the kubernetes resources
within it.</p>
<p>Assuming your current working directory is a working copy of your application,
and that your kubernetes deployment yaml files are in a directory called
<code>kubernetes_deploy</code>, immediately below your current working directory, you
would run the following command to delete everything within your namespace.</p>
<div class="highlight"><pre class="highlight plaintext"><code>kubectl delete --filename kubernetes_deploy --namespace [your namespace]
</code></pre></div><p>This is analogous to using <code>kubectl apply</code> to create the resources from your
YAML files, but it will delete all the named resources.</p>
<p>If you are using <a href="https://helm.sh">Helm</a>, the equivalent command is:</p>
<div class="highlight"><pre class="highlight plaintext"><code>helm delete --purge
</code></pre></div><h3 id="5-delete-your-namespace-from-the-cluster">5. Delete your namespace from the cluster.</h3><p>Deleting a namespace requires admin access to the cluster.</p>
<p>Please raise a <a href="https://help.github.com/en/articles/about-pull-requests">PR</a> against the <a href="https://github.com/ministryofjustice/cloud-platform-environments">cloud-platform-environments</a> repository,
specifying the namespace you would like the team to delete.</p>
<h3 id="summary">Summary</h3><p>Removing your namespace and associated resources is a multi-stage process:</p>

<ol>
<li>Stop any CI/CD process from recreating everything</li>
<li>Get terraform to delete AWS resources</li>
<li>Remove the code that defines your namespace</li>
<li>Remove everything inside the namespace</li>
<li>Tell the cloud platform team to delete the namespace</li>
</ol>


<h2 id="monitoring-applications">Monitoring Applications</h2>
<h3 id="using-the-cloud-platform-prometheus-alertmanager-and-grafana">Using the Cloud Platform Prometheus, AlertManager and Grafana</h3><h4 id="using-the-cloud-platform-prometheus-alertmanager-and-grafana-introduction">Introduction</h4><p>Prometheus, AlertManager and Grafana have been installed on Live-0 to ensure the reliability and availability of the Cloud Platform. This document will briefly outline how to access the monitoring tools and where to find further information.</p>
<h4 id="what-is-prometheus">What is Prometheus</h4><p>Prometheus is an open-source systems monitoring and alerting toolkit originally built at SoundCloud. The Cloud Platform uses <a href="https://github.com/coreos/prometheus-operator">Prometheus Operator from CoreOS</a> which allows a number of Prometheus instances to be installed on a cluster.</p>
<p>Prometheus scrapes metrics from instrumented jobs, either directly or via an intermediary push gateway for short-lived jobs. It stores all scraped samples locally and runs rules over this data to either aggregate and record new time series from existing data or generate alerts. Grafana or other API consumers can be used to visualize the collected data.</p>
<h4 id="what-is-alertmanager">What is AlertManager</h4><p>The Alertmanager handles alerts sent by client applications such as the Prometheus server. It takes care of deduplicating, grouping, and routing them to the correct receiver integration such PagerDuty. It also takes care of silencing and inhibition of alerts.</p>
<h4 id="what-is-grafana">What is Grafana</h4><p>Grafana allows you to query, visualize, alert on and understand your metrics no matter where they are stored. Create, explore, and share dashboards with your team and foster a data driven culture.</p>
<h5 id="creating-dashboards">Creating dashboards</h5><p>Grafana is set up as a stateless app, managed entirely through code. This also helps achieve better availability. However, it means that dashboards will not persist in its database. To create a dashboard:</p>

<ol>
<li><p>Login to Grafana (see the links below) with your GitHub account. All users are able to edit dashboards but cannot save the changes. Find the dashboard titled &lsquo;Blank Dashboard&rsquo; and modify it as you see fit.</p></li>
<li><p>Once happy with your dashboard, click the share icon on the top right corner, select the <code>Export</code> tab, check the <code>Export for sharing externally</code> box and click on <code>View JSON</code>. Copy the JSON string into a <code>ConfigMap</code> according to the example below.</p></li>
</ol>
<div class="highlight"><pre class="highlight plaintext"><code>---
apiVersion: v1
kind: ConfigMap
metadata:
  name: &lt;my-dashboard&gt;
  namespace: &lt;my-namespace&gt;
  labels:
    grafana_dashboard: ""
data:
  my-dashboard.json: |
    {
      [ ... ]
    }
</code></pre></div><p>Make sure you&rsquo;ve included the <code>label</code> and that the JSON string is properly indented. Also, name of the key in the <code>ConfigMap</code> must end in <code>-dashboard.json</code>. Please note that you can have multiple dashboards exported in a single <code>ConfigMap</code> as well.</p>

<ol>
<li>Use <code>kubectl</code> to apply the <code>ConfigMap</code> above, your dashboard should be visible in Grafana shortly.</li>
</ol>
<h4 id="how-to-access-monitoring-tools">How to access monitoring tools</h4><p>All links provided below require you to authenticate with your Github account.</p>
<p>Prometheus: <a href="https://prometheus.cloud-platform.service.justice.gov.uk">https://prometheus.cloud-platform.service.justice.gov.uk</a></p>
<p>AlertManager: <a href="https://alertmanager.cloud-platform.service.justice.gov.uk/">https://alertmanager.cloud-platform.service.justice.gov.uk</a></p>
<p>Grafana: <a href="https://grafana.cloud-platform.service.justice.gov.uk">https://grafana.cloud-platform.service.justice.gov.uk</a></p>
<h4 id="further-documentation">Further documentation</h4><p><a href="https://prometheus.io/docs/prometheus/latest/querying/basics">Prometheus querying</a></p>
<p><a href="https://prometheus.io/docs/alerting/alertmanager">AlertManager</a></p>
<p><a href="https://prometheus.io/docs/introduction/overview/###architecture">Architecture</a></p>

<h3 id="creating-your-own-custom-alerts">Creating your own custom alerts</h3><h4 id="creating-your-own-custom-alerts-overview">Overview</h4><p>Alertmanager allows you define your own alert conditions based on <a href="https://prometheus.io/docs/prometheus/latest/querying/basics">Prometheus expression language</a> expressions.</p>
<p>The aim of this document is to provide you with the necessary information to create and send application specific alerts to a Slack channel of your choosing.</p>
<h4 id="creating-your-own-custom-alerts-prerequisites">Prerequisites</h4><p>This guide assumes the following:</p>

<ul>
<li>You have <a href="/tasks.html#creating-a-cloud-platform-environment">created a namespace for your application</a></li>
</ul>
<h4 id="creating-a-slack-webhook-and-amend-alertmanager">Creating a slack webhook and amend Alertmanager</h4><p>This step requires the Cloud Platform team to create a receiver in <a href="https://github.com/ministryofjustice/cloud-platform-infrastructure/blob/master/terraform/cloud-platform-components/templates/prometheus-operator.yaml.tpl###L115">Alertmanager</a> and a <a href="https://api.slack.com/incoming-webhooks">Slack webhook</a>.</p>
<p>Create a ticket to request a new alert route in Alertmanager. The team will need the following information:</p>

<ul>
<li>namespace name</li>
<li>team name</li>
<li>application name</li>
<li>slack channel</li>
</ul>
<p>The team will provide you with a &ldquo;<code>custom severity level</code>&rdquo; that&rsquo;ll need to be defined in the next step. Please copy it to your clipboard.</p>
<h4 id="create-a-prometheusrule">Create a PrometheusRule</h4><p>A <code>PrometheusRule</code> is a custom resource that defines your triggered alert. This file will contain the alert name, promql expression and time of check.</p>
<p>To create your own custom alert you&rsquo;ll need to fill in the template below and deploy it to your namespace (tip: you can check rules in your namespace by running <code>kubectl get prometheusrule -n &lt;namespace&gt;</code>).</p>

<ul>
<li>Create a file called <code>prometheus-custom-rules-&lt;application_name&gt;.yaml</code></li>
<li>Copy in the template below and replace the bracket values, specifying the requirements of your alert. The <code>&lt;custom_severity_level&gt;</code> field is the value you were passed earlier.</li>
</ul>
<div class="highlight"><pre class="highlight yaml"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">monitoring.coreos.com/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PrometheusRule</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">&lt;namespace&gt;</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">role</span><span class="pi">:</span> <span class="s">alert-rules</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">prometheus-custom-rules-&lt;application_name&gt;</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">groups</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">application-rules</span>
    <span class="na">rules</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">alert</span><span class="pi">:</span> <span class="s">&lt;alert_name&gt;</span>
      <span class="na">expr</span><span class="pi">:</span> <span class="s">&lt;alert_query&gt;</span>
      <span class="na">for</span><span class="pi">:</span> <span class="s">&lt;check_time_length&gt;</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">severity</span><span class="pi">:</span> <span class="s">&lt;custom_severity_level&gt;</span>
      <span class="na">annotations</span><span class="pi">:</span>
        <span class="na">message</span><span class="pi">:</span> <span class="s">&lt;alert_message&gt;</span>
        <span class="na">runbook_url</span><span class="pi">:</span> <span class="s">&lt;http://my-support-docs&gt;</span>
</code></pre></div>
<ul>
<li>Run <code>kubectl apply -f prometheus-custom-rules-&lt;application_name&gt;.yaml -n &lt;namespace&gt;</code></li>
</ul>
<p>A working example of this would be:</p>
<div class="highlight"><pre class="highlight yaml"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">monitoring.coreos.com/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PrometheusRule</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">creationTimestamp</span><span class="pi">:</span> <span class="no">null</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">monitoring</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">role</span><span class="pi">:</span> <span class="s">alert-rules</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">prometheus-custom-rules-my-application</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">groups</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">node.rules</span>
    <span class="na">rules</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">alert</span><span class="pi">:</span> <span class="s">Quota-Exceeded</span>
      <span class="na">expr</span><span class="pi">:</span> <span class="s">100 * kube_resourcequota{job="kube-state-metrics",type="used",namespace="monitoring"} / ignoring(instance, job, type) (kube_resourcequota{job="kube-state-metrics",type="hard"} &gt; 0) &gt; 90</span>
      <span class="na">for</span><span class="pi">:</span> <span class="s">5m</span>
      <span class="na">labels</span><span class="pi">:</span>
        <span class="na">severity</span><span class="pi">:</span> <span class="s">cp-team</span>
      <span class="na">annotations</span><span class="pi">:</span>
        <span class="na">message</span><span class="pi">:</span> <span class="s">Namespace {{ $labels.namespace }} is using {{ printf "%0.0f" $value}}% of its {{ $labels.resource }} quota.</span>
        <span class="na">runbook_url</span><span class="pi">:</span> <span class="s">https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md###alert-name-kubequotaexceeded</span>
</code></pre></div><p>The <code>alert</code> name, <code>message</code> and <code>runbook_url</code> annotations will be sent to the Slack channel when the rule has been triggered.</p>
<p>You can view the applied rules with the following command:</p>
<div class="highlight"><pre class="highlight shell"><code>kubectl <span class="nt">-n</span> &lt;namespace&gt; describe prometheusrules prometheus-custom-rules-&lt;application_name&gt;
</code></pre></div><h4 id="prometheusrule-examples">PrometheusRule examples</h4><p>If you&rsquo;re struggling for ideas on how and which alerts to setup, please see some examples <a href="https://github.com/ministryofjustice/cloud-platform-infrastructure/blob/master/terraform/cloud-platform-components/resources/prometheusrule-examples/application-alerts.yaml">here</a>.</p>
<h4 id="advisory-note-1-prometheusrules-status-incase-of-dr-requirement-for-a-new-prometheus-install">Advisory Note 1: PrometheusRules status incase of DR/requirement for a new Prometheus Install</h4><p>The  <code>PrometheusRule</code> is a <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/##customresourcedefinitions">CRD</a> that declaratively defines a desired Prometheus rule to be consumed by Prometheus and applied using a YAML file. However, if for any reason Prometheus has to be uninstalled, <code>all PrometheusRules are removed with the CRD.</code></p>
<p>We recommend all PrometheusRules to be added to the <a href="https://github.com/ministryofjustice/cloud-platform-environments">Environments Repo</a> within the namespace folder the rules refer to. This will ensure all rules are applied/present at all times.</p>
<p>PrometheusRules can still be tested/amended/applied manually, then a PR can be created to add to the Environments Repo when ready.</p>
<h4 id="advisory-note-2-cputhrottlinghigh-alert">Advisory Note 2: CPUThrottlingHigh Alert</h4><p>The <code>CPUThrottlingHigh</code> alert is configured as part of the default rules when installing prometheus-operator. The alert can trigger when containers have low cpu limits, spiky workloads but very low average usage. CPU throttling can activate during those spikes. CPU usage is based on <a href="https://en.wikipedia.org/wiki/Completely_Fair_Scheduler">CFS</a>.</p>
<p>If you think this may be causing an issue with your application, we recommend raising your CPU limit, whilst keeping the container CPU request as close to the 95%-ile average usage as possible.</p>
<h4 id="creating-your-own-custom-alerts-further-reading">Further reading</h4>
<ul>
<li><a href="https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/getting-started.md">Prometheus Operator - Getting Started</a></li>
<li><a href="https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/alerting.md">Alerting</a></li>
</ul>

<h3 id="getting-application-metrics-into-prometheus">Getting Application Metrics into Prometheus</h3><p>Prometheus collects metrics from monitored targets by scraping metrics HTTP endpoints on these targets. There are two ways to create a metrics endpoint. The first is when the metrics endpoint is embedded within the application referred to as <code>instrumentation</code>. The second is when the metrics endpoint is part of a deployed process that bridges the gap between Prometheus and systems that do not export metrics in the prometheus format, this is called an <code>exporter</code>.</p>
<p>The following exporters are installed as part of the Cloud Platform cluster build:</p>

<ul>
<li>kubeEtcd</li>
<li>kubeApiServer</li>
<li>kubeDns</li>
<li>kubeControllerManager</li>
<li>kubeScheduler</li>
<li>kube-state-metrics</li>
<li>nodeExporter</li>
<li>kubelet</li>
</ul>
<p>Click <a href="https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exporters.md">here</a> for a list of exporters and client libraries listed on the official Prometheus Github repo.</p>
<h4 id="instrumentation-of-the-cloud-platform-reference-application">Instrumentation of The Cloud-Platform Reference Application</h4><p>The <a href="https://github.com/ministryofjustice/cloud-platform-reference-app">Cloud Platform Reference Application</a> has <code>instrumentation</code> setup using <a href="https://github.com/korfuri/django-prometheus">django-prometheus</a>. The default install steps were followed which created a metrics endpoint.</p>
<p>See screenshot below of how the metrics endpoint looks on a browser:
<a href="https://raw.githubusercontent.com/ministryofjustice/cloud-platform-user-docs/master/images/metrics-endpoint.png" target="_blank" rel="noopener noreferrer"><img src="https://raw.githubusercontent.com/ministryofjustice/cloud-platform-user-docs/master/images/metrics-endpoint.png" alt="Image of metrics" /></a></p>
<h4 id="create-a-service-to-expose-pods">Create a Service to expose Pods</h4><p>Scraping an exporter or separate metrics port requires a service that targets the Pod(s) of the exporter or application.</p>
<p>Example:</p>
<div class="highlight"><pre class="highlight yaml"><code><span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-app-service</span>
  <span class="na">labels</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">my-app</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">my-app</span>
  <span class="na">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">protocol</span><span class="pi">:</span> <span class="s">TCP</span>
    <span class="na">port</span><span class="pi">:</span> <span class="s">8000</span>
    <span class="na">targetPort</span><span class="pi">:</span> <span class="s">8000</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">metrics</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">NodePort</span>
</code></pre></div><h4 id="create-a-service-monitor">Create a Service-Monitor</h4><p>A <code>ServiceMonitor</code> is a resource the Prometheus Operator introduces for Kubernetes that describes the set of targets to be monitored by Prometheus</p>
<p>Service Monitor Architecture
<a href="https://raw.githubusercontent.com/ministryofjustice/cloud-platform-user-docs/master/images/service-monitor-arch.png" target="_blank" rel="noopener noreferrer"><img src="https://raw.githubusercontent.com/ministryofjustice/cloud-platform-user-docs/master/images/service-monitor-arch.png" alt="Image of Service-Monitor Architecture" /></a></p>
<p>Example:</p>
<div class="highlight"><pre class="highlight yaml"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">monitoring.coreos.com/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ServiceMonitor</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">my-app</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">my-app</span>
  <span class="na">endpoints</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="s">metrics</span>
    <span class="na">interval</span><span class="pi">:</span> <span class="s">15s</span>
</code></pre></div><p>More detailed information about service-monitors can be found <a href="https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/running-exporters.md">here</a></p>
<h4 id="networkpolicy-for-monitoring-namespace">NetworkPolicy for Monitoring Namespace</h4><p>By default, all connections from outside a namespace are blocked, therefore a network policy is required for the <code>monitoring</code> namespace to be able to connect into an application namespace to scrape the metrics endpoint.</p>
<p>Example:</p>
<div class="highlight"><pre class="highlight yaml"><code><span class="na">kind</span><span class="pi">:</span> <span class="s">NetworkPolicy</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">networking.k8s.io/v1</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">allow-prometheus-scraping</span>
  <span class="na">namespace</span><span class="pi">:</span> <span class="s">my-app-namespace</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">podSelector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
      <span class="na">app</span><span class="pi">:</span> <span class="s">my-app</span>
  <span class="na">policyTypes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">Ingress</span>
  <span class="na">ingress</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">from</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">namespaceSelector</span><span class="pi">:</span>
        <span class="na">matchLabels</span><span class="pi">:</span>
          <span class="na">component</span><span class="pi">:</span> <span class="s">monitoring</span>
</code></pre></div><p>You can view your current NetworkPolices with the following command:</p>
<div class="highlight"><pre class="highlight shell"><code>kubectl <span class="nt">-n</span> &lt;namespace&gt; get networkpolicies
</code></pre></div><h4 id="advisory-note-applications-configured-to-use-multiple-processes">Advisory note: Applications configured to use multiple processes</h4><p>If you&rsquo;re using a pre-forking web server (like unicorn or puma for Ruby, or gunicorn for Python) and have it configured to use multiple processes, then you need to use a Prometheus client library which supports exporting metrics from multiple processes. Not all the official clients do that. If you don&rsquo;t use a library which supports this, then requests to <code>/metrics</code> could be served by any of the processes, which would mean Prometheus sees inconsistent data on each scrape</p>



<h2 id="application-logging">Application Logging</h2>
<h3 id="application-log-collection-and-storage">Application Log Collection and Storage</h3><h4 id="application-log-collection-and-storage-overview">Overview</h4><p>The Cloud Platform supports the ability for application logs to be collected, stored, and accessed.</p>
<p>Logs are collected automatically, via Fluentd, stored in an AWS-Hosted ElasticSearch Cluster, and accessed via AWS-Hosted Kibana dashboard.</p>
<h4 id="log-collection">Log Collection</h4><p>Fluentd is a cluster-wide log collection service that runs in it&rsquo;s own dedicated environment.</p>
<p>The Fluentd application is configured with cluster-wide read permissions. The only requirement for Fluentd to start automatically collecting your application&rsquo;s logs is to have your application output logs to <code>stdout</code>.</p>
<h4 id="log-storage">Log Storage</h4><p>As an application engineer, you won&rsquo;t really need to pay much attention to how the logs are stored, as this is handled by the Cloud Platform team.</p>
<p>The logs are shipped from Fluentd and stored in an AWS-Hosted ElasticSearch cluster. All application logs are retained for 30 days, before being curated for deletion.</p>

<h3 id="accessing-application-log-data">Accessing Application Log Data</h3><h4 id="accessing-application-log-data-overview">Overview</h4><p>This document is intended to assist engineers in accessing application and system logs stored in a centralized Elasticsearch cluster.</p>
<h4 id="access-kibana">Access Kibana</h4><p>The Cloud Platform collects, indexes and presents your application and system log data enabling you to query using Kibana’s standard query language (based on Lucene query syntax).</p>
<p>To access Kibana, follow the appropriate link below and authenticate with your GitHub credentials:</p>
<h5 id="live-1-cluster">Live-1 Cluster</h5><p><a href="https://kibana.cloud-platform.service.justice.gov.uk/_plugin/kibana">https://kibana.cloud-platform.service.justice.gov.uk/_plugin/kibana</a></p>
<h5 id="live-0-cluster">Live-0 Cluster</h5><p><a href="https://kibana.apps.cloud-platform-live-0.k8s.integration.dsd.io/_plugin/kibana/">https://kibana.apps.cloud-platform-live-0.k8s.integration.dsd.io/plugin/kibana/</a></p>
<h4 id="using-kibana">Using Kibana</h4><p>As a quick example, we will filter down to the logs of a particular environment.</p>
<p>1) On the Kibana dashboard, select the &lsquo;Discover&rsquo; tab.</p>
<p>2) Select &#39;Add a filter +&rsquo;</p>
<p>3) Filter <code>kubernetes.namespace_name</code>, with operator <code>is</code> and the value equal to your environment name.</p>
<p>More in-depth guides on using Kibana can be found below:</p>
<p><a href="https://www.elastic.co/guide/en/kibana/6.3/search.html">https://www.elastic.co/guide/en/kibana/6.3/search.html</a></p>
<p><a href="https://www.elastic.co/guide/en/beats/packetbeat/current/kibana-queries-filters.html">https://www.elastic.co/guide/en/beats/packetbeat/current/kibana-queries-filters.html</a></p>



<h2 id="migrating-from-live-0-to-live-1">Migrating from Live-0 to Live-1</h2><h3 id="migrating-from-live-0-to-live-1-overview">Overview</h3><p>After some long consideration of possible options, the decision has been made to migrate from the <code>live-0</code> cluster to the new <code>live-1</code> cluster.</p>
<p>The reason behind this decision is based on the need to move to a dedicated AWS account, which will be much easier to support, and the need to move away from the Ireland (EU) region to the London (UK) region.</p>

<blockquote>
<p><strong>Currently, there is no fixed deadline by which services must migrate off Live-0.</strong> This guide will be updated as and when this changes.</p>
</blockquote>
<p>The purpose of this document is to aid development teams in migrating their existing applications from <code>live-0</code> to <code>live-1</code>.</p>
<p>The migration steps that need to be taken may differ for individual applications.</p>
<p>The following steps are for an application that is considered to be fairly normal and deployed through CircleCI.</p>
<p>Appending these steps are a few extra consideration points, that are not covered in the example, but may apply to your application.</p>
<h3 id="accessing-the-live-1-cluster">Accessing the Live-1 cluster</h3><p>To access the <code>live-1</code> cluster, follow the steps in the <a href="/tasks.html#authentication">authentication</a> section of this guide, and download your Kube config file.</p>
<p>Kubernetes provides a <a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#set-the-kubeconfig-environment-variable">brief guide</a> on how to set up <code>kubectl</code> to use multiple config files simultaneously.</p>
<p>You should now be able to switch contexts between the <code>live-0</code> and <code>live-1</code> clusters.</p>
<h3 id="generating-a-new-environment">Generating a new environment</h3><p>Start by following the guide to generate a new environment, this follows the same process as was followed for <code>live-0</code>, and you should use the same details as you did for your environment then.</p>
<p><a href="/tasks.html#create-an-environment">Environment generation guide.</a></p>
<p>Run a <code>kubectl get namespaces</code> to check your environment has been successfully created.</p>
<h3 id="generating-a-new-ecr-repository">Generating a new ECR repository</h3><p>Once you&rsquo;ve generated a new environment in the <code>live-1</code> cluster, you will need to generate a new ECR repository for your application to be pushed to.</p>
<p>The reason why the previous ECR repo can&rsquo;t be used is due to the new <code>live-1</code> cluster being hosted in a separate AWS account.</p>
<p>If you need reminding of the ECR creation process, please see the <a href="/tasks.html#creating-an-ecr-repository">user documentation</a>.</p>
<h3 id="changing-the-circleci-environment-variables">Changing the CircleCI environment variables</h3><p>Now that you have a new empty environment and ECR repository set-up, the next step is to point your existing CircleCI pipeline away from the <code>live-0</code> environment, to your new <code>live-1</code> environment.</p>
<p>This is done by replacing the CircleCI environment variables with the ones generated for your <code>live-1</code> environment and then rerunning the pipeline.</p>
<p>Our helper script expects environment variables to be named according to the list below where <code>&lt;ENVIRONMENT&gt;</code> should be replaced by some identifier of your choosing (eg.: <code>STAGING</code>, <code>PRODUCTION</code>).</p>
<p>Please refer to the instructions <a href="/tasks.html#deploy-to-kubernetes">here</a>.
The environment variables you will need to replace are as follows:</p>

<div style="height:1px;font-size:1px;">&nbsp;</div>
<div class="table-container">
        <table>
          <tr>
<th>Variable</th>
<th style="text-align: left"></th>
</tr>
<tr>
<td><code>AWS_DEFAULT_REGION</code></td>
<td style="text-align: left">The default region will now be <code>eu-west-2</code>.</td>
</tr>
<tr>
<td><code>AWS_ACCESS_KEY_ID</code></td>
<td style="text-align: left">The access key can be found in the secret created by the ECR generation. This requires base64 decoding.</td>
</tr>
<tr>
<td><code>AWS_SECRET_ACCESS_KEY</code></td>
<td style="text-align: left">The secret key can be found in the secret created by the ECR generation. This requires base64 decoding.</td>
</tr>
<tr>
<td><code>ECR_ENDPOINT</code></td>
<td style="text-align: left">The ECR endpoint for all repos in <code>live-1</code> is <code>754256621582.dkr.ecr.eu-west-2.amazonaws.com</code></td>
</tr>
<tr>
<td><code>KUBE_ENV_&lt;ENVIRONMENT&gt;_NAME</code></td>
<td style="text-align: left">The cluster name is <code>live-1.cloud-platform.service.justice.gov.uk</code></td>
</tr>
<tr>
<td><code>KUBE_ENV_&lt;ENVIRONMENT&gt;_NAMESPACE</code></td>
<td style="text-align: left">This variable should be equal to the name of your namespace.</td>
</tr>
<tr>
<td><code>KUBE_ENV_&lt;ENVIRONMENT&gt;_TOKEN</code></td>
<td style="text-align: left">The token can be found in your ServiceAccount&rsquo;s <code>Secret</code> (eg.: <code>circleci-token-abcdef</code>) and needs base64 decoding.</td>
</tr>
<tr>
<td><code>KUBE_ENV_&lt;ENVIRONMENT&gt;_CACERT</code></td>
<td style="text-align: left">The cert is an attribute found in the ServiceAccount&rsquo;s <code>Secret</code> and does not need base64 decoding.</td>
</tr>

        </table>
      </div>
<div style="height:1px;font-size:1px;">&nbsp;</div>
<p>After triggering the CircleCI pipeline, your application should now deploy into your new environment.</p>
<h3 id="deleting-your-live-0-deployment">Deleting your Live-0 deployment</h3><p>The last thing you will need to do is to delete your application from the <code>live-0</code> cluster.</p>
<p>Please see the documentation on <a href="/tasks.html#cleaning-up">cleaning up within the Cloud Platform</a>.</p>
<h3 id="other-considerations">Other considerations</h3><h4 id="podsecuritypolicy">PodSecurityPolicy</h4><p>In the <code>live-1</code> cluster we are introducing a restrictive <a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/"><code>PodSecurityPolicy</code></a> as part of our effort to harden the cluster and provide a stable, secure and reliable service.</p>
<p>The major change this brings is that root containers are not allowed to run. What this means is that the containers that run on the platform need to run as a non-root user. The <a href="https://github.com/ministryofjustice/cloud-platform-multi-container-demo-app/blob/9ad6caf101cc21117742e5ab2cbe5507efd54efd/rails-app/Dockerfile">solution</a> is straightforward for images we build: by using the <code>USER</code> directive in the <code>Dockerfile</code> with a <strong>numeric uid</strong>.</p>
<p>Sometimes we use images built by third parties which may or may not have taken the steps to build them as non-root images. One such very common example is nginx from the dockerhub library. If you need to run an nginx container we recommend that you use the <a href="https://github.com/bitnami/bitnami-docker-nginx"><code>bitnami/nginx</code></a> image.</p>
<p>See <a href="https://docs.bitnami.com/containers/how-to/work-with-non-root-containers">here</a> for more information on non-root containers.</p>


<h2 id="other-topics">Other Topics</h2>
<h3 id="git-crypt">Git-Crypt</h3><p>We use <code>git-crypt</code> to ensure that application secrets are encrypted at rest in git.</p>
<h4 id="git-crypt-prerequisites">Prerequisites</h4>
<ol>
<li>Install <a href="https://gnupg.org/">GPG</a></li>
<li>Install <a href="https://www.agwa.name/projects/git-crypt/">git-crypt</a></li>
<li>Generate a key pair, if you don&rsquo;t have one already. The <a href="https://help.github.com/articles/generating-a-new-gpg-key/">GitHub documentation</a> is a good reference.</li>
<li>Push your public key to a key server: <code>gpg --send-keys PUBKEYID</code></li>
<li>Add the pubkey to your GitHub account, again, following <a href="https://help.github.com/articles/adding-a-new-gpg-key-to-your-github-account/">the documentation</a></li>
</ol>
<h4 id="setup">Setup</h4>
<ul>
<li>If the repository has not been setup before, please follow the <a href="https://github.com/AGWA/git-crypt##using-git-crypt">git-crypt documentation</a> to do so.</li>
</ul>
<p>otherwise,</p>

<ul>
<li>Share your <code>PUBKEYID</code> with an existing member of the CloudPlatforms team. They will need to trust your key and add you to the repository (see git-crypt documentation above).</li>
</ul>
<h4 id="git-crypt-usage">Usage</h4><p>Once the above has been setup, update your local repository clone and unlock the secrets:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ git pull
$ git-crypt unlock
</code></pre></div><p>From this point on, <code>git-crypt</code> operates transparently.</p>
<p>You can verify the status of files by using <code>git-crypt status</code>.</p>

<h3 id="secrets-overview">Secrets overview</h3><p>We identify secrets as one of three kinds:
- user secrets
- system secrets
- application secrets</p>
<h4 id="user-secrets">User Secrets</h4><p>They are essentially any kind of secret that is owned by a specific user (eg. GitHub or AWS credentials). The user is responsible for securely managing these secrets, typically using a password manager and they should not be shared with other individuals or used in applications.</p>
<h4 id="system-secrets">System Secrets</h4><p>They are secrets used in system components, usually configured by someone who manages, configures or supports the system. For example, when setting up a CI pipeline, the credentials it uses to fetch the source code and push the produced artifacts to a repository are considered system secrets.</p>
<p>These should not be tied to an individual user but machine users should be employed. The responsibility of managing these secrets securely lies with the owner of the system.</p>
<h4 id="application-secrets">Application Secrets</h4><p>These are the secrets that the application requires at runtime. Some examples are: API keys for third-party services, keys that applications might use to communicate with each other, database credentials, cookie encryption keys and so on.</p>
<p>This kind of secrets falls under the shared responsibility model:</p>

<ul>
<li><p>the owners of the application are responsible for securely managing the secrets at rest (eg. using <a href="/tasks.html#git-crypt">git-crypt</a> to encrypt them alongside the source code) and also for managing access to the secrets once they&rsquo;ve been added to an environment,</p></li>
<li><p>the Cloud Platform team, on the other hand, is responsible for ensuring the secrets remain secure inside the environment.</p></li>
</ul>

<h3 id="migrating-an-rds-instance">Migrating an RDS instance</h3><p>This guide covers different ways of migrating RDS instances from one AWS account to another:</p>

<ul>
<li>AWS Database Migration Service (DMS)<br></li>
<li>Pure <code>pg_dump</code> &amp; <code>psql</code><br></li>
</ul>
<p><strong>The only difference between the processes is at Step 3</strong></p>
<p>This guide assumes the migration comply with the following :</p>

<ul>
<li>The migration happens from a <em>source</em> postresql RDS instance to a <em>target</em> postresql RDS instance</li>
<li>[DMS only] The <em>source</em> RDS is running postgresql 9.4.9 or above</li>
<li>Elevated &amp; short-lived sets of postgres credentials are available for both <em>source</em> and <em>target</em></li>
</ul>
<h4 id="migrating-an-rds-instance-overview">Overview</h4><h5 id="postgres-utilities">Postgres utilities</h5><p>It is possible to do a full database migration using only official CLI tools, provided by Postgres. 
Using <code>pg_dump</code> and <code>psql</code>, this documents describes the migration process.</p>
<p><a href="https://www.postgresql.org/docs/9.4/app-pgdump.html">pg_dump</a></p>
<p><strong>Using this tooling implies having a <em>source</em> database downtime</strong>. (As you don&rsquo;t want data being written to it while migrating it.)</p>
<p>The steps including those tools will always be the same; on one side we export from source, on the target side we restore.</p>
<h5 id="dms">DMS</h5><p>AWS Database Migration Service (DMS) is useful to migrate data from a database to another, and <em>keep them unidirectionally in sync</em>.</p>
<p>In  other words, DMS can only migrate data, but it ensures that any changes on <em>source</em> will be replicated to <em>target</em>
Note: any change to <em>target</em> will not be replicated to <em>source</em>.</p>
<p>Even if DMS is used to migrate the data, the postgres utilities are still needed to migrate the meta-data. (FK, sequences, etc.).</p>
<h5 id="pre-data-data-post-data">Pre-Data, Data, Post-Data</h5><p><code>pg_dump</code> can be used to export one big archive that can then be restored with <code>pg_restore</code>.
One issue with that approach is the difficulty of troubleshooting migration issues.</p>
<p>Since we are trying to make this process as clear as possible, the following guide is decomposing <code>pg_dump</code> into its three components :</p>

<ul>
<li>Pre-data : The table structures, functions.</li>
<li>Post-data: indexes, triggers, rules, and constraints</li>
<li>Data : data</li>
</ul>
<p>With that deconstructed process, it is easier to debug issues (and get help from the Cloud Platform team) and most importantly your team can perform validation/testing incrementally.</p>
<h4 id="step-0-pod">Step 0 - Pod</h4><p>In order to run postgresql commands against both of those endpoints, there needs to be a place that has access to both.</p>
<p>This is solved by running a pod into the kubernetes cluster, on live-1, into the team&rsquo;s namespace. 
The migration steps outlined below have been tested from a pod running a <code>bitnami/postgresql</code> Docker image.</p>
<p>Regarding the network access:</p>

<ul>
<li>The <em>source</em> RDS needs to have its <code>public accessibility</code> config turned on.</li>
<li>The RDS security group needs to be opened to the Cluster. For that, add inbound rules from the NAT gateways&rsquo; IP address on the 5432 port.<br></li>
<li>The RDS instance needs to support SSL connections</li>
</ul>
<h4 id="step-1-pre-data">Step 1 - Pre-Data</h4><p>First, to export,  we run :</p>
<div class="highlight"><pre class="highlight plaintext"><code>pg_dump -U source_username \
     -h source_endpoint \
     -d source_database \
     -O \
     --section=pre-data &gt; pre-data.sql
</code></pre></div><p>Here, <code>-O</code> tells RDS to export the table structure without owners.
The command above stores the data in a local file.</p>
<p>Then to restore this into the <em>target</em>, we use <code>psql</code>:</p>
<div class="highlight"><pre class="highlight plaintext"><code>psql -U target_username \
     -h target_endpoint \
     -d target_database \
     -f pre-data.sql

</code></pre></div><p>If using a local file is problematic, those two commands can be piped together (<code>|</code>)</p>
<h4 id="step-2-sequences">Step 2 - Sequences</h4><p>Sequences are essential for your database to know what the latest increment of the primary keys is. Sequences are held in special tables that will not be migrated from step 1.</p>
<p>First, to export,  we run :</p>
<div class="highlight"><pre class="highlight plaintext"><code>pg_dump -U source_username \
     -h source_endpoint \
     -d source_database \
     -t '*_seq' &gt; sequences.sql
</code></pre></div><p>Here, <code>-t &#39;*_seq&#39;</code> indicates to <code>pg_dump</code> that we only want to export the table ending in <code>_seq</code>, which are the sequences.</p>
<p>Then to restore this into the <em>target</em>, we use <code>psql</code>:</p>
<div class="highlight"><pre class="highlight plaintext"><code>psql -U target_username \
     -h target_endpoint \
     -d target_database \
     -f sequences.sql

</code></pre></div><p>If using a local file is problematic, those two commands can be piped together (<code>|</code>)</p>
<h4 id="step-3-dms-only">Step 3 [ DMS ONLY ]</h4><p>This step has to be done with the assistance of the Cloud Platform.</p>
<p>The DMS stack is build using a terraform module.</p>
<p>Please refer to <a href="https://github.com/ministryofjustice/cloud-platform-terraform-dms">this</a>
 repository to see the DMS module instructions:<br>
 <a href="https://github.com/ministryofjustice/cloud-platform-terraform-dms">https://github.com/ministryofjustice/cloud-platform-terraform-dms</a></p>
<p>IF YOU HAVE FOLLOWED THAT STEP, GO STRAIGHT TO STEP 4</p>
<h4 id="step-3-pure-pg-dump">Step 3 [ PURE PG_DUMP ]</h4><p>IF YOU HAVE FOLLOWED THE DMS STEP ABOVE (DMS), SKIP THIS ONE.</p>
<p>Sequences are essential for your database to know what the latest increment of the primary keys is. Sequences are held in special tables that will not be migrated from step 1.</p>
<p>First, to export,  we run :</p>
<div class="highlight"><pre class="highlight plaintext"><code>pg_dump -U source_username \
     -h source_endpoint \
     -d source_database \
     -O \
     --section=data &gt; data.sql
</code></pre></div><p>Here, <code>-O</code> tells RDS to export the table structure without owners.
The command above stores the data in a local file.</p>
<p>Then to restore this into the <em>target</em>, we use <code>psql</code>:</p>
<div class="highlight"><pre class="highlight plaintext"><code>psql -U target_username \
     -h target_endpoint \
     -d target_database \
     -f data.sql

</code></pre></div><p>If using a local file is problematic, those two commands can be piped together (<code>|</code>)</p>
<h4 id="step-4-post-data">Step 4 - Post-Data</h4><p>Any constraints, indexes and foreign keys are also a special kind of metadata that would not be migrated during any of the steps above. 
All of data is contained within the <code>post-data</code> section.</p>
<p>The process is almost identical as Step 1 :</p>
<p>First, to export,  we run :</p>
<div class="highlight"><pre class="highlight plaintext"><code>pg_dump -U source_username \
     -h source_endpoint \
     -d source_database \
     -O \
     --section=post-data &gt; post-data.sql
</code></pre></div><p>Then to restore this into the <em>target</em>, we use <code>psql</code>:</p>
<div class="highlight"><pre class="highlight plaintext"><code>psql -U target_username \
     -h target_endpoint \
     -d target_database \
     -f post-data.sql
</code></pre></div><p>If using a local file is problematic, those two commands can be piped together (<code>|</code>)</p>
<h4 id="step-5-data-validation-very-important">Step 5 - Data Validation (Very Important)</h4><p>After a migration, <strong>it is your team&rsquo;s responsibility</strong> to ensure the data, its integrity and anything required by your application to operate properly have been preserved.</p>
<p>Even though the process above is handling the data and the meta-data migration, it is essential for you to have a <em>data validation strategy</em> to confirm everything is in order.</p>
<p>The Cloud Platform team can&rsquo;t provide a how-to guide on data validation, as each database migrations are wildly different.</p>
<h4 id="step-6-clean-up">Step 6 - Clean up</h4><p>After a successful migration, we can clean up by :</p>

<ul>
<li>Deleting the pod from STEP 1 </li>
<li>Disabling the network access from the live-1 cluster to the <em>source</em> RDS</li>
<li>Remove the DMS stack (if applicable)</li>
<li>Revoke the temporary credentials created for the migration</li>
</ul>

<h3 id="kubectl-quick-reference">Kubectl quick reference</h3><p>This document acts as a quick reference to <code>kubectl</code>, listing some of the most common operations.</p>
<p>The examples here only address the most basic approach to these operations. For more options, please refer to the command-line help of <code>kubectl</code> subcommands.</p>
<p>There is also a more detailed <a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/">cheatsheet</a> in the official kubernetes documentation.</p>
<h6 id="inspecting-running-instances-of-the-application">Inspecting running instances of the application</h6><p>To list running <code>Pods</code>:
<code>
$ kubectl -n &lt;namespace&gt; get pods
</code>
To view details for a <code>Pod</code>:
<code>
$ kubectl -n &lt;namespace&gt; describe pod &lt;pod&gt;
</code></p>
<h6 id="viewing-logs">Viewing logs</h6><p>To access the logs of a running container:
<code>
$ kubectl -n &lt;namespace&gt; logs &lt;pod&gt;
</code></p>
<h6 id="viewing-kubernetes-events">Viewing kubernetes events</h6><p>To see kubernetes events, which can help debugging:
<code>
$ kubectl -n &lt;namespace&gt; get events
</code></p>
<h6 id="container-shell">Container shell</h6><p>You can get a shell inside a running container:
<code>
$ kubectl -n &lt;namespace&gt; exec -it &lt;pod&gt; sh
</code></p>
<p>For more information, click <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/">here</a></p>
<h6 id="pod-port-forwarding">Pod port-forwarding</h6><p>To forward port <code>5000</code> on <code>localhost</code> to port <code>5001</code> in the <code>Pod</code>:
<code>
$ kubectl -n &lt;namespace&gt; port-forward &lt;pod&gt; 5000:5001
</code></p>
<p>For more information, click <a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/">here</a></p>

<h3 id="cloud-platform-support">Cloud Platform Support</h3><p>This google document describes the Cloud Platform Support process, along with a description of what is/is not supported.</p>
<p><a href="https://docs.google.com/document/d/1M89TXQJIQAqu8yb2wzatlI8PgsXrgOi-GxLtB76XgjU/edit?usp=sharing">https://docs.google.com/document/d/1M89TXQJIQAqu8yb2wzatlI8PgsXrgOi-GxLtB76XgjU/edit?usp=sharing</a></p>

<h3 id="zero-downtime-deployments">Zero Downtime Deployments</h3><h4 id="zero-downtime-deployments-introduction">Introduction</h4><p>Zero Downtime Deployments are a significant feature of the Cloud Platform, that bring a host of advantages.</p>
<h4 id="rolling-updates">Rolling Updates</h4><p>Rolling updates introduce the ability to update an application without any downtime.</p>
<p>A rolling update works by ensuring there is always one extra Pod than the maximum number stated in the deployment.</p>
<p>The new deployment is applied incrementally, usually one to two Pods at a time until all Pods are running the latest version. How the deployment is rolled out, can be configured by the user. More information about configuring a rolling update can be found <a href="https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/">here</a>.</p>
<p>If an application is exposed publicly, traffic will only be routed to the available Pods. However, this is only the case if the user configured <code>readinessProbes</code> correctly. This is a large topic, so more information can be found <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">here</a>.</p>
<h5 id="the-major-advantage">The Major Advantage</h5><p>Where rolling updates really brings benefit is the enabling of Continuous Integration and Continuous Delivery.</p>
<p>The ability to constantly update your application, with zero downtime, brings a host of benefits.</p>
<h5 id="rolling-updates-further-reading">Further Reading</h5><p>If you&rsquo;d like to read more in-depth into rolling updates, then Kubernetes has some great documentation <a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/">here</a>.</p>
<h5 id="readiness-liveness-probes-and-ssl-in-rails-applications">Readiness/Liveness probes and SSL in Rails applications</h5><p>For zero downtime deployments, you will need a readiness probe in your application, so that the cluster knows when your container is ready to receive traffic. You will also need a liveness probe, so that the cluster knows if it needs to restart your container after a crash.</p>
<p>SSL will be terminated outside of your pod, so your probes will need to respond to HTTP requests. However, Ruby on Rails applications are sometimes configured to only respond to HTTPS traffic (by adding <code>config.force_ssl = true</code> in the <code>config/environments/production.rb</code> file).</p>
<p>In this case, the application will respond to any HTTP request with a redirect status code, asking the requester to resend the request to the equivalent HTTPS URL. This will cause your probes to fail, because the redirect will not be followed.</p>
<p>To fix this, the probe needs to tell the application that it is an HTTPS request, even though it isn&rsquo;t, so that the application will process the request rather than sending a redirect response. You can do this by adding some HTTP headers to your probe definitions like this:</p>
<div class="highlight"><pre class="highlight plaintext"><code>readinessProbe:
  httpGet:
    path: /ping.json
    port: 3000
    httpHeaders:
      - name: X-Forwarded-Proto
        value: https
      - name: X-Forwarded-Ssl
        value: "on"
</code></pre></div><p>This will send an HTTP request to <code>/ping.json</code>, but the rails application will respond as if it is HTTPS, and your probe should work as expected. This works for both readiness and liveness probes.</p>

<h3 id="using-a-custom-domain">Using a custom domain</h3><h4 id="background">Background</h4><p>Every application running on Cloud Platform is able to use a hostname for their
HTTP endpoints, under a pre-defined domain. For example, on the <code>live-1</code>
cluster, this would be <code>*.apps.live-1.cloud-platform.service.justice.gov.uk</code>. As
long as it is defined on the <code>Ingress</code> resource, it works automatically, using a
wildcard TLS certificate.</p>
<p>However, most applications will typically need to be served on their own,
application-specific <code>gov.uk</code> hostname. These hostnames (or usually, entire domains)
are managed individually and there is a number of actions in order to set them
up for usage.</p>
<h4 id="using-a-custom-domain-setup">Setup</h4><p>Domains are managed inside DNS zones. You can read more about the structure of
the Domain Name System in this <a href="https://en.wikipedia.org/wiki/Domain_Name_System#Structure">page</a>. It is recommended
that applications use their own DNS zones, in order to aid with management and
provide better isolation.</p>
<h5 id="defining-the-dns-zone">Defining the DNS zone</h5><p>To create the zone, you simply need to define it as a resource in your
environment. Make sure to read <a href="https://ministryofjustice.github.io/technical-guidance/standards/naming-domains/#naming-domains">the guidance on naming domains</a>
first and follow the instructions to <a href="/tasks.html#creating-a-route-53-hosted-zone">create the Route53 zone</a>.
To simplify management, we recommend that you only define a single zone, as part
of the resources for your production environment, if possible. You can still use
subdomains of that zone for different environments.</p>
<p>Once the zone is created, you will need to setup the necessary name server (NS)
records in the parent DNS zone, before you&rsquo;re able to use it. For more
information on how this delegation method works, you can read about authoritative
name servers in this <a href="https://en.wikipedia.org/wiki/Name_server#Authoritative_name_server">page</a>.</p>
<p>If your zone is for a subdomain of <code>service.justice.gov.uk</code> (eg.: <code>https://myapp.service.justice.gov.uk</code>),
the Cloud Platform team can help you set it up; please <a href="http://goo.gl/msfGiS">create a support ticket</a>.</p>
<p>For any other domains (including any subdomain of <code>gov.uk</code>, eg.: <code>https://myservice.gov.uk</code>),
you will need to contact the parent zone&rsquo;s administrators to set this up. If in
doubt, don&rsquo;t hesitate to get in touch with us in <code>#ask-cloud-platform</code>.</p>
<p>Please note, once you setup the NS records, you&rsquo;ll be delegating control of the
zone to the Cloud Platform. Hostnames used by your services (using <code>Ingresses</code>)
will be automatically managed by the cluster.</p>
<p>If you wish to create custom records in your zone you can do so by defining them
in the <a href="https://github.com/ministryofjustice/cloud-platform-environments/">environments repository</a> using the terraform
<a href="https://www.terraform.io/docs/providers/aws/r/route53_record.html"><code>aws_route53_record</code></a> resource.</p>
<h5 id="obtaining-a-certificate">Obtaining a certificate</h5>

<ol>
<li>Create the <code>Certificate</code> resource, filling in any placeholders with your
details. The <code>secretName</code> attribute defines the name of a <code>Secret</code> in your
namespace where the certificate and key material will be stored</li>
</ol>

<div class="highlight"><pre class="highlight plaintext"><code>   ---
   apiVersion: certmanager.k8s.io/v1alpha1
   kind: Certificate
   metadata:
     name: &lt;my-cert&gt;
     namespace: &lt;my-namespace&gt;
   spec:
     secretName: &lt;my-cert-secret&gt;
     issuerRef:
       name: letsencrypt-production
       kind: ClusterIssuer
     commonName: '&lt;hostname&gt;'
     acme:
       config:
       - domains:
         - '&lt;hostname&gt;'
         dns01:
           provider: route53-cloud-platform
</code></pre></div>

<ol>
<li>Make sure the certificate has been issued correctly, by checking its <code>Status</code>:</li>
</ol>

<div class="highlight"><pre class="highlight plaintext"><code>   $ kubectl describe certificate &lt;my-cert&gt;
</code></pre></div><p>The certificate status of type <code>&quot;Ready&quot;</code> should be <code>True</code>:</p>
<div class="highlight"><pre class="highlight plaintext"><code>   Status:
     Conditions:
       Last Transition Time:  2019-06-05T10:16:43Z
       Message:               Certificate is up to date and has not expired
       Reason:                Ready
       Status:                True
       Type:                  Ready
     Not After:               2019-09-03T09:16:42Z
   Events:
     Type    Reason         Age   From          Message
     ----    ------         ----  ----          -------
     Normal  Generated      3m    cert-manager  Generated new private key
     Normal  OrderCreated   3m    cert-manager  Created Order resource "&lt;my-cert&gt;-3189350212"
     Normal  OrderComplete  1m    cert-manager  Order "&lt;my-cert&gt;-3189350212" completed successfully
     Normal  CertIssued     1m    cert-manager  Certificate issued successfully
</code></pre></div><p>It generally takes but a few minutes for the certificate to be prepared and
   the events displayed should indicate if there is a problem or it simply needs
   more time. If you cannot obtain a certificate, please get in touch with us in
   <code>#ask-cloud-platform</code>.</p>

<ol>
<li>You will need to update your <code>Ingress</code> spec to include the new hostname.</li>
</ol>
<div class="highlight"><pre class="highlight plaintext"><code>     apiVersion: extensions/v1beta1
     kind: Ingress
     metadata:
       name: &lt;my-ingress&gt;
       namespace: &lt;my-namespace&gt;
     spec:
       tls:
       - hosts:
         - my-app.apps.live-1.cloud-platform.service.justice.gov.uk
   +   - hosts:
   +     - &lt;hostname&gt;
   +     secretName: &lt;my-cert-secret&gt;
       rules:
       - host: my-app.apps.live-1.cloud-platform.service.justice.gov.uk
         http:
           paths:
           - path: /
             backend:
               serviceName: &lt;my-svc&gt;
               servicePort: 80
   +   - host: &lt;hostname&gt;
   +     http:
   +       paths:
   +       - path: /
   +         backend:
   +           serviceName: &lt;my-svc&gt;
   +           servicePort: 80
</code></pre></div><p>Once you&rsquo;ve made the changes to your <code>Ingress</code>, the cluster (and more
specifically, <code>external-dns</code>) will update the necessary records defined in it.
This usually takes less than a minute before you are able to access your
endpoint. However, depending on the DNS name servers your workstation uses, you
might need to wait longer or try to &ldquo;flush&rdquo; your local DNS cache in order to
speed up the process. You should search online for the proper method to do so,
based on your operating system and/or browser.</p>

<h3 id="ip-whitelisting">IP Whitelisting</h3><h4 id="inbound-ip-whitelisting">Inbound IP Whitelisting</h4><p>Allowed client IP source ranges can be specified using the nginx.ingress.kubernetes.io/whitelist-source-range annotation. The value is a comma separated list of CIDRs, e.g. 1.1.1.1/24,10.0.0.0/24.</p>
<p><a href="https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#whitelist-source-range">Kubernetes official documentation on whitelisting source ranges</a>.</p>
<p>An example configuration using &ldquo;nginx.ingress.kubernetes.io/whitelist-source-range: 1.1.1.1/24,10.0.0.0/24&rdquo;</p>
<div class="highlight"><pre class="highlight plaintext"><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
          {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{"kubernetes.io/ingress.class":"nginx","nginx.ingress.kubernetes.io/whitelist-source-range":"1.1.1.1/24,10.0.0.0/24"},"name":"&lt;my-ingress&gt;","namespace":"&lt;my-namespace&gt;"},"spec":{"rules":[{"host":"my-app.apps.live-1.cloud-platform.service.justice.gov.uk","http":{"paths":[{"backend":{"serviceName":"&lt;my-svc&gt;","servicePort":3000},"path":"/"}]}}],"tls":[{"hosts":["my-app.apps.live-1.cloud-platform.service.justice.gov.uk"]}]}}
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/whitelist-source-range: 1.1.1.1/24,10.0.0.0/24
  creationTimestamp: 2019-03-21T14:26:31Z
  generation: 1
  name: &lt;my-ingress&gt;
  namespace: &lt;my-namespace&gt;
spec:
  rules:
  - host: my-app.apps.live-1.cloud-platform.service.justice.gov.uk
    http:
      paths:
      - path: /
      - backend:
          serviceName: &lt;my-svc&gt;
          servicePort: 3000
  tls:
  - hosts:
    - my-app.apps.live-1.cloud-platform.service.justice.gov.uk
status:
  loadBalancer:
    ingress:
    - hostname: &lt;hostname&gt;
</code></pre></div><p>Testing with the annotation set:</p>
<div class="highlight"><pre class="highlight plaintext"><code>curl -v -H "Host: my-app.apps.live-1.cloud-platform.service.justice.gov.uk" &lt;HOST-IP&gt;
</code></pre></div><p>Will return a &ldquo;403 forbidden&rdquo; status</p>
<h4 id="outbound-ip-whitelisting">Outbound IP Whitelisting</h4><h5 id="nat-gateways">NAT Gateways</h5><p>Many applications use third-party tools for a variety of reasons, and many of these tools require IP Whitelisting.</p>
<p>The Cloud Platform uses NAT Gateways as its external IP Endpoints.</p>
<p>The IP addresses for the clusters are as follows:</p>
<h6 id="nat-gateways-live-0-cluster">Live-0 Cluster</h6><div class="highlight"><pre class="highlight plaintext"><code>52.17.133.167
34.251.93.81
34.247.134.240
</code></pre></div><h6 id="nat-gateways-live-1-cluster">Live-1 Cluster</h6><div class="highlight"><pre class="highlight plaintext"><code>35.178.209.113
3.8.51.207
35.177.252.54
</code></pre></div>
<p><h3 id="setup-postgres-container">Setup Postgres container</h3><p>If you need to quickly create DB for test/dev, you can set up a pod in your namespace running postgres in a container. Deploying <a href="https://github.com/helm/charts/tree/master/stable/postgresql">Bitnami PostgreSQL</a> as Helm Chart is the easiest way to get started with PostgreSQL on Kubernetes. This chart bootstraps a PostgreSQL deployment on a Kubernetes cluster using the Helm package manager.</p>
<p>The <a href="/tasks.html#deploying-an-application-to-the-cloud-platform-with-helm">django-reference-application</a> uses Bitnami PostgreSQL Helm Chart to add a postgres instance, you can use the same Chart to setup postgres in your namespace.</p></p>

<blockquote>
<p>Note: Even though we are going to install a database within the Kubernetes cluster, it is recommended to use a database as a service offering such as <a href="https://aws.amazon.com/rds/">AWS RDS</a> if running in production.</p>
</blockquote>
<p><h5 id="setup-postgres-container-requirements">Requirements</h5><p>It is assumed you have the following:</p></p>

<ul>
<li>You have <a href="/tasks.html#creating-a-cloud-platform-environment">created an environment for your application</a></li>
<li>You have installed <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">Kubectl</a> on your local machine.</li>
<li>You have <a href="/tasks.html#authentication">Authenticated</a> to the cloud-platform-live-1 cluster.</li>
<li>You have configured <a href="/tasks.html#using-helm">Helm and Tiller</a>. </li>
</ul>
<p><h5 id="setup-postgres-container-set-up">Set up</h5><p>First copy <a href="https://github.com/ministryofjustice/cloud-platform-reference-app/blob/master/helm_deploy/django-app/charts/postgresql/values.yaml">values.yaml</a> file in to your working directory. You now have a <a href="https://github.com/helm/charts/tree/master/stable/postgresql">Bitnami PostgreSQL</a> Helm Chart &ldquo;values.yaml&rdquo; file. If you need to create your own values for postgresqlUsername,postgresqlPassword and postgresqlDatabase you can update the <code>values.yaml</code> or provide those as an argument on our installation command, the <code>set postgresql values</code> overwrites the value stored in <code>value.yaml</code> file.</p>
<p>Run the following (replacing the <code>YourName</code> with your own name and <code>env-name</code> with your environment name:</p>
<div class="highlight"><pre class="highlight plaintext"><code>    $ helm install --name &lt;YourName&gt; -f values.yaml stable/postgresql \
      --namespace &lt;env-name&gt; \
      --set postgresqlUsername=postgres,postgresqlPassword=secretpassword,postgresqlDatabase=my-database \
      --tiller-namespace &lt;env-name&gt;
</code></pre></div><h5 id="viewing-your-postgresql-db">Viewing your PostgreSQL DB</h5><p>You can now check PostgreSQL Helm Chart is deployed sucessfully:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ kubectl get pods --namespace &lt;env-name&gt;
</code></pre></div><p>If the Installation was successful you should be seeing something similar to the below:</p>
<div class="highlight"><pre class="highlight plaintext"><code>NAME                              READY     STATUS    RESTARTS   AGE
&lt;YourName&gt;-postgresql-0           1/1       Running   0          39m
</code></pre></div><p>You should have a postgres pod with the status <strong>running</strong>. You can also check the logs of the PostgreSQL pod:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ kubectl --namespace &lt;env-name&gt; logs &lt;YourName&gt;-postgresql-0 
</code></pre></div><p>If the PostgreSQL setup was successful you should be seeing tail of log as below:</p>
<div class="highlight"><pre class="highlight plaintext"><code> 12:49:28.02 INFO  ==&gt; ** PostgreSQL setup finished! **
</code></pre></div><h5 id="accessing-your-postgresql-db">Accessing your PostgreSQL DB</h5><p>PostgreSQL can be accessed via port 5432 on the following DNS name from within your cluster:</p>
<div class="highlight"><pre class="highlight plaintext"><code>&lt;YourName&gt;-postgresql.&lt;env-name&gt;.svc.cluster.local - Read/Write connection
</code></pre></div><p>The <code>postgresqlPassword</code> you have set will be stored as a secret in your namespace, to get the password for &ldquo;postgres&rdquo; run:</p>
<div class="highlight"><pre class="highlight plaintext"><code>export POSTGRES_PASSWORD=$(kubectl get secret --namespace &lt;env-name&gt; &lt;YourName&gt;-postgresql -o jsonpath=&quot;{.data.postgresql-password}&quot; | base64 --decode)
</code></pre></div><p>To connect to your database from outside the cluster execute the following commands:</p>
<div class="highlight"><pre class="highlight plaintext"><code>kubectl port-forward --namespace &lt;env-name&gt; svc/&lt;YourName&gt;-postgresql 5432:5432 &amp;
PGPASSWORD=&quot;$POSTGRES_PASSWORD&quot; psql --host 127.0.0.1 -U postgres -d my-database -p 5432
</code></pre></div><p>Congratulations on getting this far. If all went well your postgresql pod is now deployed and you could connect to your database from outside the cluster.</p></p>
<p><h3 id="applying-a-maintenance-page">Applying a Maintenance Page</h3><h4 id="applying-a-maintenance-page-overview">Overview</h4><p>This document will serve as a guide on how to apply a default
maintenance page to your application on the Cloud Platform.</p>
<h4 id="deploying-the-page">Deploying the page</h4><p>A repository has been created to store all the files related to the
maintenance page:</p>
<p><a href="https://github.com/ministryofjustice/cloud-platform-maintenance-page">cloud-platform-maintenance-page</a></p>
<p>The repository contains the manifest files needed to deploy a
standard maintenance page into your namespace. This directory also
contains the maintenance page HTML file, along with a DockerFile
to build an image to serve the maintenance page.</p>
<p>Within the <code>Kubectl_deploy</code> directory, there are 3 simple manifest
files that make up the deployment, <code>deploy.yaml</code>, <code>ingress.yaml</code>
and <code>service.yaml</code>.</p>
<p>To use this example, copy the files into your namespace and make
any changes you require, to tailor the maintenance page to your
service.</p>
<p>Once you have done this, the maintenance page will be deployed
into your namespace, ready for you to use as and when you need
it.</p>
<h5 id="maintenance-deploy-yaml">maintenance-deploy.yaml</h5><p>A notable part of this file is the <code>image</code> line, which points to
the ECR URI.</p>
<p>If you wish to customize the maintenance page, you must edit and
build the image and update the <code>image</code> value.</p>
<h5 id="maintenance-ingress-yaml">maintenance-ingress.yaml</h5><p>In this file, change the example <code>host</code> field to your applications
URL.</p>
<p>Rather than using this file, you may prefer to change your existing
<code>Ingress</code> so that the <code>backend</code> points to your maintenance page,
whenever you need to replace your service with the maintenance page.</p>
<p>This will ensure that you do not incur any downtime (by deleting
the previous ingress and creating a new one).</p></p>
<p><h3 id="how-to-decommission-unused-template-deploy-services">How to decommission unused template deploy services</h3><h4 id="1-stack-deletion-will-also-delete-related-instances-vpu-s-security-groups-network-gateways-etc">1. Stack Deletion (will also delete related instances, vpu’s, security groups, network gateways etc).</h4><p>NOTE As a first port of call we would recommend raising a support ticket in <a href="https://mojdt.slack.com/messages/C57UPMZLY/team/U58MLFA0M/">#ask-cloud-platform</a> requesting that cloud-platform delete your AWS stack.</p>
<p>In most cases cloud-platform’s will have the necessary credentials to delete the stack for you (most template-deploy stacks will have been created within the mojdsd AWS account).</p>
<p>NOTE Caution should also be taken as AWS Route53 dns configuration may be deleted (and this could be referenced in new AWS account Kubernetes setups). We would recommend making a copy of any such configuration so that it can be reinstated if necessary. 
Should you wish to go ahead yourselves, whilst it is possible to delete from the AWS stack dashboard, for a cleaner operation (the deleting of related AWS resources) the following fabric command should be carried out:</p>
<p>in order to proceed, you need to have :</p></p>

<ul>
<li>installed the prerequisite tools</li>
<li>configured your AWS credentials</li>
<li>configured your github access</li>
<li>create your own GPG key</li>
</ul>

<p>a.   Run <a href="https://dsdmoj.atlassian.net/wiki/spaces/PLAT/pages/82640903/Quickstart+-+Template+Deploy">mkvirtualenv</a> to create your local environment.</p>

<p>b.   Run the fabric command to delete your stack</p>

<p>The Fabric command to delete an AWS stack:</p>

<div class="highlight"><pre class="highlight plaintext"><code>$ fab aws:&lt;awsaccountname&gt; environment:&lt;environmentname&gt; application:&lt;projectname&gt; config:cloudformation/&lt;projectname&gt;.yaml  passwords:cloudformation/&lt;projectname&gt;-secrets.yaml -u &lt;githubsshkey&gt; cfn_delete
</code></pre></div><h4 id="2-jenkins-projects">2. Jenkins Projects:</h4><p>NOTE If you do not have the necessary permissions to carry out the following actions, raise a support ticket in [#ask-cloud-platform](https://mojdt.slack.com/messages/C57UPMZLY/team/U58MLFA0M/</p>
<p>a.   Disable the project (if you wish to retain the configuration for reference):</p>
<div class="highlight"><pre class="highlight plaintext"><code>i.   Locate your project’s job:

ii.  Click on the “Disable Project” button:
</code></pre></div><p><a href="https://i.stack.imgur.com/n77nl.png" target="_blank" rel="noopener noreferrer"><img src="https://i.stack.imgur.com/n77nl.png" alt="Image description" /></a></p>
<div class="highlight"><pre class="highlight plaintext"><code>iii. If you wish to resurrect your project job, click on “Enable” button: 
</code></pre></div><p>b.   Delete the project (if you no longer require the Jenkins deploy config):</p>
<div class="highlight"><pre class="highlight plaintext"><code>i.   Click on “Delete Project”:
</code></pre></div><p><a href="https://issues.jenkins-ci.org/secure/attachment/31690/upload.png" target="_blank" rel="noopener noreferrer"><img src="https://issues.jenkins-ci.org/secure/attachment/31690/upload.png" alt="Image description" /></a></p>
<h4 id="3-github-deploy-repos-this-should-take-place-after-you-have-deleted-the-aws-resources">3. Github deploy repos (this should take place after you have deleted the AWS resources)</h4><p>NOTE If you do not have the necessary permissions to carry out the following actions, raise a support ticket in [#ask-cloud-platform](https://mojdt.slack.com/messages/C57UPMZLY/team/U58MLFA0M/</p>
<p>a.   <a href="https://help.github.com/en/articles/archiving-repositories">Archive the Github deploy repos</a>  (if you still require the deploy configuration for reference)</p>
<div class="highlight"><pre class="highlight plaintext"><code>i.   Locate your deploy-repos. Click on the “Settings” button:

ii.  Click on the “Archive this repository” button:

iii. Type in the name of the repository and then click “I understand the consequences, archive this repository”:

iv.  The deploy repository will then be archived (confirmatory message will appear):
</code></pre></div><p>b.   <a href="https://help.github.com/en/articles/deleting-a-repository">Delete the Github deploy repos</a>  (if you no longer require the deploy configuration for reference)</p>
<div class="highlight"><pre class="highlight plaintext"><code>i.   As for archiving -Locate your deploy-repos. Click on the “Settings” button. Then click on the “Delete this repository” button:
</code></pre></div><h4 id="4-docker-images">4. Docker Images:</h4><p>a.   The docker image repository will be completely decommissioned when we have switched everything over from template-deploy</p>

<h1 id="ssl-connections-with-rds">SSL connections with RDS</h1><p>RDS instances are configured to allow SSL connections by default and also the latest versions of the client (<code>psql</code>) and
libraries (eg.: the <code>pg</code> ruby gem which builds on <code>libpq</code>) will establish an SSL connection by default.</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ psql "$url"
psql (9.6.13, server 10.6)
WARNING: psql major version 9.6, server major version 10.
         Some psql features might not work.
SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)
Type "help" for help.

dba02192a049ed7ce8=&gt; ^D\q
</code></pre></div><p>Where <code>$url</code> is of the form <code>postgres://user:pass@host:port/db</code>.</p>
<p>Additionally, AWS <a href="https://d1.awsstatic.com/whitepapers/aws-security-whitepaper.pdf">offer strong assurances</a> that a malicious actor cannot spoof their traffic or sniff
another tenant&rsquo;s traffic, even if they operate inside the same VPC.</p>
<p>However, best practices dictate that an SSL connection should be explicitly forced. PostgreSQL implements various
<a href="https://www.postgresql.org/docs/current/libpq-ssl.html">modes of operation</a>, each one offering a different level of security. Without any additional setup, we can
establish an encrypted connection with <code>sslmode=require</code>, which forces an SSL connection but does not verify the server
certificate.</p>
<h2 id="full-verification-of-certificates">Full verification of certificates</h2><p>In order to establish a connection with <code>sslmode=verify-full</code>, which offers <a href="https://en.wikipedia.org/wiki/Man-in-the-middle_attack">MITM</a> protection, we have to provide
the client with the root CA certificate before it is able to verify the chain of trust. AWS offers <a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html#PostgreSQL.Concepts.General.SSL">detailed instructions</a>
on how to exactly that.</p>
<p>As you can see below, unless provided with the root CA certificate, the client cannot fully verify the endpoint:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ psql "$url?sslmode=verify-full"
psql: could not get home directory to locate root certificate file
Either provide the file or change sslmode to disable server certificate verification.
</code></pre></div><div class="highlight"><pre class="highlight plaintext"><code>$ psql "$url?sslmode=verify-full&amp;sslrootcert=/tmp/rds-combined-ca-bundle.pem"
psql (9.6.13, server 10.6)
WARNING: psql major version 9.6, server major version 10.
         Some psql features might not work.
SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off)
Type "help" for help.

dba02192a049ed7ce8=&gt; ^D\q
</code></pre></div><p>This CA bundle can be added into your application&rsquo;s docker image. You can simply add the following directive in your
<code>Dockerfile</code>:</p>
<div class="highlight"><pre class="highlight plaintext"><code>ADD https://s3.amazonaws.com/rds-downloads/rds-combined-ca-bundle.pem /path/to/rds-combined-ca-bundle.pem
</code></pre></div><p>If you&rsquo;re developing a Ruby on Rails application, you can configure this by adding the following two options in your
<code>config/database.yml</code>:</p>
<div class="highlight"><pre class="highlight plaintext"><code>  sslmode: verify-full
  sslrootcert: /path/to/rds-combined-ca-bundle.pem
</code></pre></div><p>For other frameworks, you should consult their documentation on how to configure the database client to use SSL
connections.</p>
<h2 id="force-ssl-connections">Force SSL connections</h2><p>Ideally, we also want to completely disable unencrypted connections (by using the RDS <code>force_ssl</code> parameter) and always
perform a full verification when connecting to the RDS endpoint.</p>
<p>In order to force using secure connections for your RDS instance, you&rsquo;ll need to set <code>force_ssl = true</code> in your module
definition. See the <a href="https://github.com/ministryofjustice/cloud-platform-terraform-rds-instance/#inputs">documentation</a> for details. Once applied, you should no longer be able to establish
a connection using <code>sslmode=disable</code>:</p>
<div class="highlight"><pre class="highlight plaintext"><code>$ psql "$url?sslmode=disable"
psql: FATAL:  no pg_hba.conf entry for host "172.20.32.241", user "fDsQgBlavX", database "dba02192a049ed7ce8", SSL off
</code></pre></div>


            
          </main>

            <ul class="contribution-banner">
              <li><a href="https://github.com/ministryofjustice/cloud-platform-user-guide/blob/master/source/tasks.html.md.erb">View source</a></li>
              <li><a href="https://github.com/ministryofjustice/cloud-platform-user-guide/issues/new?labels=bug&amp;title=Re:%20'Tasks'&amp;body=Problem%20with%20'Tasks'%20(https://user-guide.cloud-platform.service.justice.gov.uk/tasks.html)">Report problem</a></li>
              <li><a href="https://github.com/ministryofjustice/cloud-platform-user-guide">GitHub Repo</a></li>
            </ul>

          <footer class="footer">
  <div class="footer__licence">
    <a class="footer__licence-logo" href="https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/" rel="license">Open Government Licence</a>
    <p class="footer__licence-description">All content is available under the <a href="https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/" rel="license">Open Government Licence v3.0</a>, except where otherwise stated</p>
  </div>

  <div class="footer__copyright">
    <a class="footer__copyright-logo" href="http://www.nationalarchives.gov.uk/information-management/re-using-public-sector-information/copyright-and-re-use/crown-copyright/">© Crown copyright</a>
  </div>
</footer>

        </div>
      </div>
    </div>

    
  </body>
</html>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-138188246-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-138188246-1');
</script>

<script>
  // Add the current window.location to the body of the
  // feedback email message.
  function sendFeedback(mailto, event) {
    event.preventDefault();

    var body = "Feedback/Problem on page: " + window.location + "\n";
    var href = mailto + '&body=' + encodeURIComponent(body);

    window.location = href;
  }

  $(document).ready(function() {
    var feedbackLink = $('a[href^="mailto:"]:contains("Feedback")')[0];
    var mailto = feedbackLink.href;
    $(feedbackLink).on('click', function(event) { sendFeedback(mailto, event); });
  });
</script>
