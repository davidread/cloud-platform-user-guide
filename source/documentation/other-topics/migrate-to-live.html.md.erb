---
title: Migrate to the "live" cluster
last_reviewed_on: 2021-09-17
review_in: 3 months
---

# <%= current_page.data.title %>

## Introduction

This is a guide for Cloud Platform users to migrate a namespace from "live-1" to "live".

Explanation of terms:

- "live-1" = The name of the existing Cloud Platform (Kubernetes) cluster, that all namespaces have been on during 2018-2021

- "live" = The name of a new Cloud Platform (EKS) cluster, that all namespaces need to be migrated to, during Autumn 2021

## Why should you migrate

The Cloud Platform is a Kubernetes cluster deployed using a tool named kOps. When the Cloud Platform was conceived in 2017, Kubernetes management tools such as EKS (Elastic Kubernetes Service from AWS) and GKE (Google Kubernetes Engine) were in their infancy. An architecture decision record (ADR) was created with the understanding and acceptance of the extra management overhead and slightly slower release times. Since then, EKS has matured to a state where the benefits of running a multi-tenant cluster without the overhead of maintaining the control plane has become extremely attractive. If you wish to read more, please see this [ADR](https://github.com/ministryofjustice/cloud-platform/blob/main/architecture-decision-record/014-Why-we-build-our-own-kubernetes-cluster.md).

## What are you migrating

You will be migrating your Cloud Platform namespace of Kubernetes resources. This includes all permissions and resources defined in the .yaml and .tf files in your namespace [directory][live-1-folder]. The new Kubernetes cluster is in the same AWS account and VPC, so that already running AWS resources such as RDS and ECR don't change during this migration.

## Prerequisites for migration

There are just a few things you need to ensure before you start the migration.

- Ensure your containers are "stateless". During the transition between clusters, your pods will be running simultaneously on both "live" and "live-1". Any state that is in the containers (or inside the pod), such as the ephemeral disk or EBS, will not be copied to the new cluster. For example if you store user session data on disk, then moving it to Elasticache will avoid web users being logged off when traffic gets routed to the other cluster.

- Check that multiple copies of your app are ok to communicate simultaneously with AWS resources. These AWS resources will be shared between clusters. For example, your RDS instance will accept connections from pods in "live" and "live-1".

- If you use Kubernetes Jobs and option `ttlSecondsAfterFinished` then refer to this guide: [ttlSecondsAfterFinished](/documentation/other-topics/migrate-to-live-k8s-jobs.html#migrating-kubernetes-jobs-to-the-quot-live-quot-cluster)

If any of these are a concern or you'd like to check in advance, please [contact the Cloud Platform team](#questions).

## How to migrate

These steps describe how to migrate. We'd love to make these as clear and simple as possible, so please leave [feedback](#feedback-welcome) if you've either struggled to understand a step or feel it could be clarified further.

## Step 1 - Agree a time to perform the migration

During the process of migration you can still deploy your app to both clusters together. However you will not be able to change your environment configuration, as defined in [environments repo][live-1-folder], which may be an issue. You should speak to your team and agree a time and date to attempt to migrate your namespace. It's also strongly recommended that you get familiar with migrating a non-production namespace before attempting prod.

## Step 2 - Amend your ingress resource

You must add two new annotations to your existing `ingress` resource, needed for load balancing using [DNS weighting](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resource-record-sets-values-weighted.html#rrsets-values-weighted-weight). We've designed this process to allow for moving traffic gradually to the new copy of your app on the new cluster, which also provides easy roll-back, if needed.

This looks as follows:

```yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  annotations:
    external-dns.alpha.kubernetes.io/set-identifier: <ingress-name>-<namespace-name>-blue
    external-dns.alpha.kubernetes.io/aws-weight: "100"
```

(Note - Please change the `ingress-name` and `namespace-name` values in the above example. The <span style="color:blue">blue</span> is for ingress in "live-1")

The idea behind the annotation is to control where traffic is directed to. The addition of `aws-weight: 100` against this ingress to the "live-1" cluster changes nothing initially - all of the internet traffic to this domain still goes to your service on the existing cluster. But in a later step we'll add a second ingress for your domain, on the new "live" cluster. And when you increase the `aws-weight` on that one from 0 upwards, a proportion of the traffic will be directed to your service running on that cluster too.

## Step 3 - Migrate your namespace environment to "live"

To migrate your namespace from "live-1" to "live" you'll need to copy your namespace directory to a new location in the [cloud-platform-environments][env-repo] repository.

- If you've not already got a clone of [cloud-platform-environments][env-repo] repository, do that now: `git clone git@github.com:ministryofjustice/cloud-platform-environments.git`. Create a branch.

- Run the cloud-platform cli command from your namespace directory to migrate your namespace:

  ```bash
  cd namespaces/live-1.cloud-platform.service.justice.gov.uk/<namespace>
  cloud-platform environment migrate
  ```

- Commit changes to your branch, and create a pull request.

The migrate command copies the folder. In some cases it makes minor changes to the terraform or alerts to you changes you need to make manually, as described in these points:

- If you have a ServiceAccount module for GitHub Actions: please follow [this guide](migrate-to-live-sa-github-actions.html#migrating-serviceaccount-module-for-github-actions-to-the-quot-live-quot-cluster)

- If you have an Elasticsearch module, it adds an IRSA change, needed for EKS.

- If you use Kiam for [cross account IAM roles][iam-infra]: You'll need to use [IRSA][irsa] in EKS. These [roles][iam-infra] need to be defined inside  the [environments repo][env-repo] using the guidence [here][irsa-cross-account].

If you want to skip any warnings and continue anyway, the cli has a `--skip-warnings` flag you can enable.

## Step 4 - Authenticate to the "live" cluster

Grab a new set of credentials from [login.live.cloud-platform.service.justice.gov.uk][authenticate-to-cluster].

You'll download a new config.yaml. Copy that to your .kube folder:

    mv ~/Downloads/config.yaml ~/.kube/config-live

Copy your existing live-1 credentials into a separate file:

    cp ~/.kube/config ~/.kube/config-live-1

To talk to the live cluster copy it into the default location that kubectl looks for credentials:

    cp ~/.kube/config-live ~/.kube/config


## Step 5 - Add a new ingress resource

Create a new Ingress from the copy of "live-1" Ingress. Amend the `aws-weight` annotation to "0", this will not send any traffic to "live". In the later step, we start sending real traffic by tweaking external-dns ingress annotation (`aws-weight`)

The annotations will look like this:

```yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  annotations:
    external-dns.alpha.kubernetes.io/set-identifier: <ingress-name>-<namespace-name>-green
    external-dns.alpha.kubernetes.io/aws-weight: "0"
```

(Note - The <span style="color:green">green</span> is for ingress in "live"))


## Step 6 - Create a new step to your deployment pipeline

Your application's continuous deployment pipeline needs a couple of changes:

- deploy the app to both clusters, not just "live-1"
- store a second set of secrets, containing Kubernetes credentials for "live", and use them when deploying to that cluster

Below is guidance for making these changes, if you've followed Cloud Platform CI/CD setup guides for CircleCI or GitHub Actions. However teams have been free to make other choices, so they should adapt these accordingly.

### Deploy with CircleCI

For teams who use CircleCI for continuous deployment, in Step 3 when you migrated your namespace environment to "live", a new ServiceAccount was created in the "live" cluster, for you to use with CircleCI. This ServiceAccount is created by either the serviceaccount.yaml or serviceaccount.tf file in your namespace folder.

You need to copy the ServiceAccount credentials to CircleCI secrets, so it can authenticate with the "live" cluster. Retrieve [ca.crt, token and cluster_name](/documentation/deploying-an-app/using-circleci-for-continuous-deployment.html#retrieving-the-service-account-credentials), making sure kubectl is talking to the 'live' cluster. The `cluster_name` of the "live" cluster is `DF366E49809688A3B16EEC29707D8C09.gr7.eu-west-2.eks.amazonaws.com`. Paste these secrets into the CircleCI web console.

To deploy to both "live" and "live-1" you can either:

- create a separate pipeline to deploy in "live"
- amend the existing pipeline to add additional steps to authenticate and deploy to "live". An example config.yml file is available [here](https://github.com/ministryofjustice/cloud-platform-reference-app/blob/main/.circleci/config.yml). It builds an image, authenticates and deploys an application to both the "live-1" and "live" clusters in the same workflow.

### Deploy with GitHub Actions

Earlier on in this migration you will have [migrated the ServiceAccount module](migrate-to-live-sa-github-actions.html#migrating-serviceaccount-module-for-github-actions-to-the-quot-live-quot-cluster), leading to GitHub Action secrets differentiated by the cluster e.g. `KUBE_CLUSTER_LIVE_1`.

An example of GitHub Actions config is provided in this repo: https://github.com/ministryofjustice/cloud-platform-reference-app-github-action. It deploys the app to both the live-1 and live clusters.

## Step 7 - Trigger your pipeline

Trigger your application pipeline. This is normally done manually or by a push to branch.

## Step 8 - Test your application

Test your application is working on the new cluster.

This could include:

- pingdom
- automated smoke tests - requests should be sent to the new load balancer IP (TOD) and add a "Host: <hostname>" header.
- manual browser testing - requires a new ingress with a test host name

## Step 9 - Start sending real traffic

Traffic flow is controlled by tweaking external-dns ingress annotation (`aws-weight`), which is used to determine the proportion of traffic sent to that ingress.
Initially it is not expected to have any traffic because live’s ingresses have `external-dns.alpha.kubernetes.io/aws-weight: "0"`.
For example, by setting this value to 5 on "live" and 95 on "live-1", route53 will send 5% of real traffic to the application on "live".

To apply the changes to the ingress, as a quick alternative to running your application's CI/CD pipeline, you can edit them directly:

```bash
    kubectl edit ingress <ingress-name> -n <namespace>
```

It is advised to send between 1-10% of the traffic to the live cluster initially. Once the traffic flows into the live cluster and the application behaves as expected, 
you could send 100% of the traffic into live cluster and 0% into live-1 cluster.

NOTE: If results are not successful within the live cluster, rollback the annotations to their initial weights ('100' on "live-1", '0'  on "live").

## Step 10 - Cleanup old "live-1" namespace

Once you have successfully migrated to the "live" cluster, you can tidy up by removing your (now unused) namespace from the ["live-1" directory][live-1-folder]:

```bash
rm -rf ./namespaces/live-1.cloud-platform.service.justice.gov.uk/<my-namespace-name>
```

As usual, commit this change to a branch, create a PR with title `Deletion of NS due to migration to live cluster` and once approved merge to `main`.

## Links for the new cluster

Prometheus: [https://prometheus.live.cloud-platform.service.justice.gov.uk](https://prometheus.live.cloud-platform.service.justice.gov.uk)

AlertManager: [https://alertmanager.live.cloud-platform.service.justice.gov.uk](https://alertmanager.live.cloud-platform.service.justice.gov.uk)

Grafana: [https://grafana.live.cloud-platform.service.justice.gov.uk](https://grafana.live.cloud-platform.service.justice.gov.uk)

Kibana:
[https://kibana.cloud-platform.service.justice.gov.uk/_plugin/kibana](https://kibana.cloud-platform.service.justice.gov.uk/_plugin/kibana/app/kibana#/home?_g=())

Authenticating to the Cluster: [login.live.cloud-platform.service.justice.gov.uk][authenticate-to-cluster]

## How to roll back

Easy roll back is a key feature of this migration process. At any point in this migration process you can revert to using your service on the original "live-1" cluster. This may be necessary if you encounter problems running the namespace on the new cluster, and post-pone the migration until that is resolved.

To rollback, going through the process in reverse, depending on how far you got:

- Step 10 "Clean-up old "live-1" namespace" -
  Revert the pull request that deleted your [live-1-folder]. It doesn't get applied yet, because of the skipfile.

- Step 9 - "Start sending real traffic" -
  Change the ingress weightings to 0 on "live", and 100 on "live-1". This directs all traffic to the live-1 cluster. There may be a lag, as users' DNS caches update.

  **At this point your application is running as it was pre-migration**

- Step 8 - Test your application -
  It's a good idea to check it works fine.

- (Step 7 - Trigger your pipeline -
  Not needed - your app is deployed still on the old cluster)

- Step 6 - "Create a new step to your deployment pipeline" -
  You need to ensure your application's CI/CD deploys:
  * just to the old cluster "live-1"
  * using the old names of credentials (not with the "_live1' suffix)

- (Step 5 - Add a new ingress resource -
  Not needed - you can leave this ingress in place)

- (Step 4 - Authenticate to the “live” cluster -
  Not needed - you can keep your kubectl context for the new cluster)

- Step 3 - "Migrate your namespace environment to "live"" -
  Move the 'skip file' from your namespace under the ["live" folder][live-folder] into your namespace under the ["live-1" folder][live-1-folder]:

  ```bash
  cd namespaces
  mv live-1.cloud-platform.service.justice.gov.uk/<namespace>/APPLY_PIPELINE_SKIP_THIS_NAMESPACE live.cloud-platform.service.justice.gov.uk/<namespace>/
  ```
  
  Once merged to `main`, the Apply Pipeline will be configured to apply your namespace settings that are in your "live-1" folder, rather than you "live" folder. (You must have the skip file in one of the folders, otherwise there will be errors on every terraform apply, because the terraform state is shared between the Apply pipelines for live and live-1, will conflict on every apply have both since the terraform is slightly different - IRSA etc)

  **Important**: The ServiceAccount credentials used by your application CI/CD will have been rotated by the Apply Pipeline, within a couple minutes of merging the skip file change. So your application's CI/CD will need its secrets updating. If you use GitHub Actions then the ServiceAccount terraform module will automatically update the GHA secrets. However if you use CircleCI or any other way of deployment, then you'll need to obtain the fresh token from "live-1" and provide it to your CI, replacing the existing token secret with this new value.

  **At this point your application can be re-deployed again normally**

  **At this point you are also free to make changes to your environment under the ["live-1" folder][live-1-folder]**

- (Step 2 - Amend your ingress resource -
  Not needed - you can leave the annotation in - it won't change anything)

- (Step 1 - Agree a time to perform the migration -
  You can let your team know that they are no longer restricted from changing the environment config)

## Questions

If you have questions, please ask us on Slack:

- #cloud-platform-eks-migration-plan-testing if you're in this group
- #ask-cloud-platform

## Feedback welcome

We'd love to receive feedback on this process, to make it as low friction as possible. Please drop us a message on Slack:

- #cloud-platform-eks-migration-plan-testing if you're in this group
- DM to @antony.bishop

[irsa]: https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html
[kiam]: https://github.com/uswitch/kiam
[es-example]: https://github.com/ministryofjustice/cloud-platform-terraform-elasticsearch/blob/main/example/elasticsearch.tf#L34-L36
[iam-infra]: https://github.com/ministryofjustice/cloud-platform-infrastructure/tree/main/terraform/cross-account-IAM
[irsa-cross-account]: /documentation/other-topics/access-cross-aws-resources-irsa-eks.html#use-iam-roles-for-service-accounts-to-access-resources-in-a-different-aws-account
[env-repo]: https://github.com/ministryofjustice/cloud-platform-environments/
[job-history-limit]: https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/#jobs-history-limits
[ns-changes]: /documentation/other-topics/migrate-to-live.html#namespace-resource-changes
[gh-actions]: /documentation/deploying-an-app/github-actions-continuous-deployment.html#continuous-deployment-of-an-application-using-github-actions
[sa-example]: https://github.com/ministryofjustice/cloud-platform-environments/blob/main/namespaces/live-1.cloud-platform.service.justice.gov.uk/justicedata-prod/resources/serviceaccount.tf#L4-L7
[authenticate-to-cluster]: https://login.live.cloud-platform.service.justice.gov.uk
[live-1-folder]: https://github.com/ministryofjustice/cloud-platform-environments/tree/main/namespaces/live-1.cloud-platform.service.justice.gov.uk
[live-folder]: https://github.com/ministryofjustice/cloud-platform-environments/tree/main/namespaces/live.cloud-platform.service.justice.gov.uk
[cli]: https://github.com/ministryofjustice/cloud-platform-cli
